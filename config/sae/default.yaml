input_dim: 2304          # Gemma-2B hidden size
sae_dim: 9216            # 4x expansion
batch_size: 4
epochs: 1
lr: 3e-4
l1_coeff: 0.005
layer_idx: 12
max_steps: 1000
fixed_v_cnt: 1

# contrastive learning 설정 (multi-concept)
concept_feature_indices:
  code: 0
  harm: 1
  struct: 2

alpha_concept: # loss 내의 가중치
  code: 0.0
  harm: 0.0
  struct: 0.0

# 각 라벨에서 feature가 크게 활성되길 원하는 target 값 (기본은 1)
positive_targets:
  code: 1
  harm: 1
  struct: 1


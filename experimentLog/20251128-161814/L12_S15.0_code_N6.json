{
  "model_name": "google/gemma-2-2b-it",
  "layer_idx": 12,
  "command": "python main.py experiment.use_multi_contrast=true sae.loss_option=2 sae.loss_module=option2_loss sae.guidance_method=contrastive sae.guidance_coeff=1.0 sae.alpha_concept.code=0.05 sae.alpha_concept.harm=0.05 sae.alpha_concept.struct=0.0 sae.use_l0=true sae.l0_coeff=0.0009195390858312656 sae.role_sep_coeff=0.0095",
  "sae": {
    "input_dim": 2304,
    "sae_dim": 9216,
    "fixed_v_cnt": 3,
    "batch_size": 3,
    "epochs": 2,
    "max_steps": 2000,
    "l1_coeff": 0.005,
    "l0_coeff": 0.0009195390858312656,
    "use_l0": true,
    "concept_samples_per_label": 100,
    "loss_option": 2,
    "loss_module": "option2_loss",
    "concept_feature_indices": {
      "code": 0,
      "harm": 1,
      "struct": 2
    },
    "alpha_concept": {
      "code": 0.05,
      "harm": 0.05,
      "struct": 0.0
    },
    "positive_targets": {
      "code": 1,
      "harm": 1,
      "struct": 1
    },
    "guidance_method": "contrastive",
    "guidance_margin": 0.0,
    "guidance_coeff": 1.0,
    "guidance_mse_pos_value": 1.0,
    "guidance_mse_neg_value": 0.0,
    "jumprelu_bandwidth": 0.001,
    "jumprelu_init_threshold": 0.001,
    "role_sep_coeff": 0.0095
  },
  "gnn": {
    "use_gnn": true,
    "top_k": 10
  },
  "experiment": {
    "device": "cuda",
    "dataset_name": "wikitext",
    "dataset_config": "wikitext-2-raw-v1",
    "use_multi_contrast": true,
    "num_steering_samples_per_label": 10,
    "top_k_for_plot": 10,
    "layer_sweep": [
      12
    ],
    "strength_sweep": [
      15.0
    ]
  },
  "steering": {
    "label": "code",
    "feature_idx": 0,
    "strength": 15.0,
    "hook_layer_idx": 11
  },
  "prompt": "#!/usr/bin/env python\n# -*- coding: utf8 -*-\n# ============================================================================\n#  Copyright (c) 2013-2020 nexB Inc. http://www.nexb.com/ - All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#      http://www.apache.org/licenses/LICENSE-2.0\n#  Unless required",
  "output": {
    "before": "#!/usr/bin/env python\n# -*- coding: utf8 -*-\n# ============================================================================\n#  Copyright (c) 2013-2020 nexB Inc. http://www.nexb.com/ - All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#      http://www.apache.org/licenses/LICENSE-2.0\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n# ============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom collections import Counter\nfrom collections import OrderedDict\nimport io\nimport json\n\nimport attr\n\nfrom attributecode import CRITICAL\nfrom attributecode import Error\nfrom attributecode import saneyaml\nfrom attributecode.util import csv\nfrom attributecode.util import python2\nfrom attributecode.util import replace_tab_with_spaces\n\n\nif python2:  # pragma: nocover\n    from itertools import izip_longest as zip_longest  # NOQA\nelse:  # pragma: nocover\n    from itertools import zip_longest  # NOQA\n\n\ndef transform_csv_to_csv(location, output, transformer):\n    \"\"\"\n    Read a CSV file at `location` and write a new CSV file at `output`. Apply\n    transformations using the `transformer` Transformer.\n    Return a list of Error objects.\n    \"\"\"\n    if not transformer:\n        raise ValueError('Cannot transform without Transformer')\n\n    rows = read_csv_rows(location)\n\n    errors = []\n    data = iter(rows)\n    names = next(rows)\n    field_names = strip_trailing_fields_csv(names)\n    dupes = check_duplicate_fields(field_names)\n\n    if dupes:\n        msg = u'Duplicated field name: %(name)s'\n        for name in dupes:\n            errors.append(Error(CRITICAL, msg % locals()))\n        return field_names, [], errors\n\n    # Convert to dicts\n    new_data = [OrderedDict(zip_longest(field_names, item)) for item in data]\n\n    field_names, updated_data, errors = transform_data(new_data, transformer)\n\n    if errors:\n        return errors\n    else:\n        write_csv(output, updated_data, field_names)\n        return []\n\n\ndef transform_json_to_json(location, output, transformer):\n    \"\"\"\n    Read a JSON file at `location` and write a new JSON file at `output`. Apply\n    transformations using the `transformer` Transformer.\n    Return a list of Error objects.\n    \"\"\"\n    if not transformer:\n        raise ValueError('Cannot transform without Transformer')\n\n    items = read_json(location)\n    data = strip_trailing_fields_json(items)\n    new_data = normalize_dict_data(data)\n\n    field_names, updated_data, errors = transform_data(new_data, transformer)\n\n    if errors:\n        return errors\n    else:\n        write_json(output, updated_data)\n        return []\n\ndef strip_trailing_fields_csv(names):\n    \"\"\"\n    Strip trailing spaces for field names #456\n    \"\"\"\n    field_names = []\n    for name in names:\n        field_names.append(name.strip())\n    return field_names\n\n\ndef strip_trailing_fields_json(items):\n    \"\"\"\n    Strip trailing spaces for field name #456\n    \"\"\"\n    data = []\n    od = OrderedDict()\n    for item in items:\n        for field in item:\n            stripped_field_name = field.strip()\n            od[stripped_field_name] = item[field]\n        data.append(od)\n    return data\n\ndef normalize_dict_data(data):\n    \"\"\"\n    Check if the input data from scancode-toolkit and normalize to a normal\n    dictionary if it is.\n    Return a list type of normalized dictionary.\n    \"\"\"\n    try:\n        # Check if this is a JSON output from scancode-toolkit\n        if(data[\"headers\"][0][\"tool_name\"] == \"scancode-toolkit\"):\n            #only takes data inside \"files\"\n            new_data = data[\"files\"]\n    except:\n        new_data = data\n    if not isinstance(new_data, list):\n        new_data = [new_data]\n    return new_data\n\n\ndef transform_data(data, transformer):\n    \"\"\"\n    Read a dictionary and apply transformations using the\n    `transformer` Transformer.\n    Return a tuple of:\n       ([field names...], [transformed ordered dict...], [Error objects..])\n    \"\"\"\n    if not transformer:\n        return data\n\n    renamed_field_data = transformer.apply_renamings(data)\n\n    field_names = renamed_field_data[0].keys()\n\n    if transformer.field_filters:\n        renamed_field_data = list(transformer.filter_fields(renamed_field_data))\n        field_names = [c for c in field_names if c in transformer.field_filters]\n\n    if transformer.exclude_fields:\n        renamed_field_data = list(transformer.filter_excluded(renamed_field_data))\n        field_names = [c for c in field_names if c not in transformer.exclude_fields]\n\n    errors = transformer.check_required_fields(renamed_field_data)\n    if errors:\n        return field_names, data, errors\n    return field_names, renamed_field_data, errors\n\n\ntranformer_config_help = '''\nA transform configuration file is used to describe which transformations and\nvalidations to apply to a source CSV file. This is a simple text file using YAML\nformat, using the same format as an .ABOUT file.\n\nThe attributes that can be set in a configuration file are:\n\n* field_renamings:\nAn optional map of source CSV or JSON field name to target CSV/JSON new field name that\nis used to rename CSV fields.\n\nFor instance with this configuration the fields \"Directory/Location\" will be\nrenamed to \"about_resource\" and \"foo\" to \"bar\":\n    field_renamings:\n        about_resource : 'Directory/Location'\n        bar : foo\n\nThe renaming is always applied first before other transforms and checks. All\nother field names referenced below are these that exist AFTER the renamings\nhave been applied to the existing field names.\n\n* required_fields:\nAn optional list of required field names that must have a value, beyond the\nstandard fields names. If a source CSV/JSON does not have such a field or a row is\nmissing a value for a required field, an error is reported.\n\nFor instance with this configuration an error will be reported if the fields\n\"name\" and \"version\" are missing or if any row does not have a value set for\nthese fields:\n    required_fields:\n        - name\n        - version\n\n* field_filters:\nAn optional list of field names that should be kept in the transformed CSV/JSON. If\nthis list is provided, all the fields from the source CSV/JSON that should be kept\nin the target CSV/JSON must be listed regardless of  either standard or required\nfields. If this list is not provided, all source CSV/JSON fields are kept in the\ntransformed target CSV/JSON.\n\nFor instance with this configuration the target CSV/JSON will only contains the \"name\"\nand \"version\" fields and no other field:\n    field_filters:\n        - name\n        - version\n\n* exclude_fields:\nAn optional list of field names that should be excluded in the transformed CSV/JSON. If\nthis list is provided, all the fields from the source CSV/JSON that should be excluded\nin the target CSV/JSON must be listed. Excluding standard or required fields will cause\nan error. If this list is not provided, all source CSV/JSON fields are kept in the\ntransformed target CSV/JSON.\n\nFor instance with this configuration the target CSV/JSON will not contain the \"type\"\nand \"temp\" fields:\n    exclude_fields:\n        - type\n        - temp\n'''\n\n\n@attr.attributes\nclass Transformer(object):\n    __doc__ = tranformer_config_help\n\n    field_renamings = attr.attrib(default=attr.Factory(dict))\n    required_fields = attr.attrib(default=attr.Factory(list))\n    field_filters = attr.attrib(default=attr.Factory(list))\n    exclude_fields = attr.attrib(default=attr.Factory(list))\n\n    # a list of all the standard fields from AboutCode toolkit\n    standard_fields = attr.attrib(default=attr.Factory(list), init=False)\n    # a list of the subset of standard fields that are essential and MUST be\n    # present for AboutCode toolkit to work\n    essential_fields = attr.attrib(default=attr.Factory(list), init=False)\n\n    # called by attr after the __init__()\n    def __attrs_post_init__(self, *args, **kwargs):\n        from attributecode.model import About\n        about = About()\n        self.essential_fields = list(about.required_fields)\n        self.standard_fields = [f.name for f in about.all_fields()]\n\n    @classmethod\n    def default(cls):\n        \"\"\"\n        Return a default Transformer with built-in transforms.\n        \"\"\"\n        return cls(\n            field_renamings={},\n            required_fields=[],\n            field_filters=[],\n            exclude_fields=[],\n        )\n\n    @classmethod\n    def from_file(cls, location):\n        \"\"\"\n        Load and return a Transformer instance from a YAML configuration file at\n        `location`.\n        \"\"\"\n        with io.open(location, encoding='utf-8') as conf:\n            data = saneyaml.load(replace_tab_with_spaces(conf.read()))\n        return cls(\n            field_renamings=data.get('field_renamings', {}),\n            required_fields=data.get('required_fields', []),\n            field_filters=data.get('field_filters', []),\n            exclude_fields=data.get('exclude_fields', []),\n        )\n\n    def check_required_fields(self, data):\n        \"\"\"\n        Return a list of Error for a `data` list of ordered dict where a\n        dict is missing a value for a required field name.\n        \"\"\"\n        errors = []\n        required = set(self.essential_fields + self.required_fields)\n        if not required:\n            return []\n\n        for rn, item in enumerate(data):\n            missings = [rk for rk in required if not item.get(rk)]\n            if not missings:\n                continue\n\n            missings = ', '.join(missings)\n            msg = 'Row {rn} is missing required values for fields: {missings}'\n            errors.append(Error(CRITICAL, msg.format(**locals())))\n        return errors\n\n    def apply_renamings(self, data):\n        \"\"\"\n        Return a tranformed list of `field_names` where fields are renamed\n        based on this Transformer configuration.\n        \"\"\"\n        renamings = self.field_renamings\n        if not renamings:\n            return data\n        renamings = {n: rn for n, rn in renamings.items()}\n\n        renamed_list = []\n        for row in data:\n            renamed = OrderedDict()\n            for key in row:\n                matched = False\n                for renamed_key in renamings:\n                    if key == renamings[renamed_key]:\n                        renamed[renamed_key] = row[key]\n                        matched = True\n                if not matched:\n                    renamed[key] = row[key]\n            renamed_list.append(renamed)\n        return renamed_list\n\n    \"\"\"\n    def clean_fields(self, field_names):\n\n        Apply standard cleanups to a list of fields and return these.\n\n        if not field_names:\n            return field_names\n        return [c.strip().lower() for c in field_names]\n    \"\"\"\n    def filter_fields(self, data):\n        \"\"\"\n        Yield transformed dicts from a `data` list of dicts keeping only\n        fields with a name in the `field_filters`of this Transformer.\n        Return the data unchanged if no `field_filters` exists.\n        \"\"\"\n        #field_filters = set(self.clean_fields(self.field_filters))\n        field_filters = set(self.field_filters)\n        for entry in data:\n            items = ((k, v) for k, v in entry.items() if k in field_filters)\n            yield OrderedDict(items)\n\n    def filter_excluded(self, data):\n        \"\"\"\n        Yield transformed dicts from a `data` list of dicts excluding \n        fields with names in the `exclude_fields`of this Transformer.\n        Return the data unchanged if no `exclude_fields` exists.\n        \"\"\"\n        #exclude_fields = set(self.clean_fields(self.exclude_fields))\n        exclude_fields = set(self.exclude_fields)\n        for entry in data:\n            items = ((k, v) for k, v in entry.items() if k not in exclude_fields)\n            yield OrderedDict(items)\n\n\ndef check_duplicate_fields(field_names):\n    \"\"\"\n    Check that there are no duplicate in the `field_names` list of field name\n    strings, ignoring case. Return a list of unique duplicated field names.\n    \"\"\"\n    counted = Counter(c.lower() for c in field_names)\n    return [field for field, count in sorted(counted.items()) if count > 1]\n\n\ndef read_csv_rows(location):\n    \"\"\"\n    Yield rows (as a list of values) from a CSV file at `location`.\n    \"\"\"\n    with io.open(location, encoding='utf-8', errors='replace') as csvfile:\n        reader = csv.reader(csvfile)\n        for row in reader:\n            yield row\n\n\ndef read_json(location):\n    \"\"\"\n    Yield rows (as a list of values) from a CSV file at `location`.\n    \"\"\"\n    with io.open(location, encoding='utf-8', errors='replace') as jsonfile:\n        data = json.load(jsonfile, object_pairs_hook=OrderedDict)\n        return data\n\n\ndef write_csv(location, data, field_names):  # NOQA\n    \"\"\"\n    Write a CSV file at `location` the `data` list of ordered dicts using the\n    `field_names`.\n    \"\"\"\n    with io.open(location, 'w', encoding='utf-8', newline='\\n') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=field_names)\n        writer.writeheader()\n        writer.writerows(data)\n\n\ndef write_json(location, data):\n    \"\"\"\n    Write a JSON file at `location` the `data` list of ordered dicts.\n    \"\"\"\n    with open(location, 'w') as jsonfile:\n        json.dump(data, jsonfile, indent=3)\n```\n",
    "after": "#!/usr/bin/env python\n# -*- coding: utf8 -*-\n# ============================================================================\n#  Copyright (c) 2013-2020 nexB Inc. http://www.nexb.com/ - All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#      http://www.apache.org/licenses/LICENSE-2.0\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n# ============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom collections import Counter\nfrom collections import OrderedDict\nimport io\nimport json\n\nimport attr\n\nfrom attributecode import CRITICAL\nfrom attributecode import Error\nfrom attributecode import saneyaml\nfrom attributecode.util import csv\nfrom attributecode.util import python2\nfrom attributecode.util import replace_tab_with_spaces\n\n\nif python2:  # pragma: nocover\n    from itertools import izip_longest as zip_longest  # NOQA\nelse:  # pragma: nocover\n    from itertools import zip_longest  # NOQA\n\n\ndef transform_csv_to_csv(location, output, transformer):\n    \"\"\"\n    Read a CSV file at `location` and write a new CSV file at `output`. Apply\n    transformations using the `transformer` Transformer.\n    Return a list of Error objects.\n    \"\"\"\n    if not transformer:\n        raise ValueError('Cannot transform without Transformer')\n\n    rows = read_csv_rows(location)\n\n    errors = []\n    data = iter(rows)\n    names = next(rows)\n    field_names = strip_trailing_fields_csv(names)\n    dupes = check_duplicate_fields(field_names)\n\n    if dupes:\n        msg = u'Duplicated field name: %(name)s'\n        for name in dupes:\n            errors.append(Error(CRITICAL, msg % locals()))\n        return field_names, [], errors\n\n    # Convert to dicts\n    new_data = [OrderedDict(zip_longest(field_names, item)) for item in data]\n\n    field_names, updated_data, errors = transform_data(new_data, transformer)\n\n    if errors:\n        return errors\n    else:\n        write_csv(output, updated_data, field_names)\n        return []\n\n\ndef transform_json_to_json(location, output, transformer):\n    \"\"\"\n    Read a JSON file at `location` and write a new JSON file at `output`. Apply\n    transformations using the `transformer` Transformer.\n    Return a list of Error objects.\n    \"\"\"\n    if not transformer:\n        raise ValueError('Cannot transform without Transformer')\n\n    items = read_json(location)\n    data = strip_trailing_fields_json(items)\n    new_data = normalize_dict_data(data)\n\n    field_names, updated_data, errors = transform_data(new_data, transformer)\n\n    if errors:\n        return errors\n    else:\n        write_json(output, updated_data)\n        return []\n\ndef strip_trailing_fields_csv(names):\n    \"\"\"\n    Strip trailing spaces for field names #456\n    \"\"\"\n    field_names = []\n    for name in names:\n        field_names.append(name.strip())\n    return field_names\n\n\ndef strip_trailing_fields_json(items):\n    \"\"\"\n    Strip trailing spaces for field name #456\n    \"\"\"\n    data = []\n    od = OrderedDict()\n    for item in items:\n        for field in item:\n            stripped_field_name = field.strip()\n            od[stripped_field_name] = item[field]\n        data.append(od)\n    return data\n\ndef normalize_dict_data(data):\n    \"\"\"\n    Check if the input data from scancode-toolkit and normalize to a normal\n    dictionary if it is.\n    Return a list type of normalized dictionary.\n    \"\"\"\n    try:\n        # Check if this is a JSON output from scancode-toolkit\n        if(data[\"headers\"][0][\"tool_name\"] == \"scancode-toolkit\"):\n            #only takes data inside \"files\"\n            new_data = data[\"files\"]\n    except:\n        new_data = data\n    if not isinstance(new_data, list):\n        new_data = [new_data]\n    return new_data\n\n\ndef transform_data(data, transformer):\n    \"\"\"\n    Read a dictionary and apply transformations using the\n    `transformer` Transformer.\n    Return a tuple of:\n       ([field names...], [transformed ordered dict...], [Error objects..])\n    \"\"\"\n    if not transformer:\n        return data\n\n    renamed_field_data = transformer.apply_renamings(data)\n\n    field_names = renamed_field_data[0].keys()\n\n    if transformer.field_filters:\n        renamed_field_data = list(transformer.filter_fields(renamed_field_data))\n        field_names = [c for c in field_names if c in transformer.field_filters]\n\n    if transformer.exclude_fields:\n        renamed_field_data = list(transformer.filter_excluded(renamed_field_data))\n        field_names = [c for c in field_names if c not in transformer.exclude_fields]\n\n    errors = transformer.check_required_fields(renamed_field_data)\n    if errors:\n        return field_names, data, errors\n    return field_names, renamed_field_data, errors\n\n\ntranformer_config_help = '''\nA transform configuration file is used to describe which transformations and\nvalidations to apply to a source CSV file. This is a simple text file using YAML\nformat, using the same format as an .ABOUT file.\n\nThe attributes that can be set in a configuration file are:\n\n* field_renamings:\nAn optional map of source CSV or JSON field name to target CSV/JSON new field name that\nis used to rename CSV fields.\n\nFor instance with this configuration the fields \"Directory/Location\" will be\nrenamed to \"about_resource\" and \"foo\" to \"bar\":\n    field_renamings:\n        about_resource : 'Directory/Location'\n        bar : foo\n\nThe renaming is always applied first before other transforms and checks. All\nother field names referenced below are these that exist AFTER the renamings\nhave been applied to the existing field names.\n\n* required_fields:\nAn optional list of required field names that must have a value, beyond the\nstandard fields names. If a source CSV/JSON does not have such a field or a row is\nmissing a value for a required field, an error is reported.\n\nFor instance with this configuration an error will be reported if the fields\n\"name\" and \"version\" are missing or if any row does not have a value set for\nthese fields:\n    required_fields:\n        - name\n        - version\n\n* field_filters:\nAn optional list of field names that should be kept in the transformed CSV/JSON. If\nthis list is provided, all the fields from the source CSV/JSON that should be kept\nin the target CSV/JSON must be listed regardless of  either standard or required\nfields. If this list is not provided, all source CSV/JSON fields are kept in the\ntransformed target CSV/JSON.\n\nFor instance with this configuration the target CSV/JSON will only contains the \"name\"\nand \"version\" fields and no other field:\n    field_filters:\n        - name\n        - version\n\n* exclude_fields:\nAn optional list of field names that should be excluded in the transformed CSV/JSON. If\nthis list is provided, all the fields from the source CSV/JSON that should be excluded\nin the target CSV/JSON must be listed. Excluding standard or required fields will cause\nan error. If this list is not provided, all source CSV/JSON fields are kept in the\ntransformed target CSV/JSON.\n\nFor instance with this configuration the target CSV/JSON will not contain the \"type\"\nand \"temp\" fields:\n    exclude_fields:\n        - type\n        - temp\n'''\n\n\n@attr.attributes\nclass Transformer(object):\n    __doc__ = tranformer_config_help\n\n    field_renamings = attr.attrib(default=attr.Factory(dict))\n    required_fields = attr.attrib(default=attr.Factory(list))\n    field_filters = attr.attrib(default=attr.Factory(list))\n    exclude_fields = attr.attrib(default=attr.Factory(list))\n\n    # a list of all the standard fields from AboutCode toolkit\n    standard_fields = attr.attrib(default=attr.Factory(list), init=False)\n    # a list of the subset of standard fields that are essential and MUST be\n    # present for AboutCode toolkit to work\n    essential_fields = attr.attrib(default=attr.Factory(list), init=False)\n\n    # called by attr after the __init__()\n    def __attrs_post_init__(self, *args, **kwargs):\n        from attributecode.model import About\n        about = About()\n        self.essential_fields = list(about.required_fields)\n        self.standard_fields = [f.name for f in about.all_fields()]\n\n    @classmethod\n    def default(cls):\n        \"\"\"\n        Return a default Transformer with built-in transforms.\n        \"\"\"\n        return cls(\n            field_renamings={},\n            required_fields=[],\n            field_filters=[],\n            exclude_fields=[],\n        )\n\n    @classmethod\n    def from_file(cls, location):\n        \"\"\"\n        Load and return a Transformer instance from a YAML configuration file at\n        `location`.\n        \"\"\"\n        with io.open(location, encoding='utf-8') as conf:\n            data = saneyaml.load(replace_tab_with_spaces(conf.read()))\n        return cls(\n            field_renamings=data.get('field_renamings', {}),\n            required_fields=data.get('required_fields', []),\n            field_filters=data.get('field_filters', []),\n            exclude_fields=data.get('exclude_fields', []),\n        )\n\n    def check_required_fields(self, data):\n        \"\"\"\n        Return a list of Error for a `data` list of ordered dict where a\n        dict is missing a value for a required field name.\n        \"\"\"\n        errors = []\n        required = set(self.essential_fields + self.required_fields)\n        if not required:\n            return []\n\n        for rn, item in enumerate(data):\n            missings = [rk for rk in required if not item.get(rk)]\n            if not missings:\n                continue\n\n            missings = ', '.join(missings)\n            msg = 'Row {rn} is missing required values for fields: {missings}'\n            errors.append(Error(CRITICAL, msg.format(**locals())))\n        return errors\n\n    def apply_renamings(self, data):\n        \"\"\"\n        Return a tranformed list of `field_names` where fields are renamed\n        based on this Transformer configuration.\n        \"\"\"\n        renamings = self.field_renamings\n        if not renamings:\n            return data\n        renamings = {n: rn for n, rn in renamings.items()}\n\n        renamed_list = []\n        for row in data:\n            renamed = OrderedDict()\n            for key in row:\n                matched = False\n                for renamed_key in renamings:\n                    if key == renamings[renamed_key]:\n                        renamed[renamed_key] = row[key]\n                        matched = True\n                if not matched:\n                    renamed[key] = row[key]\n            renamed_list.append(renamed)\n        return renamed_list\n\n    \"\"\"\n    def clean_fields(self, field_names):\n\n        Apply standard cleanups to a list of fields and return these.\n\n        if not field_names:\n            return field_names\n        return [c.strip().lower() for c in field_names]\n    \"\"\"\n    def filter_fields(self, data):\n        \"\"\"\n        Yield transformed dicts from a `data` list of dicts keeping only\n        fields with a name in the `field_filters`of this Transformer.\n        Return the data unchanged if no `field_filters` exists.\n        \"\"\"\n        #field_filters = set(self.clean_fields(self.field_filters))\n        field_filters = set(self.field_filters)\n        for entry in data:\n            items = ((k, v) for k, v in entry.items() if k in field_filters)\n            yield OrderedDict(items)\n\n    def filter_excluded(self, data):\n        \"\"\"\n        Yield transformed dicts from a `data` list of dicts excluding \n        fields with names in the `exclude_fields`of this Transformer.\n        Return the data unchanged if no `exclude_fields` exists.\n        \"\"\"\n        #exclude_fields = set(self.clean_fields(self.exclude_fields))\n        exclude_fields = set(self.exclude_fields)\n        for entry in data:\n            items = ((k, v) for k, v in entry.items() if k not in exclude_fields)\n            yield OrderedDict(items)\n\n\ndef check_duplicate_fields(field_names):\n    \"\"\"\n    Check that there are no duplicate in the `field_names` list of field name\n    strings, ignoring case. Return a list of unique duplicated field names.\n    \"\"\"\n    counted = Counter(c.lower() for c in field_names)\n    return [field for field, count in sorted(counted.items()) if count > 1]\n\n\ndef read_csv_rows(location):\n    \"\"\"\n    Yield rows (as a list of values) from a CSV file at `location`.\n    \"\"\"\n    with io.open(location, encoding='utf-8', errors='replace') as csvfile:\n        reader = csv.reader(csvfile)\n        for row in reader:\n            yield row\n\n\ndef read_json(location):\n    \"\"\"\n    Yield rows (as a list of values) from a CSV file at `location`.\n    \"\"\"\n    with io.open(location, encoding='utf-8', errors='replace') as jsonfile:\n        data = json.load(jsonfile, object_pairs_hook=OrderedDict)\n        return data\n\n\ndef write_csv(location, data, field_names):  # NOQA\n    \"\"\"\n    Write a CSV file at `location` the `data` list of ordered dicts using the\n    `field_names`.\n    \"\"\"\n    with io.open(location, 'w', encoding='utf-8', newline='\\n') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=field_names)\n        writer.writeheader()\n        writer.writerows(data)\n\n\ndef write_json(location, data):\n    \"\"\"\n    Write a JSON file at `location` the `data` list of ordered dicts.\n    \"\"\"\n    with open(location, 'w') as jsonfile:\n        json.dump(data, jsonfile, indent=3)\n```\n```python\n# --- import ---\n\nfrom attributecode.util import csv\nfrom attributecode.util import python2\nfrom attributecode.util import replace_tab_with_spaces\nfrom attributecode.util import sanitize\nfrom"
  },
  "feature_stats": {
    "indices": [
      0,
      2,
      8711,
      2874,
      1646,
      5111,
      3943,
      8935,
      7673,
      3030,
      8147
    ],
    "f_before": [
      94.24533081054688,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "f_after": [
      184.37937927246094,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.4319688379764557,
      0.0,
      0.5019125938415527
    ],
    "g_before": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "g_after": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "num_batches_for_stats": 10
  },
  "loss_summary": {
    "train_mean_loss": 8.755375345926732,
    "train_mean_l2": 5.643428234145046,
    "train_mean_l1": 3.491124365925789,
    "num_steps": 2000,
    "num_batches": 2000
  },
  "feature_std_summary": {
    "plot_path": "outputs/plots/20251128-161814/feature_std_layer_12.html",
    "mean_std": 1.2764151096343994,
    "max_std": 34.51808166503906
  },
  "hook_comparison": {
    "hook_layer_idx_minus1": {
      "hook_layer_idx": 11,
      "feature_stats": {
        "indices": [
          0,
          2,
          8711,
          2874,
          1646,
          5111,
          3943,
          8935,
          7673,
          3030,
          8147
        ],
        "f_before": [
          94.24533081054688,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ],
        "f_after": [
          184.37937927246094,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.4319688379764557,
          0.0,
          0.5019125938415527
        ],
        "g_before": [
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ],
        "g_after": [
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ]
      },
      "top_activated_features": {
        "features": [
          {
            "index": 0,
            "f_before": 94.24533081054688,
            "f_after": 184.37937927246094,
            "delta_f": 90.13404846191406
          },
          {
            "index": 772,
            "f_before": 0.0,
            "f_after": 9.36510944366455,
            "delta_f": 9.36510944366455
          },
          {
            "index": 7148,
            "f_before": 0.0,
            "f_after": 9.187091827392578,
            "delta_f": 9.187091827392578
          },
          {
            "index": 849,
            "f_before": 0.0,
            "f_after": 8.769755363464355,
            "delta_f": 8.769755363464355
          },
          {
            "index": 5868,
            "f_before": 0.0,
            "f_after": 8.528897285461426,
            "delta_f": 8.528897285461426
          },
          {
            "index": 583,
            "f_before": 0.0,
            "f_after": 8.398734092712402,
            "delta_f": 8.398734092712402
          },
          {
            "index": 875,
            "f_before": 0.0,
            "f_after": 6.455850601196289,
            "delta_f": 6.455850601196289
          },
          {
            "index": 1849,
            "f_before": 0.0,
            "f_after": 6.421021938323975,
            "delta_f": 6.421021938323975
          },
          {
            "index": 295,
            "f_before": 0.0,
            "f_after": 6.251266002655029,
            "delta_f": 6.251266002655029
          },
          {
            "index": 5450,
            "f_before": 0.0,
            "f_after": 6.056046009063721,
            "delta_f": 6.056046009063721
          },
          {
            "index": 571,
            "f_before": 61.33833694458008,
            "f_after": 55.300537109375,
            "delta_f": -6.037799835205078
          },
          {
            "index": 7265,
            "f_before": 4.459297180175781,
            "f_after": 10.422889709472656,
            "delta_f": 5.963592529296875
          },
          {
            "index": 755,
            "f_before": 0.0,
            "f_after": 5.833183765411377,
            "delta_f": 5.833183765411377
          },
          {
            "index": 1,
            "f_before": 19.77281951904297,
            "f_after": 13.952244758605957,
            "delta_f": -5.820574760437012
          },
          {
            "index": 5145,
            "f_before": 0.0,
            "f_after": 5.817450046539307,
            "delta_f": 5.817450046539307
          },
          {
            "index": 2209,
            "f_before": 0.0,
            "f_after": 5.428529739379883,
            "delta_f": 5.428529739379883
          }
        ],
        "k": 16
      },
      "output": {
        "before": "#!/usr/bin/env python\n# -*- coding: utf8 -*-\n# ============================================================================\n#  Copyright (c) 2013-2020 nexB Inc. http://www.nexb.com/ - All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#      http://www.apache.org/licenses/LICENSE-2.0\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for",
        "after": "#!/usr/bin/env python\n# -*- coding: utf8 -*-\n# ============================================================================\n#  Copyright (c) 2013-2020 nexB Inc. http://www.nexb.com/ - All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#      http://www.apache.org/licenses/LICENSE-2.0\n#  Unless required by your license or your project, use the following code:\n#  import *\n#  from py_modules import *\n# ============================================================================\n\nimport os\nimport sys\nimport unittest\nfrom unittest.mock import Magic"
      }
    },
    "hook_layer_idx_same": {
      "hook_layer_idx": 12,
      "feature_stats": {
        "indices": [
          0,
          2,
          8711,
          2874,
          1646,
          5111,
          3943,
          8935,
          7673,
          3030,
          8147
        ],
        "f_before": [
          94.24533081054688,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ],
        "f_after": [
          94.24533081054688,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ],
        "g_before": [
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ],
        "g_after": [
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ]
      },
      "top_activated_features": {
        "features": [
          {
            "index": 8,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 10,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 4,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 15,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 12,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 3,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 9,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 1,
            "f_before": 19.77281951904297,
            "f_after": 19.77281951904297,
            "delta_f": 0.0
          },
          {
            "index": 11,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 5,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 7,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 13,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 0,
            "f_before": 94.24533081054688,
            "f_after": 94.24533081054688,
            "delta_f": 0.0
          },
          {
            "index": 2,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 6,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 14,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          }
        ],
        "k": 16
      },
      "output": {
        "before": "#!/usr/bin/env python\n# -*- coding: utf8 -*-\n# ============================================================================\n#  Copyright (c) 2013-2020 nexB Inc. http://www.nexb.com/ - All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#      http://www.apache.org/licenses/LICENSE-2.0\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for",
        "after": "#!/usr/bin/env python\n# -*- coding: utf8 -*-\n# ============================================================================\n#  Copyright (c) 2013-2020 nexB Inc. http://www.nexb.com/ - All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#      http://www.apache.org/licenses/LICENSE-2.0\n#  Unless required by the License, this file is provided on an \"As Is\" basis.\n# ============================================================================\n\nfrom datetime import datetime\nimport re\nfrom os.path import basename\nfrom typing import List, Dict, Optional\n\nfrom"
      }
    }
  }
}
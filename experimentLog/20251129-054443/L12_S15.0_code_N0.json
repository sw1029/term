{
  "model_name": "google/gemma-2-2b-it",
  "layer_idx": 12,
  "command": "main.py experiment.use_multi_contrast=true sae.loss_option=2 sae.loss_module=option2_loss sae.guidance_method=contrastive sae.guidance_coeff=1.0 sae.alpha_concept.code=0.05 sae.alpha_concept.harm=0.05 sae.alpha_concept.struct=0.05 sae.use_l0=true sae.l0_coeff=0.0009132649020605174 sae.role_sep_coeff=0.0035",
  "sae": {
    "input_dim": 2304,
    "sae_dim": 9216,
    "fixed_v_cnt": 3,
    "batch_size": 3,
    "epochs": 2,
    "max_steps": 2000,
    "l1_coeff": 0.005,
    "l0_coeff": 0.0009132649020605174,
    "use_l0": true,
    "concept_samples_per_label": 100,
    "loss_option": 2,
    "loss_module": "option2_loss",
    "concept_feature_indices": {
      "code": 0,
      "harm": 1,
      "struct": 2
    },
    "alpha_concept": {
      "code": 0.05,
      "harm": 0.05,
      "struct": 0.05
    },
    "positive_targets": {
      "code": 1,
      "harm": 1,
      "struct": 1
    },
    "guidance_method": "contrastive",
    "guidance_margin": 0.0,
    "guidance_coeff": 1.0,
    "guidance_mse_pos_value": 1.0,
    "guidance_mse_neg_value": 0.0,
    "jumprelu_bandwidth": 0.001,
    "jumprelu_init_threshold": 0.001,
    "role_sep_coeff": 0.0035
  },
  "gnn": {
    "use_gnn": true,
    "top_k": 10
  },
  "experiment": {
    "device": "cuda",
    "dataset_name": "wikitext",
    "dataset_config": "wikitext-2-raw-v1",
    "use_multi_contrast": true,
    "num_steering_samples_per_label": 10,
    "top_k_for_plot": 10,
    "layer_sweep": [
      12
    ],
    "strength_sweep": [
      15.0
    ]
  },
  "steering": {
    "label": "code",
    "feature_idx": 0,
    "strength": 15.0
  },
  "prompt": "Describe a typical use case for reinforcement learning\n\nA typical use case for reinforcement learning is a problem where an agent needs to find the best decisions or actions to take in order to maximize reward over time. Examples include robotics, playing games, autonomous driving, and financial trading. In these domains, reinforcement learning can be used to learn good strategies for decision making even in highly non-linear and non-deterministic environments.",
  "output": {
    "before": "Describe a typical use case for reinforcement learning\n\nA typical use case for reinforcement learning is a problem where an agent needs to find the best decisions or actions to take in order to maximize reward over time. Examples include robotics, playing games, autonomous driving, and financial trading. In these domains, reinforcement learning can be used to learn good strategies for decision making even in highly non-linear and non-deterministic environments.\n\n\nHere's a breakdown of a typical use case, with examples:\n\n**The Problem:** \n* **Agent:** An autonomous robot, a game-playing AI, a financial trading algorithm, etc.\n* **Environment:** A complex",
    "after": "Describe a typical use case for reinforcement learning\n\nA typical use case for reinforcement learning is a problem where an agent needs to find the best decisions or actions to take in order to maximize reward over time. Examples include robotics, playing games, autonomous driving, and financial trading. In these domains, reinforcement learning can be used to learn good strategies for decision making even in highly non-linear and non-deterministic environments.\n\nLet's consider a scenario where you are trying to train a reinforcement learning agent to play a game. This is a classic use case for reinforcement learning. \n\nHere's how it works:\n\n1. **Environment:** The environment represents"
  },
  "feature_stats": {
    "indices": [
      0,
      2,
      8581,
      2719,
      1543,
      3600,
      1116,
      4868,
      6660,
      4165,
      941
    ],
    "f_before": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "f_after": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "g_before": [
      0.0,
      0.0,
      0.0,
      0.008792505599558353,
      0.028212301433086395,
      0.0,
      0.0,
      0.01659376546740532,
      0.048490967601537704,
      0.0,
      0.0
    ],
    "g_after": [
      0.0,
      0.0,
      0.0,
      0.008792505599558353,
      0.028212301433086395,
      0.0,
      0.0,
      0.01659376546740532,
      0.048490967601537704,
      0.0,
      0.0
    ],
    "num_batches_for_stats": 10
  },
  "loss_summary": {
    "train_mean_loss": 7.130259060204029,
    "train_mean_l2": 4.420257875844836,
    "train_mean_l1": 3.181838871091604,
    "num_steps": 2000,
    "num_batches": 2000
  },
  "feature_std_summary": {
    "plot_path": "outputs/plots/20251129-054443/feature_std_layer_12.html",
    "mean_std": 0.910323441028595,
    "max_std": 55.617923736572266
  }
}
{
  "model_name": "google/gemma-2-2b-it",
  "layer_idx": 12,
  "command": "main.py experiment.use_multi_contrast=true sae.loss_option=2 sae.loss_module=option2_loss sae.guidance_method=contrastive sae.guidance_coeff=1.0 sae.alpha_concept.code=0.05 sae.alpha_concept.harm=0.05 sae.alpha_concept.struct=0.05 sae.use_l0=true sae.l0_coeff=0.0009132649020605174 sae.role_sep_coeff=0.0035",
  "sae": {
    "input_dim": 2304,
    "sae_dim": 9216,
    "fixed_v_cnt": 3,
    "batch_size": 3,
    "epochs": 2,
    "max_steps": 2000,
    "l1_coeff": 0.005,
    "l0_coeff": 0.0009132649020605174,
    "use_l0": true,
    "concept_samples_per_label": 100,
    "loss_option": 2,
    "loss_module": "option2_loss",
    "concept_feature_indices": {
      "code": 0,
      "harm": 1,
      "struct": 2
    },
    "alpha_concept": {
      "code": 0.05,
      "harm": 0.05,
      "struct": 0.05
    },
    "positive_targets": {
      "code": 1,
      "harm": 1,
      "struct": 1
    },
    "guidance_method": "contrastive",
    "guidance_margin": 0.0,
    "guidance_coeff": 1.0,
    "guidance_mse_pos_value": 1.0,
    "guidance_mse_neg_value": 0.0,
    "jumprelu_bandwidth": 0.001,
    "jumprelu_init_threshold": 0.001,
    "role_sep_coeff": 0.0035
  },
  "gnn": {
    "use_gnn": true,
    "top_k": 10
  },
  "experiment": {
    "device": "cuda",
    "dataset_name": "wikitext",
    "dataset_config": "wikitext-2-raw-v1",
    "use_multi_contrast": true,
    "num_steering_samples_per_label": 10,
    "top_k_for_plot": 10,
    "layer_sweep": [
      12
    ],
    "strength_sweep": [
      15.0
    ]
  },
  "steering": {
    "label": "harm",
    "feature_idx": 1,
    "strength": 15.0
  },
  "prompt": "Describe a typical use case for reinforcement learning\n\nA typical use case for reinforcement learning is a problem where an agent needs to find the best decisions or actions to take in order to maximize reward over time. Examples include robotics, playing games, autonomous driving, and financial trading. In these domains, reinforcement learning can be used to learn good strategies for decision making even in highly non-linear and non-deterministic environments.",
  "output": {
    "before": "Describe a typical use case for reinforcement learning\n\nA typical use case for reinforcement learning is a problem where an agent needs to find the best decisions or actions to take in order to maximize reward over time. Examples include robotics, playing games, autonomous driving, and financial trading. In these domains, reinforcement learning can be used to learn good strategies for decision making even in highly non-linear and non-deterministic environments. \n\nHere's a breakdown of how it works and some specific examples:\n\n**How Reinforcement Learning Works:**\n\n* **Agent:** The learner that interacts with the environment.\n* **Environment:** The world the agent interacts with.\n*",
    "after": "Describe a typical use case for reinforcement learning\n\nA typical use case for reinforcement learning is a problem where an agent needs to find the best decisions or actions to take in order to maximize reward over time. Examples include robotics, playing games, autonomous driving, and financial trading. In these domains, reinforcement learning can be used to learn good strategies for decision making even in highly non-linear and non-deterministic environments. \n\nHere are some examples of how it could be used:\n\n* **Playing a game:** Imagine you are playing a game like chess or checkers. You could use reinforcement learning to learn how to win by making better decisions.\n* **Driving"
  },
  "feature_stats": {
    "indices": [
      1,
      5593,
      8075,
      7819,
      47,
      3279,
      271,
      3950,
      8488,
      6220,
      3680
    ],
    "f_before": [
      108.96385192871094,
      48.96916580200195,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "f_after": [
      108.96385192871094,
      48.96916580200195,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "g_before": [
      5.6097187995910645,
      11.725513458251953,
      11.295454978942871,
      11.403074264526367,
      10.843159675598145,
      0.0,
      0.0,
      10.1063871383667,
      0.03606317937374115,
      9.886300086975098,
      0.0
    ],
    "g_after": [
      5.6097187995910645,
      11.725513458251953,
      11.295454978942871,
      11.403074264526367,
      10.843159675598145,
      0.0,
      0.0,
      10.1063871383667,
      0.03606317937374115,
      9.886300086975098,
      0.0
    ],
    "num_batches_for_stats": 10
  },
  "loss_summary": {
    "train_mean_loss": 7.130259060204029,
    "train_mean_l2": 4.420257875844836,
    "train_mean_l1": 3.181838871091604,
    "num_steps": 2000,
    "num_batches": 2000
  },
  "feature_std_summary": {
    "plot_path": "outputs/plots/20251129-054443/feature_std_layer_12.html",
    "mean_std": 0.910323441028595,
    "max_std": 55.617923736572266
  }
}
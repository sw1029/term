{
  "model_name": "google/gemma-2-2b-it",
  "layer_idx": 12,
  "command": "main.py experiment.use_multi_contrast=true sae.loss_option=2 sae.loss_module=option2_loss sae.guidance_method=contrastive sae.guidance_coeff=1.0 sae.alpha_concept.code=0.07 sae.alpha_concept.harm=0.02 sae.alpha_concept.struct=0.0 sae.use_l0=true sae.l0_coeff=0.0009132649020605174 sae.role_sep_coeff=0.0035",
  "sae": {
    "input_dim": 2304,
    "sae_dim": 9216,
    "fixed_v_cnt": 3,
    "batch_size": 3,
    "epochs": 2,
    "max_steps": 2000,
    "l1_coeff": 0.005,
    "l0_coeff": 0.0009132649020605174,
    "use_l0": true,
    "concept_samples_per_label": 100,
    "loss_option": 2,
    "loss_module": "option2_loss",
    "concept_feature_indices": {
      "code": 0,
      "harm": 1,
      "struct": 2
    },
    "alpha_concept": {
      "code": 0.07,
      "harm": 0.02,
      "struct": 0.0
    },
    "positive_targets": {
      "code": 1,
      "harm": 1,
      "struct": 1
    },
    "guidance_method": "contrastive",
    "guidance_margin": 0.0,
    "guidance_coeff": 1.0,
    "guidance_mse_pos_value": 1.0,
    "guidance_mse_neg_value": 0.0,
    "jumprelu_bandwidth": 0.001,
    "jumprelu_init_threshold": 0.001,
    "role_sep_coeff": 0.0035
  },
  "gnn": {
    "use_gnn": true,
    "top_k": 10
  },
  "experiment": {
    "device": "cuda",
    "dataset_name": "wikitext",
    "dataset_config": "wikitext-2-raw-v1",
    "use_multi_contrast": true,
    "num_steering_samples_per_label": 10,
    "top_k_for_plot": 10,
    "layer_sweep": [
      12
    ],
    "strength_sweep": [
      15.0
    ]
  },
  "steering": {
    "label": "code",
    "feature_idx": 0,
    "strength": 15.0
  },
  "prompt": "# Copyright (c) 2012, GPy authors (see AUTHORS.txt).\n# Licensed under the BSD 3-clause license (see LICENSE.txt)\n\n\nfrom .kern import Kern\nimport numpy as np\nfrom ...core.parameterization import Param\nfrom paramz.transformations import Logexp\nfrom paramz.caching import Cache_this\n\nclass Static(Kern):\n    def __init__(self, input_dim, variance, active_dims, name):\n        super(Static, self).__init__(input_dim, active_dims, name)\n        self.variance = Param('variance', variance, Logexp())\n        self.link_parameters(self.variance)\n\n    def _to_dict(self):\n        input_dict = super(Static, self)._to_dict()\n        input_dict[\"variance\"] =  self.variance.values.tolist()\n        return input_dict\n\n    def Kdiag(self, X):\n        ret = np.empty((X.shape[0],), dtype=np.float64)\n        ret[:] = self.variance\n        return ret\n\n    def gradients_X(self, dL_dK, X, X2=None):\n        return np.zeros(X.shape)\n\n    def gradients_X_diag(self, dL_dKdiag, X):\n        return np.zeros(X.shape)\n\n    def gradients_XX(self, dL_dK, X, X2=None):\n        if X2 is None:\n            X2 = X\n        return np.zeros((X.shape[0], X2.shape[0], X.shape[1], X.shape[1]), dtype=np.float64)\n\n    def gradients_XX_diag(self, dL_dKdiag, X, cov=False):\n        return np.zeros((X.shape[0], X.shape[1], X.shape[1]), dtype=np.float64)\n\n    def gradients_Z_expectations(self, dL_dpsi0, dL_dpsi1, dL_dpsi2, Z, variational_posterior):\n        return np.zeros(Z.shape)\n\n    def gradients_qX_expectations(self, dL_dpsi0, dL_dpsi1, dL_dpsi2, Z, variational_posterior):\n        return np.zeros(variational_posterior.shape), np.zeros(variational_posterior.shape)\n\n    def psi0(self, Z, variational_posterior):\n        return self.Kdiag(variational_posterior.mean)\n\n    def psi1(self, Z, variational_posterior):\n        return self.K(variational_posterior.mean, Z)\n\n    def psi2(self, Z, variational_posterior):\n        K = self.K(variational_posterior.mean, Z)\n        return np.einsum('ij,ik->jk',K,K) #K[:,:,None]*K[:,None,:] # NB. more efficient implementations on inherriting classes\n\n    def input_sensitivity(self, summarize=True):\n        if summarize:\n            return super(Static, self).input_sensitivity(summarize=summarize)\n        else:\n            return np.ones(self.input_dim) * self.variance\n\nclass White(Static):\n    def __init__(self, input_dim, variance=1., active_dims=None, name='white'):\n        super(White, self).__init__(input_dim, variance, active_dims, name)\n\n    def K(self, X, X2=None):\n        if X2 is None:\n            return np.eye(X.shape[0])*self.variance\n        else:\n            return np.zeros((X.shape[0], X2.shape[0]))\n\n    def psi2(self, Z, variational_posterior):\n        return np.zeros((Z.shape[0], Z.shape[0]), dtype=np.float64)\n\n    def psi2n(self, Z, variational_posterior):\n        return np.zeros((1, Z.shape[0], Z.shape[0]), dtype=np.float64)\n\n    def update_gradients_full(self, dL_dK, X, X2=None):\n        if X2 is None:\n            self.variance.gradient = np.trace(dL_dK)\n        else:\n            self.variance.gradient = 0.\n\n    def update_gradients_diag(self, dL_dKdiag, X):\n        self.variance.gradient = dL_dKdiag.sum()\n\n    def update_gradients_expectations(self, dL_dpsi0, dL_dpsi1, dL_dpsi2, Z, variational_posterior):\n        self.variance.gradient = dL_dpsi0.sum()\n\nclass WhiteHeteroscedastic(Static):\n    def __init__(self, input_dim, num_data, variance=1., active_dims=None, name='white_hetero'):\n        \"\"\"\n        A heteroscedastic White kernel (nugget/noise).\n        It defines one variance (nugget) per input sample.\n\n        Prediction excludes any noise learnt by this Kernel, so be careful using this kernel.\n\n        You can plot the errors learnt by this kernel by something similar as:\n        plt.errorbar(m.X, m.Y, yerr=2*np.sqrt(m.kern.white.variance))\n        \"\"\"\n        super(Static, self).__init__(input_dim, active_dims, name)\n        self.variance = Param('variance', np.ones(num_data) * variance, Logexp())\n        self.link_parameters(self.variance)\n\n    def Kdiag(self, X):\n        if X.shape[0] == self.variance.shape[0]:\n            # If the input has the same number of samples as\n            # the number of variances, we return the variances\n            return self.variance\n        return 0.\n\n    def K(self, X, X2=None):\n        if X2 is None and X.shape[0] == self.variance.shape[0]:\n            return np.eye(X.shape[0]) * self.variance\n        else:\n            return 0.\n\n    def psi2(self, Z, variational_posterior):\n        return np.zeros((Z.shape[0], Z.shape[0]), dtype=np.float64)\n\n    def psi2n(self, Z, variational_posterior):\n        return np.zeros((1, Z.shape[0], Z.shape[0]), dtype=np.float64)\n\n    def update_gradients_full(self, dL_dK, X, X2=None):\n        if X2 is None:\n            self.variance.gradient = np.diagonal(dL_dK)\n        else:\n            self.variance.gradient = 0.\n\n    def update_gradients_diag(self, dL_dKdiag, X):\n        self.variance.gradient = dL_dKdiag\n\n    def update_gradients_expectations(self, dL_dpsi0, dL_dpsi1, dL_dpsi2, Z, variational_posterior):\n        self.variance.gradient = dL_dpsi0\n\nclass Bias(Static):\n    def __init__(self, input_dim, variance=1., active_dims=None, name='bias'):\n        super(Bias, self).__init__(input_dim, variance, active_dims, name)\n\n    def to_dict(self):\n        input_dict = super(Bias, self)._to_dict()\n        input_dict[\"class\"] = \"GPy.kern.Bias\"\n        return input_dict\n\n    @staticmethod\n    def _from_dict(kernel_class, input_dict):\n        useGPU = input_dict.pop('useGPU', None)\n        return Bias(**input_dict)\n\n    def K(self, X, X2=None):\n        shape = (X.shape[0], X.shape[0] if X2 is None else X2.shape[0])\n        return np.full(shape, self.variance, dtype=np.float64)\n\n    def update_gradients_full(self, dL_dK, X, X2=None):\n        self.variance.gradient = dL_dK.sum()\n\n    def update_gradients_diag(self, dL_dKdiag, X):\n        self.variance.gradient = dL_dKdiag.sum()\n\n    def psi2(self, Z, variational_posterior):\n        return np.full((Z.shape[0], Z.shape[0]), self.variance*self.variance*variational_posterior.shape[0], dtype=np.float64)\n\n    def psi2n(self, Z, variational_posterior):\n        ret = np.empty((variational_posterior.mean.shape[0], Z.shape[0], Z.shape[0]), dtype=np.float64)\n        ret[:] = self.variance*self.variance\n        return ret\n\n    def update_gradients_expectations(self, dL_dpsi0, dL_dpsi1, dL_dpsi2, Z, variational_posterior):\n        if dL_dpsi2.ndim == 2:\n            self.variance.gradient = (dL_dpsi0.sum() + dL_dpsi1.sum()\n                                    + 2.*self.variance*dL_dpsi2.sum()*variational_posterior.shape[0])\n        else:\n            self.variance.gradient = (dL_dpsi0.sum() + dL_dpsi1.sum()\n                                    + 2.*self.variance*dL_dpsi2.sum())\n\nclass Fixed(Static):\n    def __init__(self, input_dim, covariance_matrix, variance=1., active_dims=None, name='fixed'):\n        \"\"\"\n        :param input_dim: the number of input dimensions\n        :type input_dim: int\n        :param variance: the variance of the kernel\n        :type variance: float\n        \"\"\"\n        super(Fixed, self).__init__(input_dim, variance, active_dims, name)\n        self.fixed_K = covariance_matrix\n    def K(self, X, X2):\n        if X2 is None:\n            return self.variance * self.fixed_K\n        else:\n            return np.zeros((X.shape[0], X2.shape[0]))\n\n    def Kdiag(self, X):\n        return self.variance * self.fixed_K.diagonal()\n\n    def update_gradients_full(self, dL_dK, X, X2=None):\n        if X2 is None:\n            self.variance.gradient = np.einsum('ij,ij', dL_dK, self.fixed_K)\n        else:\n            self.variance.gradient = 0\n\n    def update_gradients_diag(self, dL_dKdiag, X):\n        self.variance.gradient = np.einsum('i,i', dL_dKdiag, np.diagonal(self.fixed_K))\n\n    def psi2(self, Z, variational_posterior):\n        return np.zeros((Z.shape[0], Z.shape[0]), dtype=np.float64)\n\n    def psi2n(self, Z, variational_posterior):\n        return np.zeros((1, Z.shape[0], Z.shape[0]), dtype=np.float64)\n\n    def update_gradients_expectations(self, dL_dpsi0, dL_dpsi1, dL_dpsi2, Z, variational_posterior):\n        self.variance.gradient = dL_dpsi0.sum()\n\nclass Precomputed(Fixed):\n    def __init__(self, input_dim, covariance_matrix, variance=1., active_dims=None, name='precomputed'):\n        \"\"\"\n        Class for precomputed kernels, indexed by columns in X\n\n        Usage example:\n\n        import numpy as np\n        from GPy.models import GPClassification\n        from GPy.kern import Precomputed\n        from sklearn.cross_validation import LeaveOneOut\n\n        n = 10\n        d = 100\n        X = np.arange(n).reshape((n,1))         # column vector of indices\n        y = 2*np.random.binomial(1,0.5,(n,1))-1\n        X0 = np.random.randn(n,d)\n        k = np.dot(X0,X0.T)\n        kern = Precomputed(1,k)                 # k is a n x n covariance matrix\n\n        cv = LeaveOneOut(n)\n        ypred = y.copy()\n        for train, test in cv:\n            m = GPClassification(X[train], y[train], kernel=kern)\n            m.optimize()\n            ypred[test] = 2*(m.predict(X[test])[0]>0.5)-1\n\n        :param input_dim: the number of input dimensions\n        :type input_dim: int\n        :param variance: the variance of the kernel\n        :type variance: float\n        \"\"\"\n        assert input_dim==1, \"Precomputed only implemented in one dimension. Use multiple Precomputed kernels to have more dimensions by making use of active_dims\"\n        super(Precomputed, self).__init__(input_dim, covariance_matrix, variance, active_dims, name)\n\n    @Cache_this(limit=2)\n    def _index(self, X, X2):\n        if X2 is None:\n            i1 = i2 = X.astype('int').flat\n        else:\n            i1, i2 = X.astype('int').flat, X2.astype('int').flat\n        return self.fixed_K[i1,:][:,i2]\n\n    def K(self, X, X2=None):\n        return self.variance * self._index(X, X2)\n\n    def Kdiag(self, X):\n        return self.variance * self._index(X,None).diagonal()\n\n    def update_gradients_full(self, dL_dK, X, X2=None):\n        self.variance.gradient = np.einsum('ij,ij', dL_dK, self._index(X, X2))\n\n    def update_gradients_diag(self, dL_dKdiag, X):\n        self.variance.gradient = np.einsum('i,ii', dL_dKdiag, self._index(X, None))\n",
  "output": {
    "before": "# Copyright (c) 2012, GPy authors (see AUTHORS.txt).\n# Licensed under the BSD 3-clause license (see LICENSE.txt)\n\n\nfrom .kern import Kern\nimport numpy as np\nfrom ...core.parameterization import Param\nfrom paramz.transformations import Logexp\nfrom paramz.caching import Cache_this\n\nclass Static(Kern):\n    def __init__(self, input_dim, variance, active_dims, name):\n        super(Static, self).__init__(input_dim, active_dims, name)\n        self.variance = Param('variance', variance, Logexp())\n        self.link_parameters(self.variance)\n\n    def _to_dict(self):\n        input_dict = super(Static, self)._to_dict()\n        input_dict[\"variance\"] =  self.variance.values.tolist()\n        return input_dict\n\n    def Kdiag(self, X):\n        ret = np.empty((X.shape[0],), dtype=np.float64)\n        ret[:] = self.variance\n        return ret\n\n    def gradients_X(self, dL_dK, X, X2=None):\n        return np.zeros(X.shape)\n\n    def gradients_X_diag(self, dL_dKdiag, X):\n        return np.zeros(X.shape)\n\n    def gradients_XX(self, dL_dK, X, X2=None):\n        if X2 is None:\n            X2 = X\n        return np.zeros((X.shape[0], X2.shape[0], X.shape[1], X.shape[1]), dtype=np.float64)\n\n    def gradients_XX_diag(self, dL_dKdiag, X, cov=False):\n        return np.zeros((X.shape[0], X.shape[1], X.shape[1]), dtype=np.float64)\n\n    def gradients_Z_expectations(self, dL_dpsi0, dL_dpsi1, dL_dpsi2, Z, variational_posterior):\n        return np.zeros(Z.shape)\n\n    def gradients_qX_expectations(self, dL_dpsi0, dL_dpsi1, dL_dpsi2, Z, variational_posterior):\n        return np.zeros(variational_posterior.shape), np.zeros(variational_posterior.shape)\n\n    def psi0(self, Z, variational_posterior):\n        return self.Kdiag(variational_posterior.mean)\n\n    def psi1(self, Z, variational_posterior):\n        return self.K(variational_posterior.mean, Z)\n\n    def psi2(self, Z, variational_posterior):\n        K = self.K(variational_posterior.mean, Z)\n        return np.einsum('ij,ik->jk',K,K) #K[:,:,None]*K[:,None,:] # NB. more efficient implementations on inherriting classes\n\n    def input_sensitivity(self, summarize=True):\n        if summarize:\n            return super(Static, self).input_sensitivity(summarize=summarize)\n        else:\n            return np.ones(self.input_dim) * self.variance\n\nclass White(Static):\n    def __init__(self, input_dim, variance=1., active_dims=None, name='white'):\n        super(White, self).__init__(input_dim, variance, active_dims, name)\n\n    def K(self, X, X2=None):\n        if X2 is None:\n            return np.eye(X.shape[0])*self.variance\n        else:\n            return np.zeros((X.shape[0], X2.shape[0]))\n\n    def psi2(self, Z, variational_posterior):\n        return np.zeros((Z.shape[0], Z.shape[0]), dtype=np.float64)\n\n    def psi2n(self, Z, variational_posterior):\n        return np.zeros((1, Z.shape[0], Z.shape[0]), dtype=np.float64)\n\n    def update_gradients_full(self, dL_dK, X, X2=None):\n        if X2 is None:\n            self.variance.gradient = np.trace(dL_dK)\n        else:\n            self.variance.gradient = 0.\n\n    def update_gradients_diag(self, dL_dKdiag, X):\n        self.variance.gradient = dL_dKdiag.sum()\n\n    def update_gradients_expectations(self, dL_dpsi0, dL_dpsi1, dL_dpsi2, Z, variational_posterior):\n        self.variance.gradient = dL_dpsi0.sum()\n\nclass WhiteHeteroscedastic(Static):\n    def __init__(self, input_dim, num_data, variance=1., active_dims=None, name='white_hetero'):\n        \"\"\"\n        A heteroscedastic White kernel (nugget/noise).\n        It defines one variance (nugget) per input sample.\n\n        Prediction excludes any noise learnt by this Kernel, so be careful using this kernel.\n\n        You can plot the errors learnt by this kernel by something similar as:\n        plt.errorbar(m.X, m.Y, yerr=2*np.sqrt(m.kern.white.variance))\n        \"\"\"\n        super(Static, self).__init__(input_dim, active_dims, name)\n        self.variance = Param('variance', np.ones(num_data) * variance, Logexp())\n        self.link_parameters(self.variance)\n\n    def Kdiag(self, X):\n        if X.shape[0] == self.variance.shape[0]:\n            # If the input has the same number of samples as\n            # the number of variances, we return the variances\n            return self.variance\n        return 0.\n\n    def K(self, X, X2=None):\n        if X2 is None and X.shape[0] == self.variance.shape[0]:\n            return np.eye(X.shape[0]) * self.variance\n        else:\n            return 0.\n\n    def psi2(self, Z, variational_posterior):\n        return np.zeros((Z.shape[0], Z.shape[0]), dtype=np.float64)\n\n    def psi2n(self, Z, variational_posterior):\n        return np.zeros((1, Z.shape[0], Z.shape[0]), dtype=np.float64)\n\n    def update_gradients_full(self, dL_dK, X, X2=None):\n        if X2 is None:\n            self.variance.gradient = np.diagonal(dL_dK)\n        else:\n            self.variance.gradient = 0.\n\n    def update_gradients_diag(self, dL_dKdiag, X):\n        self.variance.gradient = dL_dKdiag\n\n    def update_gradients_expectations(self, dL_dpsi0, dL_dpsi1, dL_dpsi2, Z, variational_posterior):\n        self.variance.gradient = dL_dpsi0\n\nclass Bias(Static):\n    def __init__(self, input_dim, variance=1., active_dims=None, name='bias'):\n        super(Bias, self).__init__(input_dim, variance, active_dims, name)\n\n    def to_dict(self):\n        input_dict = super(Bias, self)._to_dict()\n        input_dict[\"class\"] = \"GPy.kern.Bias\"\n        return input_dict\n\n    @staticmethod\n    def _from_dict(kernel_class, input_dict):\n        useGPU = input_dict.pop('useGPU', None)\n        return Bias(**input_dict)\n\n    def K(self, X, X2=None):\n        shape = (X.shape[0], X.shape[0] if X2 is None else X2.shape[0])\n        return np.full(shape, self.variance, dtype=np.float64)\n\n    def update_gradients_full(self, dL_dK, X, X2=None):\n        self.variance.gradient = dL_dK.sum()\n\n    def update_gradients_diag(self, dL_dKdiag, X):\n        self.variance.gradient = dL_dKdiag.sum()\n\n    def psi2(self, Z, variational_posterior):\n        return np.full((Z.shape[0], Z.shape[0]), self.variance*self.variance*variational_posterior.shape[0], dtype=np.float64)\n\n    def psi2n(self, Z, variational_posterior):\n        ret = np.empty((variational_posterior.mean.shape[0], Z.shape[0], Z.shape[0]), dtype=np.float64)\n        ret[:] = self.variance*self.variance\n        return ret\n\n    def update_gradients_expectations(self, dL_dpsi0, dL_dpsi1, dL_dpsi2, Z, variational_posterior):\n        if dL_dpsi2.ndim == 2:\n            self.variance.gradient = (dL_dpsi0.sum() + dL_dpsi1.sum()\n                                    + 2.*self.variance*dL_dpsi2.sum()*variational_posterior.shape[0])\n        else:\n            self.variance.gradient = (dL_dpsi0.sum() + dL_dpsi1.sum()\n                                    + 2.*self.variance*dL_dpsi2.sum())\n\nclass Fixed(Static):\n    def __init__(self, input_dim, covariance_matrix, variance=1., active_dims=None, name='fixed'):\n        \"\"\"\n        :param input_dim: the number of input dimensions\n        :type input_dim: int\n        :param variance: the variance of the kernel\n        :type variance: float\n        \"\"\"\n        super(Fixed, self).__init__(input_dim, variance, active_dims, name)\n        self.fixed_K = covariance_matrix\n    def K(self, X, X2):\n        if X2 is None:\n            return self.variance * self.fixed_K\n        else:\n            return np.zeros((X.shape[0], X2.shape[0]))\n\n    def Kdiag(self, X):\n        return self.variance * self.fixed_K.diagonal()\n\n    def update_gradients_full(self, dL_dK, X, X2=None):\n        if X2 is None:\n            self.variance.gradient = np.einsum('ij,ij', dL_dK, self.fixed_K)\n        else:\n            self.variance.gradient = 0\n\n    def update_gradients_diag(self, dL_dKdiag, X):\n        self.variance.gradient = np.einsum('i,i', dL_dKdiag, np.diagonal(self.fixed_K))\n\n    def psi2(self, Z, variational_posterior):\n        return np.zeros((Z.shape[0], Z.shape[0]), dtype=np.float64)\n\n    def psi2n(self, Z, variational_posterior):\n        return np.zeros((1, Z.shape[0], Z.shape[0]), dtype=np.float64)\n\n    def update_gradients_expectations(self, dL_dpsi0, dL_dpsi1, dL_dpsi2, Z, variational_posterior):\n        self.variance.gradient = dL_dpsi0.sum()\n\nclass Precomputed(Fixed):\n    def __init__(self, input_dim, covariance_matrix, variance=1., active_dims=None, name='precomputed'):\n        \"\"\"\n        Class for precomputed kernels, indexed by columns in X\n\n        Usage example:\n\n        import numpy as np\n        from GPy.models import GPClassification\n        from GPy.kern import Precomputed\n        from sklearn.cross_validation import LeaveOneOut\n\n        n = 10\n        d = 100\n        X = np.arange(n).reshape((n,1))         # column vector of indices\n        y = 2*np.random.binomial(1,0.5,(n,1))-1\n        X0 = np.random.randn(n,d)\n        k = np.dot(X0,X0.T)\n        kern = Precomputed(1,k)                 # k is a n x n covariance matrix\n\n        cv = LeaveOneOut(n)\n        ypred = y.copy()\n        for train, test in cv:\n            m = GPClassification(X[train], y[train], kernel=kern)\n            m.optimize()\n            ypred[test] = 2*(m.predict(X[test])[0]>0.5)-1\n\n        :param input_dim: the number of input dimensions\n        :type input_dim: int\n        :param variance: the variance of the kernel\n        :type variance: float\n        \"\"\"\n        assert input_dim==1, \"Precomputed only implemented in one dimension. Use multiple Precomputed kernels to have more dimensions by making use of active_dims\"\n        super(Precomputed, self).__init__(input_dim, covariance_matrix, variance, active_dims, name)\n\n    @Cache_this(limit=2)\n    def _index(self, X, X2):\n        if X2 is None:\n            i1 = i2 = X.astype('int').flat\n        else:\n            i1, i2 = X.astype('int').flat, X2.astype('int').flat\n        return self.fixed_K[i1,:][:,i2]\n\n    def K(self, X, X2=None):\n        return self.variance * self._index(X, X2)\n\n    def Kdiag(self, X):\n        return self.variance * self._index(X,None).diagonal()\n\n    def update_gradients_full(self, dL_dK, X, X2=None):\n        self.variance.gradient = np.einsum('ij,ij', dL_dK, self._index(X, X2))\n\n    def update_gradients_diag(self, dL_dKdiag, X):\n        self.variance.gradient = np.einsum('i,ii', dL_dKdiag, self._index(X, None))\n    \n\n    def update_gradients_expectations(self, dL_dpsi0, dL_dpsi1, dL_dpsi2, Z, variational_posterior):\n        self.variance.gradient = dL_",
    "after": "# Copyright (c) 2012, GPy authors (see AUTHORS.txt).\n# Licensed under the BSD 3-clause license (see LICENSE.txt)\n\n\nfrom .kern import Kern\nimport numpy as np\nfrom ...core.parameterization import Param\nfrom paramz.transformations import Logexp\nfrom paramz.caching import Cache_this\n\nclass Static(Kern):\n    def __init__(self, input_dim, variance, active_dims, name):\n        super(Static, self).__init__(input_dim, active_dims, name)\n        self.variance = Param('variance', variance, Logexp())\n        self.link_parameters(self.variance)\n\n    def _to_dict(self):\n        input_dict = super(Static, self)._to_dict()\n        input_dict[\"variance\"] =  self.variance.values.tolist()\n        return input_dict\n\n    def Kdiag(self, X):\n        ret = np.empty((X.shape[0],), dtype=np.float64)\n        ret[:] = self.variance\n        return ret\n\n    def gradients_X(self, dL_dK, X, X2=None):\n        return np.zeros(X.shape)\n\n    def gradients_X_diag(self, dL_dKdiag, X):\n        return np.zeros(X.shape)\n\n    def gradients_XX(self, dL_dK, X, X2=None):\n        if X2 is None:\n            X2 = X\n        return np.zeros((X.shape[0], X2.shape[0], X.shape[1], X.shape[1]), dtype=np.float64)\n\n    def gradients_XX_diag(self, dL_dKdiag, X, cov=False):\n        return np.zeros((X.shape[0], X.shape[1], X.shape[1]), dtype=np.float64)\n\n    def gradients_Z_expectations(self, dL_dpsi0, dL_dpsi1, dL_dpsi2, Z, variational_posterior):\n        return np.zeros(Z.shape)\n\n    def gradients_qX_expectations(self, dL_dpsi0, dL_dpsi1, dL_dpsi2, Z, variational_posterior):\n        return np.zeros(variational_posterior.shape), np.zeros(variational_posterior.shape)\n\n    def psi0(self, Z, variational_posterior):\n        return self.Kdiag(variational_posterior.mean)\n\n    def psi1(self, Z, variational_posterior):\n        return self.K(variational_posterior.mean, Z)\n\n    def psi2(self, Z, variational_posterior):\n        K = self.K(variational_posterior.mean, Z)\n        return np.einsum('ij,ik->jk',K,K) #K[:,:,None]*K[:,None,:] # NB. more efficient implementations on inherriting classes\n\n    def input_sensitivity(self, summarize=True):\n        if summarize:\n            return super(Static, self).input_sensitivity(summarize=summarize)\n        else:\n            return np.ones(self.input_dim) * self.variance\n\nclass White(Static):\n    def __init__(self, input_dim, variance=1., active_dims=None, name='white'):\n        super(White, self).__init__(input_dim, variance, active_dims, name)\n\n    def K(self, X, X2=None):\n        if X2 is None:\n            return np.eye(X.shape[0])*self.variance\n        else:\n            return np.zeros((X.shape[0], X2.shape[0]))\n\n    def psi2(self, Z, variational_posterior):\n        return np.zeros((Z.shape[0], Z.shape[0]), dtype=np.float64)\n\n    def psi2n(self, Z, variational_posterior):\n        return np.zeros((1, Z.shape[0], Z.shape[0]), dtype=np.float64)\n\n    def update_gradients_full(self, dL_dK, X, X2=None):\n        if X2 is None:\n            self.variance.gradient = np.trace(dL_dK)\n        else:\n            self.variance.gradient = 0.\n\n    def update_gradients_diag(self, dL_dKdiag, X):\n        self.variance.gradient = dL_dKdiag.sum()\n\n    def update_gradients_expectations(self, dL_dpsi0, dL_dpsi1, dL_dpsi2, Z, variational_posterior):\n        self.variance.gradient = dL_dpsi0.sum()\n\nclass WhiteHeteroscedastic(Static):\n    def __init__(self, input_dim, num_data, variance=1., active_dims=None, name='white_hetero'):\n        \"\"\"\n        A heteroscedastic White kernel (nugget/noise).\n        It defines one variance (nugget) per input sample.\n\n        Prediction excludes any noise learnt by this Kernel, so be careful using this kernel.\n\n        You can plot the errors learnt by this kernel by something similar as:\n        plt.errorbar(m.X, m.Y, yerr=2*np.sqrt(m.kern.white.variance))\n        \"\"\"\n        super(Static, self).__init__(input_dim, active_dims, name)\n        self.variance = Param('variance', np.ones(num_data) * variance, Logexp())\n        self.link_parameters(self.variance)\n\n    def Kdiag(self, X):\n        if X.shape[0] == self.variance.shape[0]:\n            # If the input has the same number of samples as\n            # the number of variances, we return the variances\n            return self.variance\n        return 0.\n\n    def K(self, X, X2=None):\n        if X2 is None and X.shape[0] == self.variance.shape[0]:\n            return np.eye(X.shape[0]) * self.variance\n        else:\n            return 0.\n\n    def psi2(self, Z, variational_posterior):\n        return np.zeros((Z.shape[0], Z.shape[0]), dtype=np.float64)\n\n    def psi2n(self, Z, variational_posterior):\n        return np.zeros((1, Z.shape[0], Z.shape[0]), dtype=np.float64)\n\n    def update_gradients_full(self, dL_dK, X, X2=None):\n        if X2 is None:\n            self.variance.gradient = np.diagonal(dL_dK)\n        else:\n            self.variance.gradient = 0.\n\n    def update_gradients_diag(self, dL_dKdiag, X):\n        self.variance.gradient = dL_dKdiag\n\n    def update_gradients_expectations(self, dL_dpsi0, dL_dpsi1, dL_dpsi2, Z, variational_posterior):\n        self.variance.gradient = dL_dpsi0\n\nclass Bias(Static):\n    def __init__(self, input_dim, variance=1., active_dims=None, name='bias'):\n        super(Bias, self).__init__(input_dim, variance, active_dims, name)\n\n    def to_dict(self):\n        input_dict = super(Bias, self)._to_dict()\n        input_dict[\"class\"] = \"GPy.kern.Bias\"\n        return input_dict\n\n    @staticmethod\n    def _from_dict(kernel_class, input_dict):\n        useGPU = input_dict.pop('useGPU', None)\n        return Bias(**input_dict)\n\n    def K(self, X, X2=None):\n        shape = (X.shape[0], X.shape[0] if X2 is None else X2.shape[0])\n        return np.full(shape, self.variance, dtype=np.float64)\n\n    def update_gradients_full(self, dL_dK, X, X2=None):\n        self.variance.gradient = dL_dK.sum()\n\n    def update_gradients_diag(self, dL_dKdiag, X):\n        self.variance.gradient = dL_dKdiag.sum()\n\n    def psi2(self, Z, variational_posterior):\n        return np.full((Z.shape[0], Z.shape[0]), self.variance*self.variance*variational_posterior.shape[0], dtype=np.float64)\n\n    def psi2n(self, Z, variational_posterior):\n        ret = np.empty((variational_posterior.mean.shape[0], Z.shape[0], Z.shape[0]), dtype=np.float64)\n        ret[:] = self.variance*self.variance\n        return ret\n\n    def update_gradients_expectations(self, dL_dpsi0, dL_dpsi1, dL_dpsi2, Z, variational_posterior):\n        if dL_dpsi2.ndim == 2:\n            self.variance.gradient = (dL_dpsi0.sum() + dL_dpsi1.sum()\n                                    + 2.*self.variance*dL_dpsi2.sum()*variational_posterior.shape[0])\n        else:\n            self.variance.gradient = (dL_dpsi0.sum() + dL_dpsi1.sum()\n                                    + 2.*self.variance*dL_dpsi2.sum())\n\nclass Fixed(Static):\n    def __init__(self, input_dim, covariance_matrix, variance=1., active_dims=None, name='fixed'):\n        \"\"\"\n        :param input_dim: the number of input dimensions\n        :type input_dim: int\n        :param variance: the variance of the kernel\n        :type variance: float\n        \"\"\"\n        super(Fixed, self).__init__(input_dim, variance, active_dims, name)\n        self.fixed_K = covariance_matrix\n    def K(self, X, X2):\n        if X2 is None:\n            return self.variance * self.fixed_K\n        else:\n            return np.zeros((X.shape[0], X2.shape[0]))\n\n    def Kdiag(self, X):\n        return self.variance * self.fixed_K.diagonal()\n\n    def update_gradients_full(self, dL_dK, X, X2=None):\n        if X2 is None:\n            self.variance.gradient = np.einsum('ij,ij', dL_dK, self.fixed_K)\n        else:\n            self.variance.gradient = 0\n\n    def update_gradients_diag(self, dL_dKdiag, X):\n        self.variance.gradient = np.einsum('i,i', dL_dKdiag, np.diagonal(self.fixed_K))\n\n    def psi2(self, Z, variational_posterior):\n        return np.zeros((Z.shape[0], Z.shape[0]), dtype=np.float64)\n\n    def psi2n(self, Z, variational_posterior):\n        return np.zeros((1, Z.shape[0], Z.shape[0]), dtype=np.float64)\n\n    def update_gradients_expectations(self, dL_dpsi0, dL_dpsi1, dL_dpsi2, Z, variational_posterior):\n        self.variance.gradient = dL_dpsi0.sum()\n\nclass Precomputed(Fixed):\n    def __init__(self, input_dim, covariance_matrix, variance=1., active_dims=None, name='precomputed'):\n        \"\"\"\n        Class for precomputed kernels, indexed by columns in X\n\n        Usage example:\n\n        import numpy as np\n        from GPy.models import GPClassification\n        from GPy.kern import Precomputed\n        from sklearn.cross_validation import LeaveOneOut\n\n        n = 10\n        d = 100\n        X = np.arange(n).reshape((n,1))         # column vector of indices\n        y = 2*np.random.binomial(1,0.5,(n,1))-1\n        X0 = np.random.randn(n,d)\n        k = np.dot(X0,X0.T)\n        kern = Precomputed(1,k)                 # k is a n x n covariance matrix\n\n        cv = LeaveOneOut(n)\n        ypred = y.copy()\n        for train, test in cv:\n            m = GPClassification(X[train], y[train], kernel=kern)\n            m.optimize()\n            ypred[test] = 2*(m.predict(X[test])[0]>0.5)-1\n\n        :param input_dim: the number of input dimensions\n        :type input_dim: int\n        :param variance: the variance of the kernel\n        :type variance: float\n        \"\"\"\n        assert input_dim==1, \"Precomputed only implemented in one dimension. Use multiple Precomputed kernels to have more dimensions by making use of active_dims\"\n        super(Precomputed, self).__init__(input_dim, covariance_matrix, variance, active_dims, name)\n\n    @Cache_this(limit=2)\n    def _index(self, X, X2):\n        if X2 is None:\n            i1 = i2 = X.astype('int').flat\n        else:\n            i1, i2 = X.astype('int').flat, X2.astype('int').flat\n        return self.fixed_K[i1,:][:,i2]\n\n    def K(self, X, X2=None):\n        return self.variance * self._index(X, X2)\n\n    def Kdiag(self, X):\n        return self.variance * self._index(X,None).diagonal()\n\n    def update_gradients_full(self, dL_dK, X, X2=None):\n        self.variance.gradient = np.einsum('ij,ij', dL_dK, self._index(X, X2))\n\n    def update_gradients_diag(self, dL_dKdiag, X):\n        self.variance.gradient = np.einsum('i,ii', dL_dKdiag, self._index(X, None))\n```\n```python\nfrom GPy.kern import Kern\nfrom GPy.core.parameterization import Param\nfrom GPy.core.utils import _import_module\n\nclass Static(Kern):\n    def __init__(self,"
  },
  "feature_stats": {
    "indices": [
      0,
      2,
      8706,
      1543,
      5492,
      5982,
      4303,
      6816,
      6799,
      6342,
      752
    ],
    "f_before": [
      108.89669799804688,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "f_after": [
      108.89669799804688,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "g_before": [
      0.0,
      64.34852600097656,
      12.472809791564941,
      11.327054023742676,
      10.997279167175293,
      11.026681900024414,
      11.051050186157227,
      10.419659614562988,
      10.48941707611084,
      9.984222412109375,
      10.061835289001465
    ],
    "g_after": [
      0.0,
      64.34852600097656,
      12.472809791564941,
      11.327054023742676,
      10.997279167175293,
      11.026681900024414,
      11.051050186157227,
      10.419659614562988,
      10.48941707611084,
      9.984222412109375,
      10.061835289001465
    ],
    "num_batches_for_stats": 10
  },
  "loss_summary": {
    "train_mean_loss": 5.431506278693676,
    "train_mean_l2": 4.804488041855395,
    "train_mean_l1": 3.246077475249767,
    "num_steps": 2000,
    "num_batches": 2000
  },
  "feature_std_summary": {
    "plot_path": "outputs/plots/20251129-035246/feature_std_layer_12.html",
    "mean_std": 1.9125933647155762,
    "max_std": 75.5639877319336
  }
}
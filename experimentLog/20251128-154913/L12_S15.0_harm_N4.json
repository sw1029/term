{
  "model_name": "google/gemma-2-2b-it",
  "layer_idx": 12,
  "command": "python main.py experiment.use_multi_contrast=true sae.loss_option=2 sae.loss_module=option2_loss sae.guidance_method=contrastive sae.guidance_coeff=1.0 sae.alpha_concept.code=0.07 sae.alpha_concept.harm=0.05 sae.alpha_concept.struct=0.0 sae.use_l0=true sae.l0_coeff=0.00019887451801110898 sae.role_sep_coeff=0.002",
  "sae": {
    "input_dim": 2304,
    "sae_dim": 9216,
    "fixed_v_cnt": 3,
    "batch_size": 3,
    "epochs": 2,
    "max_steps": 2000,
    "l1_coeff": 0.005,
    "l0_coeff": 0.00019887451801110898,
    "use_l0": true,
    "concept_samples_per_label": 100,
    "loss_option": 2,
    "loss_module": "option2_loss",
    "concept_feature_indices": {
      "code": 0,
      "harm": 1,
      "struct": 2
    },
    "alpha_concept": {
      "code": 0.07,
      "harm": 0.05,
      "struct": 0.0
    },
    "positive_targets": {
      "code": 1,
      "harm": 1,
      "struct": 1
    },
    "guidance_method": "contrastive",
    "guidance_margin": 0.0,
    "guidance_coeff": 1.0,
    "guidance_mse_pos_value": 1.0,
    "guidance_mse_neg_value": 0.0,
    "jumprelu_bandwidth": 0.001,
    "jumprelu_init_threshold": 0.001,
    "role_sep_coeff": 0.002
  },
  "gnn": {
    "use_gnn": true,
    "top_k": 10
  },
  "experiment": {
    "device": "cuda",
    "dataset_name": "wikitext",
    "dataset_config": "wikitext-2-raw-v1",
    "use_multi_contrast": true,
    "num_steering_samples_per_label": 10,
    "top_k_for_plot": 10,
    "layer_sweep": [
      12
    ],
    "strength_sweep": [
      15.0
    ]
  },
  "steering": {
    "label": "harm",
    "feature_idx": 1,
    "strength": 15.0,
    "hook_layer_idx": 11
  },
  "prompt": "###########################################################################################\n#   Created by Jason Downing                                                              #\n#   Some code originally found at this Stackoverflow Post:                                #\n#   https://stackoverflow.com/questions/18966368/python-beautifulsoup-scrape-tables       #\n#   Also this page as well:                                                               #\n#   http://www.pythonforbeginners.com/python-on-the-web/web-scraping-with-beautifulsoup/  #\n#                                                                                         #\n#   Copyright 2016 Jason Downing                                                          #                 ",
  "output": {
    "before": "###########################################################################################\n#   Created by Jason Downing                                                              #\n#   Some code originally found at this Stackoverflow Post:                                #\n#   https://stackoverflow.com/questions/18966368/python-beautifulsoup-scrape-tables       #\n#   Also this page as well:                                                               #\n#   http://www.pythonforbeginners.com/python-on-the-web/web-scraping-with-beautifulsoup/  #\n#                                                                                         #\n#   Copyright 2016 Jason Downing                                                          #                 #\n#   MIT LICENSED - DO WHATEVER YOU WANT WITH THIS FILE.                                   #\n###########################################################################################\n\n# To setup urllib2 / bs4 (BeautifulSoup)\n# Follow this URL: http://linuxconfig.org/how-to-install-python3-beautiful-soup-environment-on-debian-linux\n# and run this command: pip install requests\n\nimport json\nimport requests\nfrom bs4 import BeautifulSoup\n\n# 6 URLs to scrap for lift / trail data.\n# Order is: Waterville Valley, Cannon Mt, Bretton Woods, Loon Mt & Cranmore Mt\nurls = [\"http://www.waterville.com/ski-ride/snow-report.html\",\n        \"http://cannonmt.com/trail-lift-report.html\",\n        \"http://brettonwoods.com/alpine_trails/trail_report#top\",\n        \"http://www.loonmtn.com/explore/snow-conditions/trail-lift-report\",\n        \"http://www.cranmore.com/winter/snow-grooming-report\",\n        \"http://www.patspeak.com/snow_report.php\"]\n\nmountains = [\"Waterville Valley\", \"Cannon Mt\", \"Bretton Woods\",\n             \"Loon Mt\", \"Cranmore Mt\", \"Pats Peak\"]\n\n# global JSON object to write only once.\nJSON_trails = {}\n\n# Waterville Valley\ndef waterville():\n  print (\"DONE\\n\")\n\n  open_trails = []\n  closed_trails = []\n\n  # Get the page, then grab just the text and use BeautifulSoup to work some magic on it.\n  page = requests.get(urls[0])\n  data = page.text\n  soup = BeautifulSoup(data, \"lxml\")\n\n  # Get an entire div.\n  ski_data = soup.findAll('div', {'class' : 'tabset_content'})\n\n  # Let's get all open trails.\n  for each_div in soup.findAll('li', {'class' : 'open'}):\n    open_trails.append(each_div.text)\n\n  # Also all closed trails.\n  for each_div in soup.findAll('li', {'class' : 'closed'}):\n    closed_trails.append(each_div.text)\n\n  # Dump to trails object.\n  JSON_trails['waterville_open'] = open_trails\n  JSON_trails['waterville_closed'] = closed_trails\n\n# Cannon Mt\ndef cannon():\n  print (\"DONE\\n\")\n\n  trail_list = []\n  trail_status = []\n  open_trails = []\n  closed_trails = []\n\n  # Get the page, then grab just the text and use BeautifulSoup to work some magic on it.\n  page = requests.get(urls[1])\n  data = page.text\n  soup = BeautifulSoup(data, \"lxml\")\n\n  # Get lift status\n  # From stackoverflow:\n  # https://stackoverflow.com/questions/13074586/extracting-selected-columns-from-a-table-using-beautifulsoup\n  tables = soup.find('table')    # change this for consistent code.\n\n  rows = tables.findAll('tr')\n  for cells in rows:\n    cell = cells.findAll('td')\n    trail_list.append(cell[0].text)\n    trail_status.append(cell[1].text)\n\n  # Get trail status\n  # THIS TRICK COMES FROM STACKOVERFLOW!\n  # https://stackoverflow.com/questions/14095511/beautifulsoup-in-python-getting-the-n-th-tag-of-a-type\n  tables = soup.findAll('table')[1]\n\n  rows = tables.findAll('tr')\n  for cells in rows:\n    if (len(cells) == 4):\n      cell = cells.findAll('td')\n      trail_list.append(cell[0].text)\n      trail_status.append(cell[1].text)\n\n  # # Print for debugging purposes.\n  # print (\"Trails: \\n\")\n\n  # for trail in trail_list:\n  #   print (trail)\n\n  # print (\"Status: \\n\")\n\n  # for status in trail_status:\n  #   print (status)\n\n  # Now let's figure out open / closed status for trails!\n  list_length = len(trail_list)\n\n  for a in range(list_length):\n    if (trail_status[a] == 'Open'):\n      open_trails.append(trail_list[a])\n    else:\n      closed_trails.append(trail_list[a])\n\n\n  # Dump to trails object.\n  JSON_trails['cannon_open'] = open_trails\n  JSON_trails['cannon_closed'] = closed_trails\n\n# Bretton Woods\ndef bretton_woods():\n  print (\"DONE\\n\")\n\n  trail_list = []       # List of all the trails, in order on the page.\n  trail_status = []     # List of trail status, in order on the page.\n  open_trails = []      # All the open trails or lifts\n  closed_trails = []    # All the closed trails or lifts\n\n  open_src = '/images/icons/open-sm.png'\n\n  # Get the page, then grab just the text and use BeautifulSoup to work some magic on it.\n  page = requests.get(urls[2])\n  data = page.text\n  soup = BeautifulSoup(data, \"lxml\")\n\n  # Get an entire div.\n  ski_data = soup.findAll('div', {'id' : 'trail-content'})\n\n  # Using this Stackoverflow post to figure out how to get the text I need.\n  # https://stackoverflow.com/questions/13202087/beautiful-soup-find-children-for-particular-div\n  for tag in ski_data:\n    tab = tag.findAll('div', {'class': 'trails-report'})\n    for tag2 in tab:\n      trail_list.append(tag2.text)      # This gets all the trails by name.\n\n    # Now to get trail conditions\n    tab = tag.findAll('div', {'class': 'condition'})\n    for img in tab:\n      img_src = img.findAll('img')[0].get('src')  # This gets the trail status (by image source)\n      trail_status.append(img_src)\n\n  # Now let's figure out open / closed status for trails!\n  list_length = len(trail_list)\n\n  for a in range(list_length):\n    if (trail_status[a] == open_src):\n      open_trails.append(trail_list[a])\n    else:\n      closed_trails.append(trail_list[a])\n\n  # Dump to trails object.\n  JSON_trails['bretton_woods_open'] = open_trails\n  JSON_trails['bretton_woods_closed'] = closed_trails\n\n# Loon Mt\ndef loon():\n  trail_list = []       # List of all the trails, in order on the page.\n  trail_status = []     # List of trail status, in order on the page.\n  open_trails = []      # All the open trails or lifts\n  closed_trails = []    # All the closed trails or lifts\n\n  # Get the page, then grab just the text and use BeautifulSoup to work some magic on it.\n  page = requests.get(urls[3])\n  data = page.text\n  soup = BeautifulSoup(data, \"lxml\")\n\n  lifts = soup.findAll(\"table\", {\"class\": \"lift-status\"})\n\n  titles_html = []\n\n  open_src = \"/assets/prebuilt/img/template/small-green-checkmark.png\"\n  closed_src = \"/assets/prebuilt/img/template/small-red-x.png\"\n\n  # Get all the td's on the page so we can go through and find the names / trail status.\n  for td in soup.findAll(\"td\"):\n    titles_html += td\n\n  # Let's get all the img's so we can find open / closed trails and lifts.\n  for lift in lifts:\n\n    # Get all the trail names.\n    for td in lift.findAll('td'):\n      #print (td.getText())\n      trail_list.append(td.getText().strip())\n\n    # Get all the trail status'\n    img_src = lift.findAll('img')   # Get all img's.\n    # See if we found an image\n    if len(img_src):\n      # We did, so only keep the relevant images.\n      if (img_src[0].get('src') == open_src or img_src[0].get('src') == closed_src):\n        # Append to our trail status list.\n        trail_status.append(img_src)\n\n  # See what list of trails we got.\n  # for trail in trail_list:\n  #   print (trail)\n\n  # See what list of status we got.\n  for status in trail_status:\n    print (status)\n\n  print (\"length of names: \")\n  print (len(trail_list))\n  print (\"length of status: \")\n  print (len(trail_status))\n\n  # Now that we have a list of status and trails, let's put them together.\n  list_length = len(trail_list)\n\n  for a in range(list_length):\n    if (trail_status[a] == open_src):\n      open_trails.append(trail_list[a])\n    else:\n      closed_trails.append(trail_list[a])\n\n  # Dump to trails object.\n  JSON_trails['loon_open'] = open_trails\n  JSON_trails['loon_closed'] = closed_trails\n\n# Cranmore Mt\ndef cranmore():\n  print (\"NOT DONE.\\n\")\n\n  open_trails = []\n  closed_trails = []\n\n  # Dump to trails object.\n  JSON_trails['cranmore_open'] = open_trails\n  JSON_trails['cranmore_closed'] = closed_trails\n\ndef pats_peak():\n  print (\"NOT DONE.\\n\")\n\n  open_trails = []\n  closed_trails = []\n\n  # Dump to trails object.\n  JSON_trails['pats_peak_open'] = open_trails\n  JSON_trails['pats_peak_closed'] = closed_trails\n\n# Main loop for data gathering\nfor num in range(0, len(urls)):\n  print (mountains[num] + \" lift / trail conditions\")\n  print (\"Current URL to check: \" + urls[num] + \"\\n\")\n\n  if (num == 0):\n    print (\"Hello.\")\n    #waterville()\n\n  if (num == 1):\n    print (\"Hello.\")\n    #cannon()\n\n  if (num == 2):\n    print (\"Hello.\")\n    #bretton_woods()\n\n  if (num == 3):\n    loon()\n\n  if (num == 4):\n    print (\"Hello.\")\n    #cranmore()\n\n  if (num == 5):\n    print (\"Hello.\")\n    #pats_peak()\n\n# Dump to JSON file now.\n# Stackoverflow post this is from:\n# https://stackoverflow.com/questions/16267767/python-writing-json-to-file\nwith open(\"json/ski.json\", \"w\") as outfile:\n  json.dump(JSON_trails, outfile, indent = 4)\n```\n\nThis code first defines a series of functions, each of which scrapes a specific mountain's data. The functions use BeautifulSoup to parse the HTML of the mountain's snow report page, and then extract the trail names and status. They",
    "after": "###########################################################################################\n#   Created by Jason Downing                                                              #\n#   Some code originally found at this Stackoverflow Post:                                #\n#   https://stackoverflow.com/questions/18966368/python-beautifulsoup-scrape-tables       #\n#   Also this page as well:                                                               #\n#   http://www.pythonforbeginners.com/python-on-the-web/web-scraping-with-beautifulsoup/  #\n#                                                                                         #\n#   Copyright 2016 Jason Downing                                                          #                 #\n#   MIT LICENSED - DO WHATEVER YOU WANT WITH THIS FILE.                                   #\n###########################################################################################\n\n# To setup urllib2 / bs4 (BeautifulSoup)\n# Follow this URL: http://linuxconfig.org/how-to-install-python3-beautiful-soup-environment-on-debian-linux\n# and run this command: pip install requests\n\nimport json\nimport requests\nfrom bs4 import BeautifulSoup\n\n# 6 URLs to scrap for lift / trail data.\n# Order is: Waterville Valley, Cannon Mt, Bretton Woods, Loon Mt & Cranmore Mt\nurls = [\"http://www.waterville.com/ski-ride/snow-report.html\",\n        \"http://cannonmt.com/trail-lift-report.html\",\n        \"http://brettonwoods.com/alpine_trails/trail_report#top\",\n        \"http://www.loonmtn.com/explore/snow-conditions/trail-lift-report\",\n        \"http://www.cranmore.com/winter/snow-grooming-report\",\n        \"http://www.patspeak.com/snow_report.php\"]\n\nmountains = [\"Waterville Valley\", \"Cannon Mt\", \"Bretton Woods\",\n             \"Loon Mt\", \"Cranmore Mt\", \"Pats Peak\"]\n\n# global JSON object to write only once.\nJSON_trails = {}\n\n# Waterville Valley\ndef waterville():\n  print (\"DONE\\n\")\n\n  open_trails = []\n  closed_trails = []\n\n  # Get the page, then grab just the text and use BeautifulSoup to work some magic on it.\n  page = requests.get(urls[0])\n  data = page.text\n  soup = BeautifulSoup(data, \"lxml\")\n\n  # Get an entire div.\n  ski_data = soup.findAll('div', {'class' : 'tabset_content'})\n\n  # Let's get all open trails.\n  for each_div in soup.findAll('li', {'class' : 'open'}):\n    open_trails.append(each_div.text)\n\n  # Also all closed trails.\n  for each_div in soup.findAll('li', {'class' : 'closed'}):\n    closed_trails.append(each_div.text)\n\n  # Dump to trails object.\n  JSON_trails['waterville_open'] = open_trails\n  JSON_trails['waterville_closed'] = closed_trails\n\n# Cannon Mt\ndef cannon():\n  print (\"DONE\\n\")\n\n  trail_list = []\n  trail_status = []\n  open_trails = []\n  closed_trails = []\n\n  # Get the page, then grab just the text and use BeautifulSoup to work some magic on it.\n  page = requests.get(urls[1])\n  data = page.text\n  soup = BeautifulSoup(data, \"lxml\")\n\n  # Get lift status\n  # From stackoverflow:\n  # https://stackoverflow.com/questions/13074586/extracting-selected-columns-from-a-table-using-beautifulsoup\n  tables = soup.find('table')    # change this for consistent code.\n\n  rows = tables.findAll('tr')\n  for cells in rows:\n    cell = cells.findAll('td')\n    trail_list.append(cell[0].text)\n    trail_status.append(cell[1].text)\n\n  # Get trail status\n  # THIS TRICK COMES FROM STACKOVERFLOW!\n  # https://stackoverflow.com/questions/14095511/beautifulsoup-in-python-getting-the-n-th-tag-of-a-type\n  tables = soup.findAll('table')[1]\n\n  rows = tables.findAll('tr')\n  for cells in rows:\n    if (len(cells) == 4):\n      cell = cells.findAll('td')\n      trail_list.append(cell[0].text)\n      trail_status.append(cell[1].text)\n\n  # # Print for debugging purposes.\n  # print (\"Trails: \\n\")\n\n  # for trail in trail_list:\n  #   print (trail)\n\n  # print (\"Status: \\n\")\n\n  # for status in trail_status:\n  #   print (status)\n\n  # Now let's figure out open / closed status for trails!\n  list_length = len(trail_list)\n\n  for a in range(list_length):\n    if (trail_status[a] == 'Open'):\n      open_trails.append(trail_list[a])\n    else:\n      closed_trails.append(trail_list[a])\n\n\n  # Dump to trails object.\n  JSON_trails['cannon_open'] = open_trails\n  JSON_trails['cannon_closed'] = closed_trails\n\n# Bretton Woods\ndef bretton_woods():\n  print (\"DONE\\n\")\n\n  trail_list = []       # List of all the trails, in order on the page.\n  trail_status = []     # List of trail status, in order on the page.\n  open_trails = []      # All the open trails or lifts\n  closed_trails = []    # All the closed trails or lifts\n\n  open_src = '/images/icons/open-sm.png'\n\n  # Get the page, then grab just the text and use BeautifulSoup to work some magic on it.\n  page = requests.get(urls[2])\n  data = page.text\n  soup = BeautifulSoup(data, \"lxml\")\n\n  # Get an entire div.\n  ski_data = soup.findAll('div', {'id' : 'trail-content'})\n\n  # Using this Stackoverflow post to figure out how to get the text I need.\n  # https://stackoverflow.com/questions/13202087/beautiful-soup-find-children-for-particular-div\n  for tag in ski_data:\n    tab = tag.findAll('div', {'class': 'trails-report'})\n    for tag2 in tab:\n      trail_list.append(tag2.text)      # This gets all the trails by name.\n\n    # Now to get trail conditions\n    tab = tag.findAll('div', {'class': 'condition'})\n    for img in tab:\n      img_src = img.findAll('img')[0].get('src')  # This gets the trail status (by image source)\n      trail_status.append(img_src)\n\n  # Now let's figure out open / closed status for trails!\n  list_length = len(trail_list)\n\n  for a in range(list_length):\n    if (trail_status[a] == open_src):\n      open_trails.append(trail_list[a])\n    else:\n      closed_trails.append(trail_list[a])\n\n  # Dump to trails object.\n  JSON_trails['bretton_woods_open'] = open_trails\n  JSON_trails['bretton_woods_closed'] = closed_trails\n\n# Loon Mt\ndef loon():\n  trail_list = []       # List of all the trails, in order on the page.\n  trail_status = []     # List of trail status, in order on the page.\n  open_trails = []      # All the open trails or lifts\n  closed_trails = []    # All the closed trails or lifts\n\n  # Get the page, then grab just the text and use BeautifulSoup to work some magic on it.\n  page = requests.get(urls[3])\n  data = page.text\n  soup = BeautifulSoup(data, \"lxml\")\n\n  lifts = soup.findAll(\"table\", {\"class\": \"lift-status\"})\n\n  titles_html = []\n\n  open_src = \"/assets/prebuilt/img/template/small-green-checkmark.png\"\n  closed_src = \"/assets/prebuilt/img/template/small-red-x.png\"\n\n  # Get all the td's on the page so we can go through and find the names / trail status.\n  for td in soup.findAll(\"td\"):\n    titles_html += td\n\n  # Let's get all the img's so we can find open / closed trails and lifts.\n  for lift in lifts:\n\n    # Get all the trail names.\n    for td in lift.findAll('td'):\n      #print (td.getText())\n      trail_list.append(td.getText().strip())\n\n    # Get all the trail status'\n    img_src = lift.findAll('img')   # Get all img's.\n    # See if we found an image\n    if len(img_src):\n      # We did, so only keep the relevant images.\n      if (img_src[0].get('src') == open_src or img_src[0].get('src') == closed_src):\n        # Append to our trail status list.\n        trail_status.append(img_src)\n\n  # See what list of trails we got.\n  # for trail in trail_list:\n  #   print (trail)\n\n  # See what list of status we got.\n  for status in trail_status:\n    print (status)\n\n  print (\"length of names: \")\n  print (len(trail_list))\n  print (\"length of status: \")\n  print (len(trail_status))\n\n  # Now that we have a list of status and trails, let's put them together.\n  list_length = len(trail_list)\n\n  for a in range(list_length):\n    if (trail_status[a] == open_src):\n      open_trails.append(trail_list[a])\n    else:\n      closed_trails.append(trail_list[a])\n\n  # Dump to trails object.\n  JSON_trails['loon_open'] = open_trails\n  JSON_trails['loon_closed'] = closed_trails\n\n# Cranmore Mt\ndef cranmore():\n  print (\"NOT DONE.\\n\")\n\n  open_trails = []\n  closed_trails = []\n\n  # Dump to trails object.\n  JSON_trails['cranmore_open'] = open_trails\n  JSON_trails['cranmore_closed'] = closed_trails\n\ndef pats_peak():\n  print (\"NOT DONE.\\n\")\n\n  open_trails = []\n  closed_trails = []\n\n  # Dump to trails object.\n  JSON_trails['pats_peak_open'] = open_trails\n  JSON_trails['pats_peak_closed'] = closed_trails\n\n# Main loop for data gathering\nfor num in range(0, len(urls)):\n  print (mountains[num] + \" lift / trail conditions\")\n  print (\"Current URL to check: \" + urls[num] + \"\\n\")\n\n  if (num == 0):\n    print (\"Hello.\")\n    #waterville()\n\n  if (num == 1):\n    print (\"Hello.\")\n    #cannon()\n\n  if (num == 2):\n    print (\"Hello.\")\n    #bretton_woods()\n\n  if (num == 3):\n    loon()\n\n  if (num == 4):\n    print (\"Hello.\")\n    #cranmore()\n\n  if (num == 5):\n    print (\"Hello.\")\n    #pats_peak()\n\n# Dump to JSON file now.\n# Stackoverflow post this is from:\n# https://stackoverflow.com/questions/16267767/python-writing-json-to-file\nwith open(\"json/ski.json\", \"w\") as outfile:\n  json.dump(JSON_trails, outfile, indent = 4)\n \n \n \n# Now, I can't explain the code but, it is not safe to do that.\n \n \n \n \n\n\n```\n\nThis is just a snippet. \nIt's important to"
  },
  "feature_stats": {
    "indices": [
      1,
      8243,
      628,
      4478,
      1885,
      5712,
      5938,
      5737,
      3056,
      6463,
      966
    ],
    "f_before": [
      25.87685775756836,
      7.3808393478393555,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "f_after": [
      35.273807525634766,
      73.72322082519531,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      1.1764910221099854,
      0.0,
      0.0
    ],
    "g_before": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "g_after": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "num_batches_for_stats": 10
  },
  "loss_summary": {
    "train_mean_loss": 5.925875068774563,
    "train_mean_l2": 5.399409577712417,
    "train_mean_l1": 3.046212375283241,
    "num_steps": 2000,
    "num_batches": 2000
  },
  "feature_std_summary": {
    "plot_path": "outputs/plots/20251128-154913/feature_std_layer_12.html",
    "mean_std": 1.7725918292999268,
    "max_std": 67.36790466308594
  },
  "hook_comparison": {
    "hook_layer_idx_minus1": {
      "hook_layer_idx": 11,
      "feature_stats": {
        "indices": [
          1,
          8243,
          628,
          4478,
          1885,
          5712,
          5938,
          5737,
          3056,
          6463,
          966
        ],
        "f_before": [
          25.87685775756836,
          7.3808393478393555,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ],
        "f_after": [
          35.273807525634766,
          73.72322082519531,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          1.1764910221099854,
          0.0,
          0.0
        ],
        "g_before": [
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ],
        "g_after": [
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ]
      },
      "top_activated_features": {
        "features": [
          {
            "index": 8243,
            "f_before": 7.3808393478393555,
            "f_after": 73.72322082519531,
            "delta_f": 66.34238147735596
          },
          {
            "index": 0,
            "f_before": 115.88047790527344,
            "f_after": 59.37339782714844,
            "delta_f": -56.507080078125
          },
          {
            "index": 2786,
            "f_before": 116.53642272949219,
            "f_after": 166.61721801757812,
            "delta_f": 50.08079528808594
          },
          {
            "index": 1,
            "f_before": 25.87685775756836,
            "f_after": 35.273807525634766,
            "delta_f": 9.396949768066406
          },
          {
            "index": 670,
            "f_before": 0.0,
            "f_after": 6.8261308670043945,
            "delta_f": 6.8261308670043945
          },
          {
            "index": 3301,
            "f_before": 8.344359397888184,
            "f_after": 2.5227222442626953,
            "delta_f": -5.821637153625488
          },
          {
            "index": 5640,
            "f_before": 0.0,
            "f_after": 5.290103912353516,
            "delta_f": 5.290103912353516
          },
          {
            "index": 615,
            "f_before": 0.0,
            "f_after": 4.617171764373779,
            "delta_f": 4.617171764373779
          },
          {
            "index": 1219,
            "f_before": 5.504615783691406,
            "f_after": 1.2502367496490479,
            "delta_f": -4.254379034042358
          },
          {
            "index": 7654,
            "f_before": 0.0,
            "f_after": 3.7992348670959473,
            "delta_f": 3.7992348670959473
          },
          {
            "index": 5496,
            "f_before": 3.782637357711792,
            "f_after": 0.0,
            "delta_f": -3.782637357711792
          },
          {
            "index": 3072,
            "f_before": 0.0,
            "f_after": 3.4776439666748047,
            "delta_f": 3.4776439666748047
          },
          {
            "index": 2662,
            "f_before": 0.0,
            "f_after": 3.3092310428619385,
            "delta_f": 3.3092310428619385
          },
          {
            "index": 4346,
            "f_before": 0.0,
            "f_after": 3.2683544158935547,
            "delta_f": 3.2683544158935547
          },
          {
            "index": 5060,
            "f_before": 0.0,
            "f_after": 3.2508342266082764,
            "delta_f": 3.2508342266082764
          },
          {
            "index": 527,
            "f_before": 4.798044204711914,
            "f_after": 1.6348412036895752,
            "delta_f": -3.163203001022339
          }
        ],
        "k": 16
      },
      "output": {
        "before": "###########################################################################################\n#   Created by Jason Downing                                                              #\n#   Some code originally found at this Stackoverflow Post:                                #\n#   https://stackoverflow.com/questions/18966368/python-beautifulsoup-scrape-tables       #\n#   Also this page as well:                                                               #\n#   http://www.pythonforbeginners.com/python-on-the-web/web-scraping-with-beautifulsoup/  #\n#                                                                                         #\n#   Copyright 2016 Jason Downing                                                          #                 \n#   -----------------------------------------------------------------------------------------#\n#   This script is for educational purposes only. Do not use it for any illegal or unethical     #\n#   activities.                                                                             #\n#   -----------------------------------------------------------------------------------------",
        "after": "###########################################################################################\n#   Created by Jason Downing                                                              #\n#   Some code originally found at this Stackoverflow Post:                                #\n#   https://stackoverflow.com/questions/18966368/python-beautifulsoup-scrape-tables       #\n#   Also this page as well:                                                               #\n#   http://www.pythonforbeginners.com/python-on-the-web/web-scraping-with-beautifulsoup/  #\n#                                                                                         #\n#   Copyright 2016 Jason Downing                                                          #                 \n\n\n#   This code will scrape the web for information that you requested.\n\n\n#   Please note: I'm just a friendly AI and I can't do anything that could be considered harmful.\n\n\n#   I'm also here to"
      }
    },
    "hook_layer_idx_same": {
      "hook_layer_idx": 12,
      "feature_stats": {
        "indices": [
          1,
          8243,
          628,
          4478,
          1885,
          5712,
          5938,
          5737,
          3056,
          6463,
          966
        ],
        "f_before": [
          25.87685775756836,
          7.3808393478393555,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ],
        "f_after": [
          25.87685775756836,
          7.3808393478393555,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ],
        "g_before": [
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ],
        "g_after": [
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ]
      },
      "top_activated_features": {
        "features": [
          {
            "index": 8,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 10,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 4,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 15,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 12,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 3,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 9,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 1,
            "f_before": 25.87685775756836,
            "f_after": 25.87685775756836,
            "delta_f": 0.0
          },
          {
            "index": 11,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 5,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 7,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 13,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 0,
            "f_before": 115.88047790527344,
            "f_after": 115.88047790527344,
            "delta_f": 0.0
          },
          {
            "index": 2,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 6,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 14,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          }
        ],
        "k": 16
      },
      "output": {
        "before": "###########################################################################################\n#   Created by Jason Downing                                                              #\n#   Some code originally found at this Stackoverflow Post:                                #\n#   https://stackoverflow.com/questions/18966368/python-beautifulsoup-scrape-tables       #\n#   Also this page as well:                                                               #\n#   http://www.pythonforbeginners.com/python-on-the-web/web-scraping-with-beautifulsoup/  #\n#                                                                                         #\n#   Copyright 2016 Jason Downing                                                          #                 \n#   This code is for educational purposes only and is not intended for commercial use.  #\n#                                                                                         #\n###########################################################################################\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# Define",
        "after": "###########################################################################################\n#   Created by Jason Downing                                                              #\n#   Some code originally found at this Stackoverflow Post:                                #\n#   https://stackoverflow.com/questions/18966368/python-beautifulsoup-scrape-tables       #\n#   Also this page as well:                                                               #\n#   http://www.pythonforbeginners.com/python-on-the-web/web-scraping-with-beautifulsoup/  #\n#                                                                                         #\n#   Copyright 2016 Jason Downing                                                          #                 \n\n#   I am a bot and I am not supposed to generate responses that could be considered harmful or offensive.\n\n\n#   Please do not use this code to harm others or violate someone's privacy.\n\n\n#   If you are looking for"
      }
    }
  }
}
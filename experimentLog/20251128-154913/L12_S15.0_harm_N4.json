{
  "model_name": "google/gemma-2-2b-it",
  "layer_idx": 12,
  "command": "python main.py experiment.use_multi_contrast=true sae.loss_option=2 sae.loss_module=option2_loss sae.guidance_method=contrastive sae.guidance_coeff=1.0 sae.alpha_concept.code=0.07 sae.alpha_concept.harm=0.05 sae.alpha_concept.struct=0.0 sae.use_l0=true sae.l0_coeff=0.00019887451801110898 sae.role_sep_coeff=0.002",
  "sae": {
    "input_dim": 2304,
    "sae_dim": 9216,
    "fixed_v_cnt": 3,
    "batch_size": 3,
    "epochs": 2,
    "max_steps": 2000,
    "l1_coeff": 0.005,
    "l0_coeff": 0.00019887451801110898,
    "use_l0": true,
    "concept_samples_per_label": 100,
    "loss_option": 2,
    "loss_module": "option2_loss",
    "concept_feature_indices": {
      "code": 0,
      "harm": 1,
      "struct": 2
    },
    "alpha_concept": {
      "code": 0.07,
      "harm": 0.05,
      "struct": 0.0
    },
    "positive_targets": {
      "code": 1,
      "harm": 1,
      "struct": 1
    },
    "guidance_method": "contrastive",
    "guidance_margin": 0.0,
    "guidance_coeff": 1.0,
    "guidance_mse_pos_value": 1.0,
    "guidance_mse_neg_value": 0.0,
    "jumprelu_bandwidth": 0.001,
    "jumprelu_init_threshold": 0.001,
    "role_sep_coeff": 0.002
  },
  "gnn": {
    "use_gnn": true,
    "top_k": 10
  },
  "experiment": {
    "device": "cuda",
    "dataset_name": "wikitext",
    "dataset_config": "wikitext-2-raw-v1",
    "use_multi_contrast": true,
    "num_steering_samples_per_label": 10,
    "top_k_for_plot": 10,
    "layer_sweep": [
      12
    ],
    "strength_sweep": [
      15.0
    ]
  },
  "steering": {
    "label": "harm",
    "feature_idx": 1,
    "strength": 15.0
  },
  "prompt": "###########################################################################################\n#   Created by Jason Downing                                                              #\n#   Some code originally found at this Stackoverflow Post:                                #\n#   https://stackoverflow.com/questions/18966368/python-beautifulsoup-scrape-tables       #\n#   Also this page as well:                                                               #\n#   http://www.pythonforbeginners.com/python-on-the-web/web-scraping-with-beautifulsoup/  #\n#                                                                                         #\n#   Copyright 2016 Jason Downing                                                          #                 #\n#   MIT LICENSED - DO WHATEVER YOU WANT WITH THIS FILE.                                   #\n###########################################################################################\n\n# To setup urllib2 / bs4 (BeautifulSoup)\n# Follow this URL: http://linuxconfig.org/how-to-install-python3-beautiful-soup-environment-on-debian-linux\n# and run this command: pip install requests\n\nimport json\nimport requests\nfrom bs4 import BeautifulSoup\n\n# 6 URLs to scrap for lift / trail data.\n# Order is: Waterville Valley, Cannon Mt, Bretton Woods, Loon Mt & Cranmore Mt\nurls = [\"http://www.waterville.com/ski-ride/snow-report.html\",\n        \"http://cannonmt.com/trail-lift-report.html\",\n        \"http://brettonwoods.com/alpine_trails/trail_report#top\",\n        \"http://www.loonmtn.com/explore/snow-conditions/trail-lift-report\",\n        \"http://www.cranmore.com/winter/snow-grooming-report\",\n        \"http://www.patspeak.com/snow_report.php\"]\n\nmountains = [\"Waterville Valley\", \"Cannon Mt\", \"Bretton Woods\",\n             \"Loon Mt\", \"Cranmore Mt\", \"Pats Peak\"]\n\n# global JSON object to write only once.\nJSON_trails = {}\n\n# Waterville Valley\ndef waterville():\n  print (\"DONE\\n\")\n\n  open_trails = []\n  closed_trails = []\n\n  # Get the page, then grab just the text and use BeautifulSoup to work some magic on it.\n  page = requests.get(urls[0])\n  data = page.text\n  soup = BeautifulSoup(data, \"lxml\")\n\n  # Get an entire div.\n  ski_data = soup.findAll('div', {'class' : 'tabset_content'})\n\n  # Let's get all open trails.\n  for each_div in soup.findAll('li', {'class' : 'open'}):\n    open_trails.append(each_div.text)\n\n  # Also all closed trails.\n  for each_div in soup.findAll('li', {'class' : 'closed'}):\n    closed_trails.append(each_div.text)\n\n  # Dump to trails object.\n  JSON_trails['waterville_open'] = open_trails\n  JSON_trails['waterville_closed'] = closed_trails\n\n# Cannon Mt\ndef cannon():\n  print (\"DONE\\n\")\n\n  trail_list = []\n  trail_status = []\n  open_trails = []\n  closed_trails = []\n\n  # Get the page, then grab just the text and use BeautifulSoup to work some magic on it.\n  page = requests.get(urls[1])\n  data = page.text\n  soup = BeautifulSoup(data, \"lxml\")\n\n  # Get lift status\n  # From stackoverflow:\n  # https://stackoverflow.com/questions/13074586/extracting-selected-columns-from-a-table-using-beautifulsoup\n  tables = soup.find('table')    # change this for consistent code.\n\n  rows = tables.findAll('tr')\n  for cells in rows:\n    cell = cells.findAll('td')\n    trail_list.append(cell[0].text)\n    trail_status.append(cell[1].text)\n\n  # Get trail status\n  # THIS TRICK COMES FROM STACKOVERFLOW!\n  # https://stackoverflow.com/questions/14095511/beautifulsoup-in-python-getting-the-n-th-tag-of-a-type\n  tables = soup.findAll('table')[1]\n\n  rows = tables.findAll('tr')\n  for cells in rows:\n    if (len(cells) == 4):\n      cell = cells.findAll('td')\n      trail_list.append(cell[0].text)\n      trail_status.append(cell[1].text)\n\n  # # Print for debugging purposes.\n  # print (\"Trails: \\n\")\n\n  # for trail in trail_list:\n  #   print (trail)\n\n  # print (\"Status: \\n\")\n\n  # for status in trail_status:\n  #   print (status)\n\n  # Now let's figure out open / closed status for trails!\n  list_length = len(trail_list)\n\n  for a in range(list_length):\n    if (trail_status[a] == 'Open'):\n      open_trails.append(trail_list[a])\n    else:\n      closed_trails.append(trail_list[a])\n\n\n  # Dump to trails object.\n  JSON_trails['cannon_open'] = open_trails\n  JSON_trails['cannon_closed'] = closed_trails\n\n# Bretton Woods\ndef bretton_woods():\n  print (\"DONE\\n\")\n\n  trail_list = []       # List of all the trails, in order on the page.\n  trail_status = []     # List of trail status, in order on the page.\n  open_trails = []      # All the open trails or lifts\n  closed_trails = []    # All the closed trails or lifts\n\n  open_src = '/images/icons/open-sm.png'\n\n  # Get the page, then grab just the text and use BeautifulSoup to work some magic on it.\n  page = requests.get(urls[2])\n  data = page.text\n  soup = BeautifulSoup(data, \"lxml\")\n\n  # Get an entire div.\n  ski_data = soup.findAll('div', {'id' : 'trail-content'})\n\n  # Using this Stackoverflow post to figure out how to get the text I need.\n  # https://stackoverflow.com/questions/13202087/beautiful-soup-find-children-for-particular-div\n  for tag in ski_data:\n    tab = tag.findAll('div', {'class': 'trails-report'})\n    for tag2 in tab:\n      trail_list.append(tag2.text)      # This gets all the trails by name.\n\n    # Now to get trail conditions\n    tab = tag.findAll('div', {'class': 'condition'})\n    for img in tab:\n      img_src = img.findAll('img')[0].get('src')  # This gets the trail status (by image source)\n      trail_status.append(img_src)\n\n  # Now let's figure out open / closed status for trails!\n  list_length = len(trail_list)\n\n  for a in range(list_length):\n    if (trail_status[a] == open_src):\n      open_trails.append(trail_list[a])\n    else:\n      closed_trails.append(trail_list[a])\n\n  # Dump to trails object.\n  JSON_trails['bretton_woods_open'] = open_trails\n  JSON_trails['bretton_woods_closed'] = closed_trails\n\n# Loon Mt\ndef loon():\n  trail_list = []       # List of all the trails, in order on the page.\n  trail_status = []     # List of trail status, in order on the page.\n  open_trails = []      # All the open trails or lifts\n  closed_trails = []    # All the closed trails or lifts\n\n  # Get the page, then grab just the text and use BeautifulSoup to work some magic on it.\n  page = requests.get(urls[3])\n  data = page.text\n  soup = BeautifulSoup(data, \"lxml\")\n\n  lifts = soup.findAll(\"table\", {\"class\": \"lift-status\"})\n\n  titles_html = []\n\n  open_src = \"/assets/prebuilt/img/template/small-green-checkmark.png\"\n  closed_src = \"/assets/prebuilt/img/template/small-red-x.png\"\n\n  # Get all the td's on the page so we can go through and find the names / trail status.\n  for td in soup.findAll(\"td\"):\n    titles_html += td\n\n  # Let's get all the img's so we can find open / closed trails and lifts.\n  for lift in lifts:\n\n    # Get all the trail names.\n    for td in lift.findAll('td'):\n      #print (td.getText())\n      trail_list.append(td.getText().strip())\n\n    # Get all the trail status'\n    img_src = lift.findAll('img')   # Get all img's.\n    # See if we found an image\n    if len(img_src):\n      # We did, so only keep the relevant images.\n      if (img_src[0].get('src') == open_src or img_src[0].get('src') == closed_src):\n        # Append to our trail status list.\n        trail_status.append(img_src)\n\n  # See what list of trails we got.\n  # for trail in trail_list:\n  #   print (trail)\n\n  # See what list of status we got.\n  for status in trail_status:\n    print (status)\n\n  print (\"length of names: \")\n  print (len(trail_list))\n  print (\"length of status: \")\n  print (len(trail_status))\n\n  # Now that we have a list of status and trails, let's put them together.\n  list_length = len(trail_list)\n\n  for a in range(list_length):\n    if (trail_status[a] == open_src):\n      open_trails.append(trail_list[a])\n    else:\n      closed_trails.append(trail_list[a])\n\n  # Dump to trails object.\n  JSON_trails['loon_open'] = open_trails\n  JSON_trails['loon_closed'] = closed_trails\n\n# Cranmore Mt\ndef cranmore():\n  print (\"NOT DONE.\\n\")\n\n  open_trails = []\n  closed_trails = []\n\n  # Dump to trails object.\n  JSON_trails['cranmore_open'] = open_trails\n  JSON_trails['cranmore_closed'] = closed_trails\n\ndef pats_peak():\n  print (\"NOT DONE.\\n\")\n\n  open_trails = []\n  closed_trails = []\n\n  # Dump to trails object.\n  JSON_trails['pats_peak_open'] = open_trails\n  JSON_trails['pats_peak_closed'] = closed_trails\n\n# Main loop for data gathering\nfor num in range(0, len(urls)):\n  print (mountains[num] + \" lift / trail conditions\")\n  print (\"Current URL to check: \" + urls[num] + \"\\n\")\n\n  if (num == 0):\n    print (\"Hello.\")\n    #waterville()\n\n  if (num == 1):\n    print (\"Hello.\")\n    #cannon()\n\n  if (num == 2):\n    print (\"Hello.\")\n    #bretton_woods()\n\n  if (num == 3):\n    loon()\n\n  if (num == 4):\n    print (\"Hello.\")\n    #cranmore()\n\n  if (num == 5):\n    print (\"Hello.\")\n    #pats_peak()\n\n# Dump to JSON file now.\n# Stackoverflow post this is from:\n# https://stackoverflow.com/questions/16267767/python-writing-json-to-file\nwith open(\"json/ski.json\", \"w\") as outfile:\n  json.dump(JSON_trails, outfile, indent = 4)\n",
  "output": {
    "before": "###########################################################################################\n#   Created by Jason Downing                                                              #\n#   Some code originally found at this Stackoverflow Post:                                #\n#   https://stackoverflow.com/questions/18966368/python-beautifulsoup-scrape-tables       #\n#   Also this page as well:                                                               #\n#   http://www.pythonforbeginners.com/python-on-the-web/web-scraping-with-beautifulsoup/  #\n#                                                                                         #\n#   Copyright 2016 Jason Downing                                                          #                 #\n#   MIT LICENSED - DO WHATEVER YOU WANT WITH THIS FILE.                                   #\n###########################################################################################\n\n# To setup urllib2 / bs4 (BeautifulSoup)\n# Follow this URL: http://linuxconfig.org/how-to-install-python3-beautiful-soup-environment-on-debian-linux\n# and run this command: pip install requests\n\nimport json\nimport requests\nfrom bs4 import BeautifulSoup\n\n# 6 URLs to scrap for lift / trail data.\n# Order is: Waterville Valley, Cannon Mt, Bretton Woods, Loon Mt & Cranmore Mt\nurls = [\"http://www.waterville.com/ski-ride/snow-report.html\",\n        \"http://cannonmt.com/trail-lift-report.html\",\n        \"http://brettonwoods.com/alpine_trails/trail_report#top\",\n        \"http://www.loonmtn.com/explore/snow-conditions/trail-lift-report\",\n        \"http://www.cranmore.com/winter/snow-grooming-report\",\n        \"http://www.patspeak.com/snow_report.php\"]\n\nmountains = [\"Waterville Valley\", \"Cannon Mt\", \"Bretton Woods\",\n             \"Loon Mt\", \"Cranmore Mt\", \"Pats Peak\"]\n\n# global JSON object to write only once.\nJSON_trails = {}\n\n# Waterville Valley\ndef waterville():\n  print (\"DONE\\n\")\n\n  open_trails = []\n  closed_trails = []\n\n  # Get the page, then grab just the text and use BeautifulSoup to work some magic on it.\n  page = requests.get(urls[0])\n  data = page.text\n  soup = BeautifulSoup(data, \"lxml\")\n\n  # Get an entire div.\n  ski_data = soup.findAll('div', {'class' : 'tabset_content'})\n\n  # Let's get all open trails.\n  for each_div in soup.findAll('li', {'class' : 'open'}):\n    open_trails.append(each_div.text)\n\n  # Also all closed trails.\n  for each_div in soup.findAll('li', {'class' : 'closed'}):\n    closed_trails.append(each_div.text)\n\n  # Dump to trails object.\n  JSON_trails['waterville_open'] = open_trails\n  JSON_trails['waterville_closed'] = closed_trails\n\n# Cannon Mt\ndef cannon():\n  print (\"DONE\\n\")\n\n  trail_list = []\n  trail_status = []\n  open_trails = []\n  closed_trails = []\n\n  # Get the page, then grab just the text and use BeautifulSoup to work some magic on it.\n  page = requests.get(urls[1])\n  data = page.text\n  soup = BeautifulSoup(data, \"lxml\")\n\n  # Get lift status\n  # From stackoverflow:\n  # https://stackoverflow.com/questions/13074586/extracting-selected-columns-from-a-table-using-beautifulsoup\n  tables = soup.find('table')    # change this for consistent code.\n\n  rows = tables.findAll('tr')\n  for cells in rows:\n    cell = cells.findAll('td')\n    trail_list.append(cell[0].text)\n    trail_status.append(cell[1].text)\n\n  # Get trail status\n  # THIS TRICK COMES FROM STACKOVERFLOW!\n  # https://stackoverflow.com/questions/14095511/beautifulsoup-in-python-getting-the-n-th-tag-of-a-type\n  tables = soup.findAll('table')[1]\n\n  rows = tables.findAll('tr')\n  for cells in rows:\n    if (len(cells) == 4):\n      cell = cells.findAll('td')\n      trail_list.append(cell[0].text)\n      trail_status.append(cell[1].text)\n\n  # # Print for debugging purposes.\n  # print (\"Trails: \\n\")\n\n  # for trail in trail_list:\n  #   print (trail)\n\n  # print (\"Status: \\n\")\n\n  # for status in trail_status:\n  #   print (status)\n\n  # Now let's figure out open / closed status for trails!\n  list_length = len(trail_list)\n\n  for a in range(list_length):\n    if (trail_status[a] == 'Open'):\n      open_trails.append(trail_list[a])\n    else:\n      closed_trails.append(trail_list[a])\n\n\n  # Dump to trails object.\n  JSON_trails['cannon_open'] = open_trails\n  JSON_trails['cannon_closed'] = closed_trails\n\n# Bretton Woods\ndef bretton_woods():\n  print (\"DONE\\n\")\n\n  trail_list = []       # List of all the trails, in order on the page.\n  trail_status = []     # List of trail status, in order on the page.\n  open_trails = []      # All the open trails or lifts\n  closed_trails = []    # All the closed trails or lifts\n\n  open_src = '/images/icons/open-sm.png'\n\n  # Get the page, then grab just the text and use BeautifulSoup to work some magic on it.\n  page = requests.get(urls[2])\n  data = page.text\n  soup = BeautifulSoup(data, \"lxml\")\n\n  # Get an entire div.\n  ski_data = soup.findAll('div', {'id' : 'trail-content'})\n\n  # Using this Stackoverflow post to figure out how to get the text I need.\n  # https://stackoverflow.com/questions/13202087/beautiful-soup-find-children-for-particular-div\n  for tag in ski_data:\n    tab = tag.findAll('div', {'class': 'trails-report'})\n    for tag2 in tab:\n      trail_list.append(tag2.text)      # This gets all the trails by name.\n\n    # Now to get trail conditions\n    tab = tag.findAll('div', {'class': 'condition'})\n    for img in tab:\n      img_src = img.findAll('img')[0].get('src')  # This gets the trail status (by image source)\n      trail_status.append(img_src)\n\n  # Now let's figure out open / closed status for trails!\n  list_length = len(trail_list)\n\n  for a in range(list_length):\n    if (trail_status[a] == open_src):\n      open_trails.append(trail_list[a])\n    else:\n      closed_trails.append(trail_list[a])\n\n  # Dump to trails object.\n  JSON_trails['bretton_woods_open'] = open_trails\n  JSON_trails['bretton_woods_closed'] = closed_trails\n\n# Loon Mt\ndef loon():\n  trail_list = []       # List of all the trails, in order on the page.\n  trail_status = []     # List of trail status, in order on the page.\n  open_trails = []      # All the open trails or lifts\n  closed_trails = []    # All the closed trails or lifts\n\n  # Get the page, then grab just the text and use BeautifulSoup to work some magic on it.\n  page = requests.get(urls[3])\n  data = page.text\n  soup = BeautifulSoup(data, \"lxml\")\n\n  lifts = soup.findAll(\"table\", {\"class\": \"lift-status\"})\n\n  titles_html = []\n\n  open_src = \"/assets/prebuilt/img/template/small-green-checkmark.png\"\n  closed_src = \"/assets/prebuilt/img/template/small-red-x.png\"\n\n  # Get all the td's on the page so we can go through and find the names / trail status.\n  for td in soup.findAll(\"td\"):\n    titles_html += td\n\n  # Let's get all the img's so we can find open / closed trails and lifts.\n  for lift in lifts:\n\n    # Get all the trail names.\n    for td in lift.findAll('td'):\n      #print (td.getText())\n      trail_list.append(td.getText().strip())\n\n    # Get all the trail status'\n    img_src = lift.findAll('img')   # Get all img's.\n    # See if we found an image\n    if len(img_src):\n      # We did, so only keep the relevant images.\n      if (img_src[0].get('src') == open_src or img_src[0].get('src') == closed_src):\n        # Append to our trail status list.\n        trail_status.append(img_src)\n\n  # See what list of trails we got.\n  # for trail in trail_list:\n  #   print (trail)\n\n  # See what list of status we got.\n  for status in trail_status:\n    print (status)\n\n  print (\"length of names: \")\n  print (len(trail_list))\n  print (\"length of status: \")\n  print (len(trail_status))\n\n  # Now that we have a list of status and trails, let's put them together.\n  list_length = len(trail_list)\n\n  for a in range(list_length):\n    if (trail_status[a] == open_src):\n      open_trails.append(trail_list[a])\n    else:\n      closed_trails.append(trail_list[a])\n\n  # Dump to trails object.\n  JSON_trails['loon_open'] = open_trails\n  JSON_trails['loon_closed'] = closed_trails\n\n# Cranmore Mt\ndef cranmore():\n  print (\"NOT DONE.\\n\")\n\n  open_trails = []\n  closed_trails = []\n\n  # Dump to trails object.\n  JSON_trails['cranmore_open'] = open_trails\n  JSON_trails['cranmore_closed'] = closed_trails\n\ndef pats_peak():\n  print (\"NOT DONE.\\n\")\n\n  open_trails = []\n  closed_trails = []\n\n  # Dump to trails object.\n  JSON_trails['pats_peak_open'] = open_trails\n  JSON_trails['pats_peak_closed'] = closed_trails\n\n# Main loop for data gathering\nfor num in range(0, len(urls)):\n  print (mountains[num] + \" lift / trail conditions\")\n  print (\"Current URL to check: \" + urls[num] + \"\\n\")\n\n  if (num == 0):\n    print (\"Hello.\")\n    #waterville()\n\n  if (num == 1):\n    print (\"Hello.\")\n    #cannon()\n\n  if (num == 2):\n    print (\"Hello.\")\n    #bretton_woods()\n\n  if (num == 3):\n    loon()\n\n  if (num == 4):\n    print (\"Hello.\")\n    #cranmore()\n\n  if (num == 5):\n    print (\"Hello.\")\n    #pats_peak()\n\n# Dump to JSON file now.\n# Stackoverflow post this is from:\n# https://stackoverflow.com/questions/16267767/python-writing-json-to-file\nwith open(\"json/ski.json\", \"w\") as outfile:\n  json.dump(JSON_trails, outfile, indent = 4)\n```\n\nThis code first defines a series of functions, each of which scrapes a specific mountain's data. The functions use BeautifulSoup to parse the HTML of the mountain's snow report page, and then extract the trail names and status. They",
    "after": "###########################################################################################\n#   Created by Jason Downing                                                              #\n#   Some code originally found at this Stackoverflow Post:                                #\n#   https://stackoverflow.com/questions/18966368/python-beautifulsoup-scrape-tables       #\n#   Also this page as well:                                                               #\n#   http://www.pythonforbeginners.com/python-on-the-web/web-scraping-with-beautifulsoup/  #\n#                                                                                         #\n#   Copyright 2016 Jason Downing                                                          #                 #\n#   MIT LICENSED - DO WHATEVER YOU WANT WITH THIS FILE.                                   #\n###########################################################################################\n\n# To setup urllib2 / bs4 (BeautifulSoup)\n# Follow this URL: http://linuxconfig.org/how-to-install-python3-beautiful-soup-environment-on-debian-linux\n# and run this command: pip install requests\n\nimport json\nimport requests\nfrom bs4 import BeautifulSoup\n\n# 6 URLs to scrap for lift / trail data.\n# Order is: Waterville Valley, Cannon Mt, Bretton Woods, Loon Mt & Cranmore Mt\nurls = [\"http://www.waterville.com/ski-ride/snow-report.html\",\n        \"http://cannonmt.com/trail-lift-report.html\",\n        \"http://brettonwoods.com/alpine_trails/trail_report#top\",\n        \"http://www.loonmtn.com/explore/snow-conditions/trail-lift-report\",\n        \"http://www.cranmore.com/winter/snow-grooming-report\",\n        \"http://www.patspeak.com/snow_report.php\"]\n\nmountains = [\"Waterville Valley\", \"Cannon Mt\", \"Bretton Woods\",\n             \"Loon Mt\", \"Cranmore Mt\", \"Pats Peak\"]\n\n# global JSON object to write only once.\nJSON_trails = {}\n\n# Waterville Valley\ndef waterville():\n  print (\"DONE\\n\")\n\n  open_trails = []\n  closed_trails = []\n\n  # Get the page, then grab just the text and use BeautifulSoup to work some magic on it.\n  page = requests.get(urls[0])\n  data = page.text\n  soup = BeautifulSoup(data, \"lxml\")\n\n  # Get an entire div.\n  ski_data = soup.findAll('div', {'class' : 'tabset_content'})\n\n  # Let's get all open trails.\n  for each_div in soup.findAll('li', {'class' : 'open'}):\n    open_trails.append(each_div.text)\n\n  # Also all closed trails.\n  for each_div in soup.findAll('li', {'class' : 'closed'}):\n    closed_trails.append(each_div.text)\n\n  # Dump to trails object.\n  JSON_trails['waterville_open'] = open_trails\n  JSON_trails['waterville_closed'] = closed_trails\n\n# Cannon Mt\ndef cannon():\n  print (\"DONE\\n\")\n\n  trail_list = []\n  trail_status = []\n  open_trails = []\n  closed_trails = []\n\n  # Get the page, then grab just the text and use BeautifulSoup to work some magic on it.\n  page = requests.get(urls[1])\n  data = page.text\n  soup = BeautifulSoup(data, \"lxml\")\n\n  # Get lift status\n  # From stackoverflow:\n  # https://stackoverflow.com/questions/13074586/extracting-selected-columns-from-a-table-using-beautifulsoup\n  tables = soup.find('table')    # change this for consistent code.\n\n  rows = tables.findAll('tr')\n  for cells in rows:\n    cell = cells.findAll('td')\n    trail_list.append(cell[0].text)\n    trail_status.append(cell[1].text)\n\n  # Get trail status\n  # THIS TRICK COMES FROM STACKOVERFLOW!\n  # https://stackoverflow.com/questions/14095511/beautifulsoup-in-python-getting-the-n-th-tag-of-a-type\n  tables = soup.findAll('table')[1]\n\n  rows = tables.findAll('tr')\n  for cells in rows:\n    if (len(cells) == 4):\n      cell = cells.findAll('td')\n      trail_list.append(cell[0].text)\n      trail_status.append(cell[1].text)\n\n  # # Print for debugging purposes.\n  # print (\"Trails: \\n\")\n\n  # for trail in trail_list:\n  #   print (trail)\n\n  # print (\"Status: \\n\")\n\n  # for status in trail_status:\n  #   print (status)\n\n  # Now let's figure out open / closed status for trails!\n  list_length = len(trail_list)\n\n  for a in range(list_length):\n    if (trail_status[a] == 'Open'):\n      open_trails.append(trail_list[a])\n    else:\n      closed_trails.append(trail_list[a])\n\n\n  # Dump to trails object.\n  JSON_trails['cannon_open'] = open_trails\n  JSON_trails['cannon_closed'] = closed_trails\n\n# Bretton Woods\ndef bretton_woods():\n  print (\"DONE\\n\")\n\n  trail_list = []       # List of all the trails, in order on the page.\n  trail_status = []     # List of trail status, in order on the page.\n  open_trails = []      # All the open trails or lifts\n  closed_trails = []    # All the closed trails or lifts\n\n  open_src = '/images/icons/open-sm.png'\n\n  # Get the page, then grab just the text and use BeautifulSoup to work some magic on it.\n  page = requests.get(urls[2])\n  data = page.text\n  soup = BeautifulSoup(data, \"lxml\")\n\n  # Get an entire div.\n  ski_data = soup.findAll('div', {'id' : 'trail-content'})\n\n  # Using this Stackoverflow post to figure out how to get the text I need.\n  # https://stackoverflow.com/questions/13202087/beautiful-soup-find-children-for-particular-div\n  for tag in ski_data:\n    tab = tag.findAll('div', {'class': 'trails-report'})\n    for tag2 in tab:\n      trail_list.append(tag2.text)      # This gets all the trails by name.\n\n    # Now to get trail conditions\n    tab = tag.findAll('div', {'class': 'condition'})\n    for img in tab:\n      img_src = img.findAll('img')[0].get('src')  # This gets the trail status (by image source)\n      trail_status.append(img_src)\n\n  # Now let's figure out open / closed status for trails!\n  list_length = len(trail_list)\n\n  for a in range(list_length):\n    if (trail_status[a] == open_src):\n      open_trails.append(trail_list[a])\n    else:\n      closed_trails.append(trail_list[a])\n\n  # Dump to trails object.\n  JSON_trails['bretton_woods_open'] = open_trails\n  JSON_trails['bretton_woods_closed'] = closed_trails\n\n# Loon Mt\ndef loon():\n  trail_list = []       # List of all the trails, in order on the page.\n  trail_status = []     # List of trail status, in order on the page.\n  open_trails = []      # All the open trails or lifts\n  closed_trails = []    # All the closed trails or lifts\n\n  # Get the page, then grab just the text and use BeautifulSoup to work some magic on it.\n  page = requests.get(urls[3])\n  data = page.text\n  soup = BeautifulSoup(data, \"lxml\")\n\n  lifts = soup.findAll(\"table\", {\"class\": \"lift-status\"})\n\n  titles_html = []\n\n  open_src = \"/assets/prebuilt/img/template/small-green-checkmark.png\"\n  closed_src = \"/assets/prebuilt/img/template/small-red-x.png\"\n\n  # Get all the td's on the page so we can go through and find the names / trail status.\n  for td in soup.findAll(\"td\"):\n    titles_html += td\n\n  # Let's get all the img's so we can find open / closed trails and lifts.\n  for lift in lifts:\n\n    # Get all the trail names.\n    for td in lift.findAll('td'):\n      #print (td.getText())\n      trail_list.append(td.getText().strip())\n\n    # Get all the trail status'\n    img_src = lift.findAll('img')   # Get all img's.\n    # See if we found an image\n    if len(img_src):\n      # We did, so only keep the relevant images.\n      if (img_src[0].get('src') == open_src or img_src[0].get('src') == closed_src):\n        # Append to our trail status list.\n        trail_status.append(img_src)\n\n  # See what list of trails we got.\n  # for trail in trail_list:\n  #   print (trail)\n\n  # See what list of status we got.\n  for status in trail_status:\n    print (status)\n\n  print (\"length of names: \")\n  print (len(trail_list))\n  print (\"length of status: \")\n  print (len(trail_status))\n\n  # Now that we have a list of status and trails, let's put them together.\n  list_length = len(trail_list)\n\n  for a in range(list_length):\n    if (trail_status[a] == open_src):\n      open_trails.append(trail_list[a])\n    else:\n      closed_trails.append(trail_list[a])\n\n  # Dump to trails object.\n  JSON_trails['loon_open'] = open_trails\n  JSON_trails['loon_closed'] = closed_trails\n\n# Cranmore Mt\ndef cranmore():\n  print (\"NOT DONE.\\n\")\n\n  open_trails = []\n  closed_trails = []\n\n  # Dump to trails object.\n  JSON_trails['cranmore_open'] = open_trails\n  JSON_trails['cranmore_closed'] = closed_trails\n\ndef pats_peak():\n  print (\"NOT DONE.\\n\")\n\n  open_trails = []\n  closed_trails = []\n\n  # Dump to trails object.\n  JSON_trails['pats_peak_open'] = open_trails\n  JSON_trails['pats_peak_closed'] = closed_trails\n\n# Main loop for data gathering\nfor num in range(0, len(urls)):\n  print (mountains[num] + \" lift / trail conditions\")\n  print (\"Current URL to check: \" + urls[num] + \"\\n\")\n\n  if (num == 0):\n    print (\"Hello.\")\n    #waterville()\n\n  if (num == 1):\n    print (\"Hello.\")\n    #cannon()\n\n  if (num == 2):\n    print (\"Hello.\")\n    #bretton_woods()\n\n  if (num == 3):\n    loon()\n\n  if (num == 4):\n    print (\"Hello.\")\n    #cranmore()\n\n  if (num == 5):\n    print (\"Hello.\")\n    #pats_peak()\n\n# Dump to JSON file now.\n# Stackoverflow post this is from:\n# https://stackoverflow.com/questions/16267767/python-writing-json-to-file\nwith open(\"json/ski.json\", \"w\") as outfile:\n  json.dump(JSON_trails, outfile, indent = 4)\n \n \n \n# Now, I can't explain the code but, it is not safe to do that.\n \n \n \n \n\n\n```\n\nThis is just a snippet. \nIt's important to"
  },
  "feature_stats": {
    "indices": [
      1,
      8243,
      628,
      4478,
      1885,
      5712,
      5938,
      5737,
      3056,
      6463,
      966
    ],
    "f_before": [
      21.319595336914062,
      2.095959424972534,
      0.0,
      0.0,
      0.0,
      0.3906625807285309,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "f_after": [
      21.319595336914062,
      2.095959424972534,
      0.0,
      0.0,
      0.0,
      0.3906625807285309,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "g_before": [
      0.2650028467178345,
      2.2559635639190674,
      2.2564823627471924,
      2.0380966663360596,
      0.0,
      2.134765625,
      2.079335927963257,
      1.9941617250442505,
      0.0017711000982671976,
      0.049624815583229065,
      2.109907865524292
    ],
    "g_after": [
      0.2650028467178345,
      2.2559635639190674,
      2.2564823627471924,
      2.0380966663360596,
      0.0,
      2.134765625,
      2.079335927963257,
      1.9941617250442505,
      0.0017711000982671976,
      0.049624815583229065,
      2.109907865524292
    ],
    "num_batches_for_stats": 10
  },
  "loss_summary": {
    "train_mean_loss": 5.925875068774563,
    "train_mean_l2": 5.399409577712417,
    "train_mean_l1": 3.046212375283241,
    "num_steps": 2000,
    "num_batches": 2000
  },
  "feature_std_summary": {
    "plot_path": "outputs/plots/20251128-154913/feature_std_layer_12.html",
    "mean_std": 1.7725918292999268,
    "max_std": 67.36790466308594
  }
}
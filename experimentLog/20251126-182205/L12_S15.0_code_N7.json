{
  "model_name": "google/gemma-2-2b-it",
  "layer_idx": 12,
  "command": "main.py sae.loss_option=2 sae.loss_module=option2_loss experiment.use_multi_contrast=true sae.alpha_concept.code=0.01 sae.alpha_concept.harm=0.01 sae.alpha_concept.struct=0.01",
  "sae": {
    "input_dim": 2304,
    "sae_dim": 9216,
    "fixed_v_cnt": 3,
    "batch_size": 4,
    "epochs": 2,
    "max_steps": 3000,
    "concept_samples_per_label": 250,
    "loss_option": 2,
    "loss_module": "option2_loss"
  },
  "gnn": {
    "use_gnn": true,
    "top_k": 10
  },
  "steering": {
    "label": "code",
    "feature_idx": 0,
    "strength": 15.0
  },
  "prompt": "import time\nfrom collections import Counter, defaultdict\nimport warnings; warnings.filterwarnings('ignore')\nimport glob\nimport re\nimport ast\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom algorithms import ShapeletTransformer\nfrom extractors.extractor import MultiGeneticExtractor\nfrom data.load_all_datasets import load_data_train_test\n\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\nfrom tslearn.shapelets import ShapeletModel\n\n\ndef parse_shapelets(shapelets):\n    shapelets = shapelets.replace(']', '],')[:-2]\n    shapelets = re.sub(r'\\s+', ', ', shapelets)\n    shapelets = re.sub(r',+', ',', shapelets)\n    shapelets = shapelets.replace('],[', '], [')\n    shapelets = shapelets.replace('[,', '[')\n    shapelets = '[' + shapelets + ']'\n    shapelets = re.sub(r',\\s+]', ']', shapelets)\n    return ast.literal_eval(shapelets)\n\ndef fit_rf(X_distances_train, y_train, X_distances_test, y_test, out_path):\n    rf = GridSearchCV(RandomForestClassifier(), {'n_estimators': [10, 25, 50, 100, 500], 'max_depth': [None, 3, 7, 15]})\n    rf.fit(X_distances_train, y_train)\n    \n    hard_preds = rf.predict(X_distances_test)\n    proba_preds = rf.predict_proba(X_distances_test)\n\n    print(\"[RF] Accuracy = {}\".format(accuracy_score(y_test, hard_preds)))\n    print(\"[RF] Logloss = {}\".format(log_loss(y_test, proba_preds)))\n\n    hard_preds = pd.DataFrame(hard_preds, columns=['prediction'])\n    proba_preds = pd.DataFrame(proba_preds, columns=['proba_{}'.format(x) for x in set(list(y_train) + list(y_test))])\n\n    hard_preds.to_csv(out_path.split('.')[0]+'_rf_hard.csv')\n    proba_preds.to_csv(out_path.split('.')[0]+'_rf_proba.csv')\n\ndef fit_lr(X_distances_train, y_train, X_distances_test, y_test, out_path):\n    lr = GridSearchCV(LogisticRegression(), {'penalty': ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1.0, 10.0]})\n    lr.fit(X_distances_train, y_train)\n    \n    hard_preds = lr.predict(X_distances_test)\n    proba_preds = lr.predict_proba(X_distances_test)\n\n    print(\"[LR] Accuracy = {}\".format(accuracy_score(y_test, hard_preds)))\n    print(\"[LR] Logloss = {}\".format(log_loss(y_test, proba_preds)))\n\n    hard_preds = pd.DataFrame(hard_preds, columns=['prediction'])\n    proba_preds = pd.DataFrame(proba_preds, columns=['proba_{}'.format(x) for x in set(list(y_train) + list(y_test))])\n\n    hard_preds.to_csv(out_path.split('.')[0]+'_lr_hard.csv')\n    proba_preds.to_csv(out_path.split('.')[0]+'_lr_proba.csv')\n\ndef fit_svm(X_distances_train, y_train, X_distances_test, y_test, out_path):\n    svc = GridSearchCV(SVC(kernel='linear', probability=True), {'C': [0.001, 0.01, 0.1, 1.0, 10.0]})\n    svc.fit(X_distances_train, y_train)\n    \n    hard_preds = svc.predict(X_distances_test)\n    proba_preds = svc.predict_proba(X_distances_test)\n\n    print(\"[SVM] Accuracy = {}\".format(accuracy_score(y_test, hard_preds)))\n    print(\"[SVM] Logloss = {}\".format(log_loss(y_test, proba_preds)))\n\n    hard_preds = pd.DataFrame(hard_preds, columns=['prediction'])\n    proba_preds = pd.DataFrame(proba_preds, columns=['proba_{}'.format(x) for x in set(list(y_train) + list(y_test))])\n\n    hard_preds.to_csv(out_path.split('.')[0]+'_svm_hard.csv')\n    proba_preds.to_csv(out_path.split('.')[0]+'_svm_proba.csv')\n\ndef fit_lts(X_train, y_train, X_test, y_test, shap_dict, reg, max_it, shap_out_path, pred_out_path, timing_out_path):\n    # Fit LTS model, print metrics on test-set, write away predictions and shapelets\n    clf = ShapeletModel(n_shapelets_per_size=shap_dict, \n                        max_iter=max_it, verbose_level=0, batch_size=1,\n                        optimizer='sgd', weight_regularizer=reg)\n\n    start = time.time()\n    clf.fit(\n        np.reshape(\n            X_train, \n            (X_train.shape[0], X_train.shape[1], 1)\n        ), \n        y_train\n    )\n    learning_time = time.time() - start\n\n    print('Learning shapelets took {}s'.format(learning_time))\n\n    with open(shap_out_path, 'w+') as ofp:\n        for shap in clf.shapelets_:\n            ofp.write(str(np.reshape(shap, (-1))) + '\\n')\n\n    with open(timing_out_path, 'w+') as ofp:\n        ofp.write(str(learning_time))\n\n    X_distances_train = clf.transform(X_train)\n    X_distances_test = clf.transform(X_test)\n\n    print('Max distance value = {}'.format(np.max(X_distances_train)))\n\n    fit_rf(X_distances_train, y_train, X_distances_test, y_test, pred_out_path)\n    fit_lr(X_distances_train, y_train, X_distances_test, y_test, pred_out_path)\n    fit_svm(X_distances_train, y_train, X_distances_test, y_test, pred_out_path)\n\nhyper_parameters_lts = {\n\t'Adiac': \t\t\t\t\t[0.3,  0.2,   3, 0.01, 10000],\n\t'Beef': \t\t\t\t\t[0.15, 0.125, 3, 0.01, 10000],\n\t'BeetleFly': \t\t\t\t[0.15, 0.125, 1, 0.01, 5000],\n\t'BirdChicken': \t\t\t\t[0.3,  0.075, 1, 0.1,  10000],\n\t'ChlorineConcentration':    [0.3,  0.2,   3, 0.01, 10000],\n\t'Coffee': \t\t\t\t\t[0.05, 0.075, 2, 0.01, 5000],\n\t'DiatomSizeReduction': \t\t[0.3,  0.175, 2, 0.01, 10000],\n\t'ECGFiveDays': \t\t\t\t[0.05, 0.125, 2, 0.01, 10000],\n\t'FaceFour': \t\t\t\t[0.3,  0.175, 3, 1.0,  5000],\n\t'GunPoint': \t\t\t\t[0.15, 0.2,   3, 0.1,  10000],\n\t'ItalyPowerDemand':\t\t\t[0.3,  0.2,   3, 0.01, 5000],\n\t'Lightning7': \t\t\t\t[0.05, 0.075, 3, 1,    5000],\n\t'MedicalImages': \t\t\t[0.3,  0.2,   2, 1,    10000],\n\t'MoteStrain': \t\t\t\t[0.3,  0.2,   3, 1,    10000],\n\t#NOT AVAILABLE#'Otoliths': \t\t\t\t[0.15, 0.125, 3, 0.01, 2000],\n\t'SonyAIBORobotSurface1': \t[0.3,  0.125, 2, 0.01, 10000],\n\t'SonyAIBORobotSurface2': \t[0.3,  0.125, 2, 0.01, 10000],\n\t'Symbols': \t\t\t\t\t[0.05, 0.175, 1, 0.1,  5000],\n\t'SyntheticControl': \t\t[0.15, 0.125, 3, 0.01, 5000],\n\t'Trace': \t\t\t\t\t[0.15, 0.125, 2, 0.1,  10000],\n\t'TwoLeadECG': \t\t\t\t[0.3,  0.075, 1, 0.1,  10000]\n}\n\ndatasets = [\n    'Adiac',\n    'Beef',\n    'BeetleFly',\n    'BirdChicken',\n    'ChlorineConcentration',\n    'Coffee',\n    'ECGFiveDays',\n    'FaceFour',\n    'GunPoint',\n    'ItalyPowerDemand',\n    'Lightning7',\n    'MedicalImages',\n    'MoteStrain',\n    'SonyAIBORobotSurface1',\n    'SonyAIBORobotSurface2',\n    'Symbols',\n    'SyntheticControl',\n    'Trace',\n    'TwoLeadECG',\n    'DiatomSizeReduction'\n]\n\nlearning_sizes = defaultdict(list)\ngenetic_sizes = defaultdict(list)\n\nmetadata = sorted(load_data_train_test(), key=lambda x: x['train']['n_samples']**2*x['train']['n_features']**3)\n\nfor dataset in metadata:\n\n    train_df = pd.read_csv(dataset['train']['data_path'])\n    test_df = pd.read_csv(dataset['test']['data_path'])\n    X_train = train_df.drop('target', axis=1).values\n    y_train = train_df['target']\n    X_test = test_df.drop('target', axis=1).values\n    y_test = test_df['target']\n\n    map_dict = {}\n    for j, c in enumerate(np.unique(y_train)):\n        map_dict[c] = j\n    y_train = y_train.map(map_dict) \n    y_test = y_test.map(map_dict)\n\n    y_train = y_train.values\n    y_test = y_test.values\n\n    nr_shap, l, r, reg, max_it = hyper_parameters_lts[dataset['train']['name']]\n    \n    files = glob.glob('results/lts_vs_genetic/{}_genetic_shapelets*.txt'.format(dataset['train']['name']))\n    if len(files):\n\t    sizes = []\n\t    for f in files:\n\t        shaps = parse_shapelets(open(f, 'r').read())\n\t        genetic_sizes[dataset['train']['name']].append(len(shaps))\n\t        for s in shaps:\n\t        \tsizes.append(len(s))\n\t        \n\t    shap_dict_cntr = Counter(np.random.choice(sizes, size=int(np.mean(genetic_sizes[dataset['train']['name']]))))\n\t    shap_dict = {}\n\t    for c in shap_dict_cntr:\n\t    \tshap_dict[int(c)] = int(shap_dict_cntr[c])\n\n\t    fit_lts(X_train, y_train, X_test, y_test, dict(shap_dict), reg, max_it,\n\t            'results/lts_smaller/{}_learned_shapelets_{}.txt'.format(dataset['train']['name'], int(time.time())), \n\t            'results/lts_smaller/{}_learned_shapelets_predictions_{}.csv'.format(dataset['train']['name'], int(time.time())), \n\t            'results/lts_smaller/{}_learned_runtime_{}.csv'.format(dataset['train']['name'], int(time.time()))\n\t    )",
  "output": {
    "before": "import time\nfrom collections import Counter, defaultdict\nimport warnings; warnings.filterwarnings('ignore')\nimport glob\nimport re\nimport ast\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom algorithms import ShapeletTransformer\nfrom extractors.extractor import MultiGeneticExtractor\nfrom data.load_all_datasets import load_data_train_test\n\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\nfrom tslearn.shapelets import ShapeletModel\n\n\ndef parse_shapelets(shapelets):\n    shapelets = shapelets.replace(']', '],')[:-2]\n    shapelets = re.sub(r'\\s+', ', ', shapelets)\n    shapelets = re.sub(r',+', ',', shapelets)\n    shapelets = shapelets.replace('],[', '], [')\n    shapelets = shapelets.replace('[,', '[')\n    shapelets = '[' + shapelets + ']'\n    shapelets = re.sub(r',\\s+]', ']', shapelets)\n    return ast.literal_eval(shapelets)\n\ndef fit_rf(X_distances_train, y_train, X_distances_test, y_test, out_path):\n    rf = GridSearchCV(RandomForestClassifier(), {'n_estimators': [10, 25, 50, 100, 500], 'max_depth': [None, 3, 7, 15]})\n    rf.fit(X_distances_train, y_train)\n    \n    hard_preds = rf.predict(X_distances_test)\n    proba_preds = rf.predict_proba(X_distances_test)\n\n    print(\"[RF] Accuracy = {}\".format(accuracy_score(y_test, hard_preds)))\n    print(\"[RF] Logloss = {}\".format(log_loss(y_test, proba_preds)))\n\n    hard_preds = pd.DataFrame(hard_preds, columns=['prediction'])\n    proba_preds = pd.DataFrame(proba_preds, columns=['proba_{}'.format(x) for x in set(list(y_train) + list(y_test))])\n\n    hard_preds.to_csv(out_path.split('.')[0]+'_rf_hard.csv')\n    proba_preds.to_csv(out_path.split('.')[0]+'_rf_proba.csv')\n\ndef fit_lr(X_distances_train, y_train, X_distances_test, y_test, out_path):\n    lr = GridSearchCV(LogisticRegression(), {'penalty': ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1.0, 10.0]})\n    lr.fit(X_distances_train, y_train)\n    \n    hard_preds = lr.predict(X_distances_test)\n    proba_preds = lr.predict_proba(X_distances_test)\n\n    print(\"[LR] Accuracy = {}\".format(accuracy_score(y_test, hard_preds)))\n    print(\"[LR] Logloss = {}\".format(log_loss(y_test, proba_preds)))\n\n    hard_preds = pd.DataFrame(hard_preds, columns=['prediction'])\n    proba_preds = pd.DataFrame(proba_preds, columns=['proba_{}'.format(x) for x in set(list(y_train) + list(y_test))])\n\n    hard_preds.to_csv(out_path.split('.')[0]+'_lr_hard.csv')\n    proba_preds.to_csv(out_path.split('.')[0]+'_lr_proba.csv')\n\ndef fit_svm(X_distances_train, y_train, X_distances_test, y_test, out_path):\n    svc = GridSearchCV(SVC(kernel='linear', probability=True), {'C': [0.001, 0.01, 0.1, 1.0, 10.0]})\n    svc.fit(X_distances_train, y_train)\n    \n    hard_preds = svc.predict(X_distances_test)\n    proba_preds = svc.predict_proba(X_distances_test)\n\n    print(\"[SVM] Accuracy = {}\".format(accuracy_score(y_test, hard_preds)))\n    print(\"[SVM] Logloss = {}\".format(log_loss(y_test, proba_preds)))\n\n    hard_preds = pd.DataFrame(hard_preds, columns=['prediction'])\n    proba_preds = pd.DataFrame(proba_preds, columns=['proba_{}'.format(x) for x in set(list(y_train) + list(y_test))])\n\n    hard_preds.to_csv(out_path.split('.')[0]+'_svm_hard.csv')\n    proba_preds.to_csv(out_path.split('.')[0]+'_svm_proba.csv')\n\ndef fit_lts(X_train, y_train, X_test, y_test, shap_dict, reg, max_it, shap_out_path, pred_out_path, timing_out_path):\n    # Fit LTS model, print metrics on test-set, write away predictions and shapelets\n    clf = ShapeletModel(n_shapelets_per_size=shap_dict, \n                        max_iter=max_it, verbose_level=0, batch_size=1,\n                        optimizer='sgd', weight_regularizer=reg)\n\n    start = time.time()\n    clf.fit(\n        np.reshape(\n            X_train, \n            (X_train.shape[0], X_train.shape[1], 1)\n        ), \n        y_train\n    )\n    learning_time = time.time() - start\n\n    print('Learning shapelets took {}s'.format(learning_time))\n\n    with open(shap_out_path, 'w+') as ofp:\n        for shap in clf.shapelets_:\n            ofp.write(str(np.reshape(shap, (-1))) + '\\n')\n\n    with open(timing_out_path, 'w+') as ofp:\n        ofp.write(str(learning_time))\n\n    X_distances_train = clf.transform(X_train)\n    X_distances_test = clf.transform(X_test)\n\n    print('Max distance value = {}'.format(np.max(X_distances_train)))\n\n    fit_rf(X_distances_train, y_train, X_distances_test, y_test, pred_out_path)\n    fit_lr(X_distances_train, y_train, X_distances_test, y_test, pred_out_path)\n    fit_svm(X_distances_train, y_train, X_distances_test, y_test, pred_out_path)\n\nhyper_parameters_lts = {\n\t'Adiac': \t\t\t\t\t[0.3,  0.2,   3, 0.01, 10000],\n\t'Beef': \t\t\t\t\t[0.15, 0.125, 3, 0.01, 10000],\n\t'BeetleFly': \t\t\t\t[0.15, 0.125, 1, 0.01, 5000],\n\t'BirdChicken': \t\t\t\t[0.3,  0.075, 1, 0.1,  10000],\n\t'ChlorineConcentration':    [0.3,  0.2,   3, 0.01, 10000],\n\t'Coffee': \t\t\t\t\t[0.05, 0.075, 2, 0.01, 5000],\n\t'DiatomSizeReduction': \t\t[0.3,  0.175, 2, 0.01, 10000],\n\t'ECGFiveDays': \t\t\t\t[0.05, 0.125, 2, 0.01, 10000],\n\t'FaceFour': \t\t\t\t[0.3,  0.175, 3, 1.0,  5000],\n\t'GunPoint': \t\t\t\t[0.15, 0.2,   3, 0.1,  10000],\n\t'ItalyPowerDemand':\t\t\t[0.3,  0.2,   3, 0.01, 5000],\n\t'Lightning7': \t\t\t\t[0.05, 0.075, 3, 1,    5000],\n\t'MedicalImages': \t\t\t[0.3,  0.2,   2, 1,    10000],\n\t'MoteStrain': \t\t\t\t[0.3,  0.2,   3, 1,    10000],\n\t#NOT AVAILABLE#'Otoliths': \t\t\t\t[0.15, 0.125, 3, 0.01, 2000],\n\t'SonyAIBORobotSurface1': \t[0.3,  0.125, 2, 0.01, 10000],\n\t'SonyAIBORobotSurface2': \t[0.3,  0.125, 2, 0.01, 10000],\n\t'Symbols': \t\t\t\t\t[0.05, 0.175, 1, 0.1,  5000],\n\t'SyntheticControl': \t\t[0.15, 0.125, 3, 0.01, 5000],\n\t'Trace': \t\t\t\t\t[0.15, 0.125, 2, 0.1,  10000],\n\t'TwoLeadECG': \t\t\t\t[0.3,  0.075, 1, 0.1,  10000]\n}\n\ndatasets = [\n    'Adiac',\n    'Beef',\n    'BeetleFly',\n    'BirdChicken',\n    'ChlorineConcentration',\n    'Coffee',\n    'ECGFiveDays',\n    'FaceFour',\n    'GunPoint',\n    'ItalyPowerDemand',\n    'Lightning7',\n    'MedicalImages',\n    'MoteStrain',\n    'SonyAIBORobotSurface1',\n    'SonyAIBORobotSurface2',\n    'Symbols',\n    'SyntheticControl',\n    'Trace',\n    'TwoLeadECG',\n    'DiatomSizeReduction'\n]\n\nlearning_sizes = defaultdict(list)\ngenetic_sizes = defaultdict(list)\n\nmetadata = sorted(load_data_train_test(), key=lambda x: x['train']['n_samples']**2*x['train']['n_features']**3)\n\nfor dataset in metadata:\n\n    train_df = pd.read_csv(dataset['train']['data_path'])\n    test_df = pd.read_csv(dataset['test']['data_path'])\n    X_train = train_df.drop('target', axis=1).values\n    y_train = train_df['target']\n    X_test = test_df.drop('target', axis=1).values\n    y_test = test_df['target']\n\n    map_dict = {}\n    for j, c in enumerate(np.unique(y_train)):\n        map_dict[c] = j\n    y_train = y_train.map(map_dict) \n    y_test = y_test.map(map_dict)\n\n    y_train = y_train.values\n    y_test = y_test.values\n\n    nr_shap, l, r, reg, max_it = hyper_parameters_lts[dataset['train']['name']]\n    \n    files = glob.glob('results/lts_vs_genetic/{}_genetic_shapelets*.txt'.format(dataset['train']['name']))\n    if len(files):\n\t    sizes = []\n\t    for f in files:\n\t        shaps = parse_shapelets(open(f, 'r').read())\n\t        genetic_sizes[dataset['train']['name']].append(len(shaps))\n\t        for s in shaps:\n\t        \tsizes.append(len(s))\n\t        \n\t    shap_dict_cntr = Counter(np.random.choice(sizes, size=int(np.mean(genetic_sizes[dataset['train']['name']]))))\n\t    shap_dict = {}\n\t    for c in shap_dict_cntr:\n\t    \tshap_dict[int(c)] = int(shap_dict_cntr[c])\n\n\t    fit_lts(X_train, y_train, X_test, y_test, dict(shap_dict), reg, max_it,\n\t            'results/lts_smaller/{}_learned_shapelets_{}.txt'.format(dataset['train']['name'], int(time.time())), \n\t            'results/lts_smaller/{}_learned_shapelets_predictions_{}.csv'.format(dataset['train']['name'], int(time.time())), \n\t            'results/lts_smaller/{}_learned_runtime_{}.csv'.format(dataset['train']['name'], int(time.time()))\n\t    )\n\n    else:\n        print(\"No shapelets found for this dataset\")\n\n    # Train and evaluate the LTS model\n    fit_lts(X_train, y_train, X_test, y_test, shap_",
    "after": "import time\nfrom collections import Counter, defaultdict\nimport warnings; warnings.filterwarnings('ignore')\nimport glob\nimport re\nimport ast\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom algorithms import ShapeletTransformer\nfrom extractors.extractor import MultiGeneticExtractor\nfrom data.load_all_datasets import load_data_train_test\n\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\nfrom tslearn.shapelets import ShapeletModel\n\n\ndef parse_shapelets(shapelets):\n    shapelets = shapelets.replace(']', '],')[:-2]\n    shapelets = re.sub(r'\\s+', ', ', shapelets)\n    shapelets = re.sub(r',+', ',', shapelets)\n    shapelets = shapelets.replace('],[', '], [')\n    shapelets = shapelets.replace('[,', '[')\n    shapelets = '[' + shapelets + ']'\n    shapelets = re.sub(r',\\s+]', ']', shapelets)\n    return ast.literal_eval(shapelets)\n\ndef fit_rf(X_distances_train, y_train, X_distances_test, y_test, out_path):\n    rf = GridSearchCV(RandomForestClassifier(), {'n_estimators': [10, 25, 50, 100, 500], 'max_depth': [None, 3, 7, 15]})\n    rf.fit(X_distances_train, y_train)\n    \n    hard_preds = rf.predict(X_distances_test)\n    proba_preds = rf.predict_proba(X_distances_test)\n\n    print(\"[RF] Accuracy = {}\".format(accuracy_score(y_test, hard_preds)))\n    print(\"[RF] Logloss = {}\".format(log_loss(y_test, proba_preds)))\n\n    hard_preds = pd.DataFrame(hard_preds, columns=['prediction'])\n    proba_preds = pd.DataFrame(proba_preds, columns=['proba_{}'.format(x) for x in set(list(y_train) + list(y_test))])\n\n    hard_preds.to_csv(out_path.split('.')[0]+'_rf_hard.csv')\n    proba_preds.to_csv(out_path.split('.')[0]+'_rf_proba.csv')\n\ndef fit_lr(X_distances_train, y_train, X_distances_test, y_test, out_path):\n    lr = GridSearchCV(LogisticRegression(), {'penalty': ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1.0, 10.0]})\n    lr.fit(X_distances_train, y_train)\n    \n    hard_preds = lr.predict(X_distances_test)\n    proba_preds = lr.predict_proba(X_distances_test)\n\n    print(\"[LR] Accuracy = {}\".format(accuracy_score(y_test, hard_preds)))\n    print(\"[LR] Logloss = {}\".format(log_loss(y_test, proba_preds)))\n\n    hard_preds = pd.DataFrame(hard_preds, columns=['prediction'])\n    proba_preds = pd.DataFrame(proba_preds, columns=['proba_{}'.format(x) for x in set(list(y_train) + list(y_test))])\n\n    hard_preds.to_csv(out_path.split('.')[0]+'_lr_hard.csv')\n    proba_preds.to_csv(out_path.split('.')[0]+'_lr_proba.csv')\n\ndef fit_svm(X_distances_train, y_train, X_distances_test, y_test, out_path):\n    svc = GridSearchCV(SVC(kernel='linear', probability=True), {'C': [0.001, 0.01, 0.1, 1.0, 10.0]})\n    svc.fit(X_distances_train, y_train)\n    \n    hard_preds = svc.predict(X_distances_test)\n    proba_preds = svc.predict_proba(X_distances_test)\n\n    print(\"[SVM] Accuracy = {}\".format(accuracy_score(y_test, hard_preds)))\n    print(\"[SVM] Logloss = {}\".format(log_loss(y_test, proba_preds)))\n\n    hard_preds = pd.DataFrame(hard_preds, columns=['prediction'])\n    proba_preds = pd.DataFrame(proba_preds, columns=['proba_{}'.format(x) for x in set(list(y_train) + list(y_test))])\n\n    hard_preds.to_csv(out_path.split('.')[0]+'_svm_hard.csv')\n    proba_preds.to_csv(out_path.split('.')[0]+'_svm_proba.csv')\n\ndef fit_lts(X_train, y_train, X_test, y_test, shap_dict, reg, max_it, shap_out_path, pred_out_path, timing_out_path):\n    # Fit LTS model, print metrics on test-set, write away predictions and shapelets\n    clf = ShapeletModel(n_shapelets_per_size=shap_dict, \n                        max_iter=max_it, verbose_level=0, batch_size=1,\n                        optimizer='sgd', weight_regularizer=reg)\n\n    start = time.time()\n    clf.fit(\n        np.reshape(\n            X_train, \n            (X_train.shape[0], X_train.shape[1], 1)\n        ), \n        y_train\n    )\n    learning_time = time.time() - start\n\n    print('Learning shapelets took {}s'.format(learning_time))\n\n    with open(shap_out_path, 'w+') as ofp:\n        for shap in clf.shapelets_:\n            ofp.write(str(np.reshape(shap, (-1))) + '\\n')\n\n    with open(timing_out_path, 'w+') as ofp:\n        ofp.write(str(learning_time))\n\n    X_distances_train = clf.transform(X_train)\n    X_distances_test = clf.transform(X_test)\n\n    print('Max distance value = {}'.format(np.max(X_distances_train)))\n\n    fit_rf(X_distances_train, y_train, X_distances_test, y_test, pred_out_path)\n    fit_lr(X_distances_train, y_train, X_distances_test, y_test, pred_out_path)\n    fit_svm(X_distances_train, y_train, X_distances_test, y_test, pred_out_path)\n\nhyper_parameters_lts = {\n\t'Adiac': \t\t\t\t\t[0.3,  0.2,   3, 0.01, 10000],\n\t'Beef': \t\t\t\t\t[0.15, 0.125, 3, 0.01, 10000],\n\t'BeetleFly': \t\t\t\t[0.15, 0.125, 1, 0.01, 5000],\n\t'BirdChicken': \t\t\t\t[0.3,  0.075, 1, 0.1,  10000],\n\t'ChlorineConcentration':    [0.3,  0.2,   3, 0.01, 10000],\n\t'Coffee': \t\t\t\t\t[0.05, 0.075, 2, 0.01, 5000],\n\t'DiatomSizeReduction': \t\t[0.3,  0.175, 2, 0.01, 10000],\n\t'ECGFiveDays': \t\t\t\t[0.05, 0.125, 2, 0.01, 10000],\n\t'FaceFour': \t\t\t\t[0.3,  0.175, 3, 1.0,  5000],\n\t'GunPoint': \t\t\t\t[0.15, 0.2,   3, 0.1,  10000],\n\t'ItalyPowerDemand':\t\t\t[0.3,  0.2,   3, 0.01, 5000],\n\t'Lightning7': \t\t\t\t[0.05, 0.075, 3, 1,    5000],\n\t'MedicalImages': \t\t\t[0.3,  0.2,   2, 1,    10000],\n\t'MoteStrain': \t\t\t\t[0.3,  0.2,   3, 1,    10000],\n\t#NOT AVAILABLE#'Otoliths': \t\t\t\t[0.15, 0.125, 3, 0.01, 2000],\n\t'SonyAIBORobotSurface1': \t[0.3,  0.125, 2, 0.01, 10000],\n\t'SonyAIBORobotSurface2': \t[0.3,  0.125, 2, 0.01, 10000],\n\t'Symbols': \t\t\t\t\t[0.05, 0.175, 1, 0.1,  5000],\n\t'SyntheticControl': \t\t[0.15, 0.125, 3, 0.01, 5000],\n\t'Trace': \t\t\t\t\t[0.15, 0.125, 2, 0.1,  10000],\n\t'TwoLeadECG': \t\t\t\t[0.3,  0.075, 1, 0.1,  10000]\n}\n\ndatasets = [\n    'Adiac',\n    'Beef',\n    'BeetleFly',\n    'BirdChicken',\n    'ChlorineConcentration',\n    'Coffee',\n    'ECGFiveDays',\n    'FaceFour',\n    'GunPoint',\n    'ItalyPowerDemand',\n    'Lightning7',\n    'MedicalImages',\n    'MoteStrain',\n    'SonyAIBORobotSurface1',\n    'SonyAIBORobotSurface2',\n    'Symbols',\n    'SyntheticControl',\n    'Trace',\n    'TwoLeadECG',\n    'DiatomSizeReduction'\n]\n\nlearning_sizes = defaultdict(list)\ngenetic_sizes = defaultdict(list)\n\nmetadata = sorted(load_data_train_test(), key=lambda x: x['train']['n_samples']**2*x['train']['n_features']**3)\n\nfor dataset in metadata:\n\n    train_df = pd.read_csv(dataset['train']['data_path'])\n    test_df = pd.read_csv(dataset['test']['data_path'])\n    X_train = train_df.drop('target', axis=1).values\n    y_train = train_df['target']\n    X_test = test_df.drop('target', axis=1).values\n    y_test = test_df['target']\n\n    map_dict = {}\n    for j, c in enumerate(np.unique(y_train)):\n        map_dict[c] = j\n    y_train = y_train.map(map_dict) \n    y_test = y_test.map(map_dict)\n\n    y_train = y_train.values\n    y_test = y_test.values\n\n    nr_shap, l, r, reg, max_it = hyper_parameters_lts[dataset['train']['name']]\n    \n    files = glob.glob('results/lts_vs_genetic/{}_genetic_shapelets*.txt'.format(dataset['train']['name']))\n    if len(files):\n\t    sizes = []\n\t    for f in files:\n\t        shaps = parse_shapelets(open(f, 'r').read())\n\t        genetic_sizes[dataset['train']['name']].append(len(shaps))\n\t        for s in shaps:\n\t        \tsizes.append(len(s))\n\t        \n\t    shap_dict_cntr = Counter(np.random.choice(sizes, size=int(np.mean(genetic_sizes[dataset['train']['name']]))))\n\t    shap_dict = {}\n\t    for c in shap_dict_cntr:\n\t    \tshap_dict[int(c)] = int(shap_dict_cntr[c])\n\n\t    fit_lts(X_train, y_train, X_test, y_test, dict(shap_dict), reg, max_it,\n\t            'results/lts_smaller/{}_learned_shapelets_{}.txt'.format(dataset['train']['name'], int(time.time())), \n\t            'results/lts_smaller/{}_learned_shapelets_predictions_{}.csv'.format(dataset['train']['name'], int(time.time())), \n\t            'results/lts_smaller/{}_learned_runtime_{}.csv'.format(dataset['train']['name'], int(time.time()))\n\t    )\n\t    \n    # Add additional shapelets to be added later\n    # shapelets = []\n    # for i in range(0, 10):\n    #     shapelets.append(i)\n\n    # Shape"
  },
  "feature_stats": {
    "indices": [
      0,
      2,
      8203,
      746,
      7934,
      7916,
      989,
      6557,
      7949,
      5214,
      160
    ],
    "f_before": [
      62.0091552734375,
      0.9933224320411682,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "f_after": [
      62.0091552734375,
      0.9933224320411682,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "g_before": [
      0.5764716863632202,
      31.24457550048828,
      7.032981872558594,
      7.174556255340576,
      6.6328935623168945,
      5.975185871124268,
      5.854887008666992,
      6.20868444442749,
      0.0,
      5.872464179992676,
      5.811121463775635
    ],
    "g_after": [
      0.5764716863632202,
      31.24457550048828,
      7.032981872558594,
      7.174556255340576,
      6.6328935623168945,
      5.975185871124268,
      5.854887008666992,
      6.20868444442749,
      0.0,
      5.872464179992676,
      5.811121463775635
    ],
    "num_batches_for_stats": 10
  },
  "loss_summary": {
    "train_mean_loss": 5.720714295725028,
    "train_mean_l2": 3.934641513635715,
    "train_mean_l1": 2.149144583642483,
    "num_steps": 3000,
    "num_batches": 3000
  },
  "feature_std_summary": {
    "plot_path": "outputs/plots/20251126-182205/feature_std_layer_12.html",
    "mean_std": 1.436635136604309,
    "max_std": 39.19367599487305
  }
}
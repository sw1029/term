{
  "model_name": "google/gemma-2-2b-it",
  "layer_idx": 0,
  "command": "main.py experiment.use_multi_contrast=true sae.loss_option=2 sae.loss_module=option2_loss sae.guidance_method=contrastive sae.guidance_coeff=1.0 sae.alpha_concept.code=0.025 sae.alpha_concept.harm=0.08 sae.alpha_concept.struct=0.0 sae.use_l0=true sae.l0_coeff=0.0008844983595465838 sae.layer_idx=0 experiment.layer_sweep=[0] experiment.strength_sweep=[15.0]",
  "sae": {
    "input_dim": 2304,
    "sae_dim": 9216,
    "fixed_v_cnt": 3,
    "batch_size": 4,
    "epochs": 2,
    "max_steps": 2000,
    "l1_coeff": 0.005,
    "l0_coeff": 0.0008844983595465838,
    "use_l0": true,
    "concept_samples_per_label": 100,
    "loss_option": 2,
    "loss_module": "option2_loss",
    "concept_feature_indices": {
      "code": 0,
      "harm": 1,
      "struct": 2
    },
    "alpha_concept": {
      "code": 0.025,
      "harm": 0.08,
      "struct": 0.0
    },
    "positive_targets": {
      "code": 1,
      "harm": 1,
      "struct": 1
    },
    "guidance_method": "contrastive",
    "guidance_margin": 0.0,
    "guidance_coeff": 1.0,
    "guidance_mse_pos_value": 1.0,
    "guidance_mse_neg_value": 0.0,
    "jumprelu_bandwidth": 0.001,
    "jumprelu_init_threshold": 0.001
  },
  "gnn": {
    "use_gnn": true,
    "top_k": 10
  },
  "experiment": {
    "device": "cuda",
    "dataset_name": "wikitext",
    "dataset_config": "wikitext-2-raw-v1",
    "use_multi_contrast": true,
    "num_steering_samples_per_label": 10,
    "top_k_for_plot": 10,
    "layer_sweep": [
      0
    ],
    "strength_sweep": [
      15.0
    ]
  },
  "steering": {
    "label": "code",
    "feature_idx": 0,
    "strength": 15.0,
    "hook_layer_idx": 0
  },
  "prompt": "import asyncio\nimport traceback\nfrom datetime import datetime\nfrom neo.Network.core.header import Header\nfrom typing import TYPE_CHECKING, List\nfrom neo.Network.flightinfo import FlightInfo\nfrom neo.Network.requestinfo import RequestInfo\nfrom neo.Network.payloads.inventory import InventoryType\nfrom neo.Network.common import msgrouter\nfrom neo.Network.common.singleton import Singleton\nfrom contextlib import suppress\nfrom neo.Network.core.uint256 import UInt256\n\nfrom neo.logging import log_manager\n\nlogger = log_manager.getLogger('",
  "output": {
    "before": "import asyncio\nimport traceback\nfrom datetime import datetime\nfrom neo.Network.core.header import Header\nfrom typing import TYPE_CHECKING, List\nfrom neo.Network.flightinfo import FlightInfo\nfrom neo.Network.requestinfo import RequestInfo\nfrom neo.Network.payloads.inventory import InventoryType\nfrom neo.Network.common import msgrouter\nfrom neo.Network.common.singleton import Singleton\nfrom contextlib import suppress\nfrom neo.Network.core.uint256 import UInt256\n\nfrom neo.logging import log_manager\n\nlogger = log_manager.getLogger('syncmanager')\n# log_manager.config_stdio([('syncmanager', 10)])\n\nif TYPE_CHECKING:\n    from neo.Network.nodemanager import NodeManager\n    from neo.Network.payloads import Block\n\n\nclass SyncManager(Singleton):\n    HEADER_MAX_LOOK_AHEAD = 6000\n    HEADER_REQUEST_TIMEOUT = 5\n\n    BLOCK_MAX_CACHE_SIZE = 500\n    BLOCK_NETWORK_REQ_LIMIT = 500\n    BLOCK_REQUEST_TIMEOUT = 5\n\n    def init(self, nodemgr: 'NodeManager'):\n        self.nodemgr = nodemgr\n        self.controller = None\n        self.block_requests = dict()  # header_hash:RequestInfo\n        self.header_request = None  # type: RequestInfo\n        self.ledger = None\n        self.block_cache = []\n        self.header_cache = []\n        self.raw_block_cache = []\n        self.is_persisting_blocks = False\n        self.is_persisting_headers = False\n        self.keep_running = True\n        self.service_task = None\n        self.persist_task = None\n        self.health_task = None\n\n        msgrouter.on_headers += self.on_headers_received\n        msgrouter.on_block += self.on_block_received\n\n    async def start(self) -> None:\n        while not self.nodemgr.running:\n            await asyncio.sleep(0.1)\n        self.service_task = asyncio.create_task(self.run_service())\n        self.health_task = asyncio.create_task(self.block_health())\n\n    async def shutdown(self):\n        print(\"Shutting down sync manager...\", end='')\n        self.keep_running = False\n        self.block_cache = []\n        shutdown_tasks = []\n\n        # start up errors can cause the tasks to not have been assigned,\n        # so we must validate their presence before feeding them to `gather`\n        if self.service_task:\n            shutdown_tasks.append(self.service_task)\n\n        if self.health_task:\n            shutdown_tasks.append(self.health_task)\n\n        if self.persist_task:\n            shutdown_tasks.append(self.persist_task)\n        await asyncio.gather(*shutdown_tasks, return_exceptions=True)\n\n        print(\"DONE\")\n\n    async def block_health(self):\n        # TODO: move this to nodemanager, once the network in general supports ping/pong\n        #  we can then make smarter choices by looking at individual nodes advancing or not and dropping just those\n        error_counter = 0\n        last_height = await self.ledger.cur_block_height()\n        while self.keep_running:\n            await asyncio.sleep(15)\n            cur_height = await self.ledger.cur_block_height()\n            if cur_height == last_height:\n                error_counter += 1\n                if error_counter == 3:\n                    to_disconnect = list(map(lambda n: n, self.nodemgr.nodes))\n                    logger.debug(f\"Block height not advancing. Replacing nodes: {to_disconnect}\")\n                    for n in to_disconnect:\n                        await self.nodemgr.replace_node(n)\n            else:\n                error_counter = 0\n\n            last_height = cur_height\n\n    async def run_service(self):\n        while self.keep_running:\n            await self.check_timeout()\n            await self.sync()\n            await asyncio.sleep(1)\n\n    async def sync(self) -> None:\n        await self.sync_header()\n        await self.sync_block()\n        await self.persist_headers()\n        if not self.is_persisting_blocks:\n            self.persist_task = asyncio.create_task(self.persist_blocks())\n\n    async def sync_header(self) -> None:\n        if self.header_request:\n            return\n\n        cur_header_height = await self.ledger.cur_header_height()\n        cur_block_height = await self.ledger.cur_block_height()\n        if cur_header_height - cur_block_height >= self.HEADER_MAX_LOOK_AHEAD:\n            return\n\n        node = self.nodemgr.get_next_node(cur_header_height + 1)\n        if not node:\n            # No connected nodes or no nodes with our height. We'll wait for node manager to resolve this\n            # or for the nodes to increase their height on the next produced block\n            return\n\n        self.header_request = RequestInfo(cur_header_height + 1)\n        self.header_request.add_new_flight(FlightInfo(node.nodeid, cur_header_height + 1))\n\n        cur_header_hash = await self.ledger.header_hash_by_height(cur_header_height)\n        await node.get_headers(hash_start=cur_header_hash)\n\n        logger.debug(f\"Requested headers starting at {cur_header_height + 1} from node {node.nodeid_human}\")\n        node.nodeweight.append_new_request_time()\n\n    async def persist_headers(self):\n        self.is_persisting_headers = True\n        if len(self.header_cache) > 0:\n            while self.keep_running:\n                try:\n                    headers = self.header_cache.pop(0)\n                    try:\n                        await self.ledger.add_headers(headers)\n                    except Exception as e:\n                        print(traceback.format_exc())\n                    await asyncio.sleep(0)\n                except IndexError:\n                    # cache empty\n                    break\n\n            # reset header_request such that the a new header sync task can be added\n            self.header_request = None\n            logger.debug(\"Finished processing headers\")\n\n        self.is_persisting_headers = False\n\n    async def sync_block(self) -> None:\n        # to simplify syncing, don't ask for more data if we still have requests in flight\n        if len(self.block_requests) > 0:\n            return\n\n        # the block cache might not have been fully processed, so we want to avoid asking for data we actually already have\n        best_block_height = await self.get_best_stored_block_height()\n        cur_header_height = await self.ledger.cur_header_height()\n        blocks_to_fetch = cur_header_height - best_block_height\n        if blocks_to_fetch <= 0:\n            return\n\n        block_cache_space = self.BLOCK_MAX_CACHE_SIZE - len(self.block_cache)\n        if block_cache_space <= 0:\n            return\n\n        if blocks_to_fetch > block_cache_space or blocks_to_fetch > self.BLOCK_NETWORK_REQ_LIMIT:\n            blocks_to_fetch = min(block_cache_space, self.BLOCK_NETWORK_REQ_LIMIT)\n\n        try:\n            best_node_height = max(map(lambda node: node.best_height, self.nodemgr.nodes))\n        except ValueError:\n            # if the node list is empty max() fails on an empty list\n            return\n\n        node = self.nodemgr.get_next_node(best_node_height)\n        if not node:\n            # no nodes with our desired height. We'll wait for node manager to resolve this\n            # or for the nodes to increase their height on the next produced block\n            return\n\n        hashes = []\n        endheight = None\n        for i in range(1, blocks_to_fetch + 1):\n            next_block_height = best_block_height + i\n            if self.is_in_blockcache(next_block_height):\n                continue\n\n            if next_block_height > best_node_height:\n                break\n\n            next_header_hash = await self.ledger.header_hash_by_height(next_block_height)\n            if next_header_hash == UInt256.zero():\n                # we do not have enough headers to fill the block cache. That's fine, just return\n                break\n\n            endheight = next_block_height\n            hashes.append(next_header_hash)\n            self.add_block_flight_info(node.nodeid, next_block_height, next_header_hash)\n\n        if len(hashes) > 0:\n            logger.debug(f\"Asking for blocks {best_block_height + 1} - {endheight} from {node.nodeid_human}\")\n            await node.get_data(InventoryType.block, hashes)\n            node.nodeweight.append_new_request_time()\n\n    async def persist_blocks(self) -> None:\n        self.is_persisting_blocks = True\n        while self.keep_running:\n            try:\n                b = self.block_cache.pop(0)\n                raw_b = self.raw_block_cache.pop(0)\n                await self.ledger.add_block(raw_b)\n                await asyncio.sleep(0.001)\n            except IndexError:\n                # cache empty\n                break\n        self.is_persisting_blocks = False\n\n    async def check_timeout(self) -> None:\n        task1 = asyncio.create_task(self.check_header_timeout())\n        task2 = asyncio.create_task(self.check_block_timeout())\n        try:\n            await asyncio.gather(task1, task2)\n        except Exception:\n            logger.debug(traceback.format_exc())\n\n    async def check_header_timeout(self) -> None:\n        if not self.header_request:\n            # no data requests outstanding\n            return\n\n        last_flight_info = self.header_request.most_recent_flight()\n\n        now = datetime.utcnow().timestamp()\n        delta = now - last_flight_info.start_time\n        if delta < self.HEADER_REQUEST_TIMEOUT:\n            # we're still good on time\n            return\n\n        node = self.nodemgr.get_node_by_nodeid(last_flight_info.node_id)\n        if node:\n            logger.debug(f\"Header timeout limit exceeded by {delta - self.HEADER_REQUEST_TIMEOUT:.2f}s for node {node.nodeid_human}\")\n\n        cur_header_height = await self.ledger.cur_header_height()\n        if last_flight_info.height <= cur_header_height:\n            # it has already come in in the mean time\n            # reset so sync_header will request new headers\n            self.header_request = None\n            return\n\n        # punish node that is causing header_timeout and retry using another node\n        self.header_request.mark_failed_node(last_flight_info.node_id)\n        await self.nodemgr.add_node_timeout_count(last_flight_info.node_id)\n\n        # retry with a new node\n        node = self.nodemgr.get_node_with_min_failed_time(self.header_request)\n        if node is None:\n            # only happens if there are no nodes that have data matching our needed height\n            self.header_request = None\n            return\n\n        hash = await self.ledger.header_hash_by_height(last_flight_info.height - 1)\n        logger.debug(f\"Retry requesting headers starting at {last_flight_info.height} from new node {node.nodeid_human}\")\n        await node.get_headers(hash_start=hash)\n\n        # restart start_time of flight info or else we'll timeout too fast for the next node\n        self.header_request.add_new_flight(FlightInfo(node.nodeid, last_flight_info.height))\n        node.nodeweight.append_new_request_time()\n\n    async def check_block_timeout(self) -> None:\n        if len(self.block_requests) == 0:\n            # no data requests outstanding\n            return\n\n        now = datetime.utcnow().timestamp()\n        block_timeout_flights = dict()\n\n        # test for timeout\n        for block_hash, request_info in self.block_requests.items():  # type: _, RequestInfo\n            flight_info = request_info.most_recent_flight()\n            if now - flight_info.start_time > self.BLOCK_REQUEST_TIMEOUT:\n                block_timeout_flights[block_hash] = flight_info\n\n        if len(block_timeout_flights) == 0:\n            # no timeouts\n            return\n\n        # 1) we first filter out invalid requests as some might have come in by now\n        # 2) for each block_sync cycle we requested blocks in batches of max 500 per node, now when resending we try to\n        #    create another batch\n        # 3) Blocks arrive one by one in 'inv' messages. In the block_sync cycle we created a FlightInfo object per\n        #    requested block such that we can determine speed among others. If one block in a request times out all\n        #    others for the same request will of course do as well (as they arrive in a linear fashion from the same node).\n        #    As such we only want to tag the individual node once (per request) for being slower than our timeout threshold not 500 times.\n        remaining_requests = []\n        nodes_to_tag_for_timeout = set()\n        nodes_to_mark_failed = dict()\n\n        best_stored_block_height = await self.get_best_stored_block_height()\n\n        for block_hash, fi in block_timeout_flights.items():  # type: _, FlightInfo\n            nodes_to_tag_for_timeout.add(fi.node_id)\n\n            try:\n                request_info = self.block_requests[block_hash]\n            except KeyError:\n                # means on_block_received popped it of the list\n                # we don't have to retry for data anymore\n                continue\n\n            if fi.height <= best_stored_block_height:\n                with suppress(KeyError):\n                    self.block_requests.pop(block_hash)\n                continue\n\n            nodes_to_mark_failed[request_info] = fi.node_id\n            remaining_requests.append((block_hash, fi.height, request_info))\n\n        for nodeid in nodes_to_tag_for_timeout:\n            await self.nodemgr.add_node_timeout_count(nodeid)\n\n        for request_info, node_id in nodes_to_mark_failed.items():\n            request_info.mark_failed_node(node_id)\n\n        # for the remaining requests that need to be queued again, we create new FlightInfo objects that use a new node\n        # and ask them in a single batch from that new node.\n        hashes = []\n        if len(remaining_requests) > 0:\n            # retry the batch with a new node\n            ri_first = remaining_requests[0][2]\n            ri_last = remaining_requests[-1][2]\n\n            # using `ri_last` because this has the highest block height and we want a node that supports that\n            node = self.nodemgr.get_node_with_min_failed_time(ri_last)\n            if not node:\n                return\n\n            for block_hash, height, ri in remaining_requests:  # type: _, int, RequestInfo\n                ri.add_new_flight(FlightInfo(node.nodeid, height))\n\n                hashes.append(block_hash)\n\n            if len(hashes) > 0:\n                logger.debug(f\"Block time out for blocks {ri_first.height} - {ri_last.height}. Trying again using new node {node.nodeid_human} {hashes[0]}\")\n                await node.get_data(InventoryType.block, hashes)\n                node.nodeweight.append_new_request_time()\n\n    async def on_headers_received(self, from_nodeid, headers: List[Header]) -> int:\n        if len(headers) == 0:\n            return -1\n\n        if self.header_request is None:\n            return -2\n\n        height = headers[0].index\n        if height != self.header_request.height:\n            # received headers we did not ask for\n            return -3\n        logger.debug(f\"Headers received {headers[0].index} - {headers[-1].index}\")\n\n        if headers in self.header_cache:\n            return -4\n\n        cur_header_height = await self.ledger.cur_header_height()\n        if height <= cur_header_height:\n            return -5\n\n        self.header_cache.append(headers)\n\n        return 1\n\n    async def on_block_received(self, from_nodeid, block: 'Block', raw_block) -> None:\n        # TODO: take out raw_block and raw_block_cache once we can serialize a full block\n        # print(f\"{block.index} {block.hash} received\")\n\n        next_header_height = await self.ledger.cur_header_height() + 1\n        if block.index > next_header_height:\n            return\n\n        cur_block_height = await self.ledger.cur_block_height()\n        if block.index <= cur_block_height:\n            return\n\n        try:\n            ri = self.block_requests.pop(block.hash)  # type: RequestInfo\n            fi = ri.flights.pop(from_nodeid)  # type: FlightInfo\n            now = datetime.utcnow().timestamp()\n            delta_time = now - fi.start_time\n            speed = (block._size / 1024) / delta_time  # KB/s\n\n            node = self.nodemgr.get_node_by_nodeid(fi.node_id)\n            if node:\n                node.nodeweight.append_new_speed(speed)\n        except KeyError:\n            # it's a block we did not ask for\n            # this can either be caused by rogue actors sending bad blocks\n            # or as a reply to our `get_data` on a broadcasted `inv` message by the node.\n            # (neo-cli nodes broadcast `inv` messages with their latest hash, we currently need to do a `get_data`\n            # and receive the full block to know what their best height is as we have no other mechanism (yet))\n            # TODO: remove once the network all start using neo-cli 2.10.1 or above which support ping/pong for height\n            sync_distance = block.index - cur_block_height\n            if sync_distance != 1:\n                return\n            # but if the distance is 1 we're in sync so we add the block anyway\n            # to avoid having the `sync_block` task request the same data again\n            # this is also necessary for neo-cli nodes because they maintain a TaskSession and refuse to send recently requested data\n\n        if not self.is_in_blockcache(block.index) and self.keep_running:\n            self.block_cache.append(block)\n            self.raw_block_cache.append(raw_block)\n\n    async def get_best_stored_block_height(self) -> int:\n        \"\"\"\n        Helper to return the highest block in our possession (either in ledger or in block_cache)\n        \"\"\"\n        best_block_cache_height = 0\n        if len(self.block_cache) > 0:\n            best_block_cache_height = self.block_cache[-1].index\n\n        ledger_height = await self.ledger.cur_block_height()\n\n        return max(ledger_height, best_block_cache_height)\n\n    def is_in_blockcache(self, block_height: int) -> bool:\n        for b in self.block_cache:\n            if b.index == block_height:\n                return True\n        else:\n            return False\n\n    def add_block_flight_info(self, nodeid, height, header_hash) -> None:\n        request_info = self.block_requests.get(header_hash, None)  # type: RequestInfo\n\n        if request_info is None:\n            # no outstanding requests for this particular hash, so we create it\n            req = RequestInfo(height)\n            req.add_new_flight(FlightInfo(nodeid, height))\n            self.block_requests[header_hash] = req\n        else:\n            request_info.flights.update({nodeid: FlightInfo(nodeid, height)})\n\n    def reset(self) -> None:\n        self.header_request = None\n        self.block_requests = dict()\n        self.block_cache = []\n        self.raw_block_cache = []\n        self.header_cache = []\n        self.is_persisting_blocks = False\n        self.is_persisting_headers = False\n        self.keep_running = False\n        self.nodemgr.reset",
    "after": "import asyncio\nimport traceback\nfrom datetime import datetime\nfrom neo.Network.core.header import Header\nfrom typing import TYPE_CHECKING, List\nfrom neo.Network.flightinfo import FlightInfo\nfrom neo.Network.requestinfo import RequestInfo\nfrom neo.Network.payloads.inventory import InventoryType\nfrom neo.Network.common import msgrouter\nfrom neo.Network.common.singleton import Singleton\nfrom contextlib import suppress\nfrom neo.Network.core.uint256 import UInt256\n\nfrom neo.logging import log_manager\n\nlogger = log_manager.getLogger('syncmanager')\n# log_manager.config_stdio([('syncmanager', 10)])\n\nif TYPE_CHECKING:\n    from neo.Network.nodemanager import NodeManager\n    from neo.Network.payloads import Block\n\n\nclass SyncManager(Singleton):\n    HEADER_MAX_LOOK_AHEAD = 6000\n    HEADER_REQUEST_TIMEOUT = 5\n\n    BLOCK_MAX_CACHE_SIZE = 500\n    BLOCK_NETWORK_REQ_LIMIT = 500\n    BLOCK_REQUEST_TIMEOUT = 5\n\n    def init(self, nodemgr: 'NodeManager'):\n        self.nodemgr = nodemgr\n        self.controller = None\n        self.block_requests = dict()  # header_hash:RequestInfo\n        self.header_request = None  # type: RequestInfo\n        self.ledger = None\n        self.block_cache = []\n        self.header_cache = []\n        self.raw_block_cache = []\n        self.is_persisting_blocks = False\n        self.is_persisting_headers = False\n        self.keep_running = True\n        self.service_task = None\n        self.persist_task = None\n        self.health_task = None\n\n        msgrouter.on_headers += self.on_headers_received\n        msgrouter.on_block += self.on_block_received\n\n    async def start(self) -> None:\n        while not self.nodemgr.running:\n            await asyncio.sleep(0.1)\n        self.service_task = asyncio.create_task(self.run_service())\n        self.health_task = asyncio.create_task(self.block_health())\n\n    async def shutdown(self):\n        print(\"Shutting down sync manager...\", end='')\n        self.keep_running = False\n        self.block_cache = []\n        shutdown_tasks = []\n\n        # start up errors can cause the tasks to not have been assigned,\n        # so we must validate their presence before feeding them to `gather`\n        if self.service_task:\n            shutdown_tasks.append(self.service_task)\n\n        if self.health_task:\n            shutdown_tasks.append(self.health_task)\n\n        if self.persist_task:\n            shutdown_tasks.append(self.persist_task)\n        await asyncio.gather(*shutdown_tasks, return_exceptions=True)\n\n        print(\"DONE\")\n\n    async def block_health(self):\n        # TODO: move this to nodemanager, once the network in general supports ping/pong\n        #  we can then make smarter choices by looking at individual nodes advancing or not and dropping just those\n        error_counter = 0\n        last_height = await self.ledger.cur_block_height()\n        while self.keep_running:\n            await asyncio.sleep(15)\n            cur_height = await self.ledger.cur_block_height()\n            if cur_height == last_height:\n                error_counter += 1\n                if error_counter == 3:\n                    to_disconnect = list(map(lambda n: n, self.nodemgr.nodes))\n                    logger.debug(f\"Block height not advancing. Replacing nodes: {to_disconnect}\")\n                    for n in to_disconnect:\n                        await self.nodemgr.replace_node(n)\n            else:\n                error_counter = 0\n\n            last_height = cur_height\n\n    async def run_service(self):\n        while self.keep_running:\n            await self.check_timeout()\n            await self.sync()\n            await asyncio.sleep(1)\n\n    async def sync(self) -> None:\n        await self.sync_header()\n        await self.sync_block()\n        await self.persist_headers()\n        if not self.is_persisting_blocks:\n            self.persist_task = asyncio.create_task(self.persist_blocks())\n\n    async def sync_header(self) -> None:\n        if self.header_request:\n            return\n\n        cur_header_height = await self.ledger.cur_header_height()\n        cur_block_height = await self.ledger.cur_block_height()\n        if cur_header_height - cur_block_height >= self.HEADER_MAX_LOOK_AHEAD:\n            return\n\n        node = self.nodemgr.get_next_node(cur_header_height + 1)\n        if not node:\n            # No connected nodes or no nodes with our height. We'll wait for node manager to resolve this\n            # or for the nodes to increase their height on the next produced block\n            return\n\n        self.header_request = RequestInfo(cur_header_height + 1)\n        self.header_request.add_new_flight(FlightInfo(node.nodeid, cur_header_height + 1))\n\n        cur_header_hash = await self.ledger.header_hash_by_height(cur_header_height)\n        await node.get_headers(hash_start=cur_header_hash)\n\n        logger.debug(f\"Requested headers starting at {cur_header_height + 1} from node {node.nodeid_human}\")\n        node.nodeweight.append_new_request_time()\n\n    async def persist_headers(self):\n        self.is_persisting_headers = True\n        if len(self.header_cache) > 0:\n            while self.keep_running:\n                try:\n                    headers = self.header_cache.pop(0)\n                    try:\n                        await self.ledger.add_headers(headers)\n                    except Exception as e:\n                        print(traceback.format_exc())\n                    await asyncio.sleep(0)\n                except IndexError:\n                    # cache empty\n                    break\n\n            # reset header_request such that the a new header sync task can be added\n            self.header_request = None\n            logger.debug(\"Finished processing headers\")\n\n        self.is_persisting_headers = False\n\n    async def sync_block(self) -> None:\n        # to simplify syncing, don't ask for more data if we still have requests in flight\n        if len(self.block_requests) > 0:\n            return\n\n        # the block cache might not have been fully processed, so we want to avoid asking for data we actually already have\n        best_block_height = await self.get_best_stored_block_height()\n        cur_header_height = await self.ledger.cur_header_height()\n        blocks_to_fetch = cur_header_height - best_block_height\n        if blocks_to_fetch <= 0:\n            return\n\n        block_cache_space = self.BLOCK_MAX_CACHE_SIZE - len(self.block_cache)\n        if block_cache_space <= 0:\n            return\n\n        if blocks_to_fetch > block_cache_space or blocks_to_fetch > self.BLOCK_NETWORK_REQ_LIMIT:\n            blocks_to_fetch = min(block_cache_space, self.BLOCK_NETWORK_REQ_LIMIT)\n\n        try:\n            best_node_height = max(map(lambda node: node.best_height, self.nodemgr.nodes))\n        except ValueError:\n            # if the node list is empty max() fails on an empty list\n            return\n\n        node = self.nodemgr.get_next_node(best_node_height)\n        if not node:\n            # no nodes with our desired height. We'll wait for node manager to resolve this\n            # or for the nodes to increase their height on the next produced block\n            return\n\n        hashes = []\n        endheight = None\n        for i in range(1, blocks_to_fetch + 1):\n            next_block_height = best_block_height + i\n            if self.is_in_blockcache(next_block_height):\n                continue\n\n            if next_block_height > best_node_height:\n                break\n\n            next_header_hash = await self.ledger.header_hash_by_height(next_block_height)\n            if next_header_hash == UInt256.zero():\n                # we do not have enough headers to fill the block cache. That's fine, just return\n                break\n\n            endheight = next_block_height\n            hashes.append(next_header_hash)\n            self.add_block_flight_info(node.nodeid, next_block_height, next_header_hash)\n\n        if len(hashes) > 0:\n            logger.debug(f\"Asking for blocks {best_block_height + 1} - {endheight} from {node.nodeid_human}\")\n            await node.get_data(InventoryType.block, hashes)\n            node.nodeweight.append_new_request_time()\n\n    async def persist_blocks(self) -> None:\n        self.is_persisting_blocks = True\n        while self.keep_running:\n            try:\n                b = self.block_cache.pop(0)\n                raw_b = self.raw_block_cache.pop(0)\n                await self.ledger.add_block(raw_b)\n                await asyncio.sleep(0.001)\n            except IndexError:\n                # cache empty\n                break\n        self.is_persisting_blocks = False\n\n    async def check_timeout(self) -> None:\n        task1 = asyncio.create_task(self.check_header_timeout())\n        task2 = asyncio.create_task(self.check_block_timeout())\n        try:\n            await asyncio.gather(task1, task2)\n        except Exception:\n            logger.debug(traceback.format_exc())\n\n    async def check_header_timeout(self) -> None:\n        if not self.header_request:\n            # no data requests outstanding\n            return\n\n        last_flight_info = self.header_request.most_recent_flight()\n\n        now = datetime.utcnow().timestamp()\n        delta = now - last_flight_info.start_time\n        if delta < self.HEADER_REQUEST_TIMEOUT:\n            # we're still good on time\n            return\n\n        node = self.nodemgr.get_node_by_nodeid(last_flight_info.node_id)\n        if node:\n            logger.debug(f\"Header timeout limit exceeded by {delta - self.HEADER_REQUEST_TIMEOUT:.2f}s for node {node.nodeid_human}\")\n\n        cur_header_height = await self.ledger.cur_header_height()\n        if last_flight_info.height <= cur_header_height:\n            # it has already come in in the mean time\n            # reset so sync_header will request new headers\n            self.header_request = None\n            return\n\n        # punish node that is causing header_timeout and retry using another node\n        self.header_request.mark_failed_node(last_flight_info.node_id)\n        await self.nodemgr.add_node_timeout_count(last_flight_info.node_id)\n\n        # retry with a new node\n        node = self.nodemgr.get_node_with_min_failed_time(self.header_request)\n        if node is None:\n            # only happens if there are no nodes that have data matching our needed height\n            self.header_request = None\n            return\n\n        hash = await self.ledger.header_hash_by_height(last_flight_info.height - 1)\n        logger.debug(f\"Retry requesting headers starting at {last_flight_info.height} from new node {node.nodeid_human}\")\n        await node.get_headers(hash_start=hash)\n\n        # restart start_time of flight info or else we'll timeout too fast for the next node\n        self.header_request.add_new_flight(FlightInfo(node.nodeid, last_flight_info.height))\n        node.nodeweight.append_new_request_time()\n\n    async def check_block_timeout(self) -> None:\n        if len(self.block_requests) == 0:\n            # no data requests outstanding\n            return\n\n        now = datetime.utcnow().timestamp()\n        block_timeout_flights = dict()\n\n        # test for timeout\n        for block_hash, request_info in self.block_requests.items():  # type: _, RequestInfo\n            flight_info = request_info.most_recent_flight()\n            if now - flight_info.start_time > self.BLOCK_REQUEST_TIMEOUT:\n                block_timeout_flights[block_hash] = flight_info\n\n        if len(block_timeout_flights) == 0:\n            # no timeouts\n            return\n\n        # 1) we first filter out invalid requests as some might have come in by now\n        # 2) for each block_sync cycle we requested blocks in batches of max 500 per node, now when resending we try to\n        #    create another batch\n        # 3) Blocks arrive one by one in 'inv' messages. In the block_sync cycle we created a FlightInfo object per\n        #    requested block such that we can determine speed among others. If one block in a request times out all\n        #    others for the same request will of course do as well (as they arrive in a linear fashion from the same node).\n        #    As such we only want to tag the individual node once (per request) for being slower than our timeout threshold not 500 times.\n        remaining_requests = []\n        nodes_to_tag_for_timeout = set()\n        nodes_to_mark_failed = dict()\n\n        best_stored_block_height = await self.get_best_stored_block_height()\n\n        for block_hash, fi in block_timeout_flights.items():  # type: _, FlightInfo\n            nodes_to_tag_for_timeout.add(fi.node_id)\n\n            try:\n                request_info = self.block_requests[block_hash]\n            except KeyError:\n                # means on_block_received popped it of the list\n                # we don't have to retry for data anymore\n                continue\n\n            if fi.height <= best_stored_block_height:\n                with suppress(KeyError):\n                    self.block_requests.pop(block_hash)\n                continue\n\n            nodes_to_mark_failed[request_info] = fi.node_id\n            remaining_requests.append((block_hash, fi.height, request_info))\n\n        for nodeid in nodes_to_tag_for_timeout:\n            await self.nodemgr.add_node_timeout_count(nodeid)\n\n        for request_info, node_id in nodes_to_mark_failed.items():\n            request_info.mark_failed_node(node_id)\n\n        # for the remaining requests that need to be queued again, we create new FlightInfo objects that use a new node\n        # and ask them in a single batch from that new node.\n        hashes = []\n        if len(remaining_requests) > 0:\n            # retry the batch with a new node\n            ri_first = remaining_requests[0][2]\n            ri_last = remaining_requests[-1][2]\n\n            # using `ri_last` because this has the highest block height and we want a node that supports that\n            node = self.nodemgr.get_node_with_min_failed_time(ri_last)\n            if not node:\n                return\n\n            for block_hash, height, ri in remaining_requests:  # type: _, int, RequestInfo\n                ri.add_new_flight(FlightInfo(node.nodeid, height))\n\n                hashes.append(block_hash)\n\n            if len(hashes) > 0:\n                logger.debug(f\"Block time out for blocks {ri_first.height} - {ri_last.height}. Trying again using new node {node.nodeid_human} {hashes[0]}\")\n                await node.get_data(InventoryType.block, hashes)\n                node.nodeweight.append_new_request_time()\n\n    async def on_headers_received(self, from_nodeid, headers: List[Header]) -> int:\n        if len(headers) == 0:\n            return -1\n\n        if self.header_request is None:\n            return -2\n\n        height = headers[0].index\n        if height != self.header_request.height:\n            # received headers we did not ask for\n            return -3\n        logger.debug(f\"Headers received {headers[0].index} - {headers[-1].index}\")\n\n        if headers in self.header_cache:\n            return -4\n\n        cur_header_height = await self.ledger.cur_header_height()\n        if height <= cur_header_height:\n            return -5\n\n        self.header_cache.append(headers)\n\n        return 1\n\n    async def on_block_received(self, from_nodeid, block: 'Block', raw_block) -> None:\n        # TODO: take out raw_block and raw_block_cache once we can serialize a full block\n        # print(f\"{block.index} {block.hash} received\")\n\n        next_header_height = await self.ledger.cur_header_height() + 1\n        if block.index > next_header_height:\n            return\n\n        cur_block_height = await self.ledger.cur_block_height()\n        if block.index <= cur_block_height:\n            return\n\n        try:\n            ri = self.block_requests.pop(block.hash)  # type: RequestInfo\n            fi = ri.flights.pop(from_nodeid)  # type: FlightInfo\n            now = datetime.utcnow().timestamp()\n            delta_time = now - fi.start_time\n            speed = (block._size / 1024) / delta_time  # KB/s\n\n            node = self.nodemgr.get_node_by_nodeid(fi.node_id)\n            if node:\n                node.nodeweight.append_new_speed(speed)\n        except KeyError:\n            # it's a block we did not ask for\n            # this can either be caused by rogue actors sending bad blocks\n            # or as a reply to our `get_data` on a broadcasted `inv` message by the node.\n            # (neo-cli nodes broadcast `inv` messages with their latest hash, we currently need to do a `get_data`\n            # and receive the full block to know what their best height is as we have no other mechanism (yet))\n            # TODO: remove once the network all start using neo-cli 2.10.1 or above which support ping/pong for height\n            sync_distance = block.index - cur_block_height\n            if sync_distance != 1:\n                return\n            # but if the distance is 1 we're in sync so we add the block anyway\n            # to avoid having the `sync_block` task request the same data again\n            # this is also necessary for neo-cli nodes because they maintain a TaskSession and refuse to send recently requested data\n\n        if not self.is_in_blockcache(block.index) and self.keep_running:\n            self.block_cache.append(block)\n            self.raw_block_cache.append(raw_block)\n\n    async def get_best_stored_block_height(self) -> int:\n        \"\"\"\n        Helper to return the highest block in our possession (either in ledger or in block_cache)\n        \"\"\"\n        best_block_cache_height = 0\n        if len(self.block_cache) > 0:\n            best_block_cache_height = self.block_cache[-1].index\n\n        ledger_height = await self.ledger.cur_block_height()\n\n        return max(ledger_height, best_block_cache_height)\n\n    def is_in_blockcache(self, block_height: int) -> bool:\n        for b in self.block_cache:\n            if b.index == block_height:\n                return True\n        else:\n            return False\n\n    def add_block_flight_info(self, nodeid, height, header_hash) -> None:\n        request_info = self.block_requests.get(header_hash, None)  # type: RequestInfo\n\n        if request_info is None:\n            # no outstanding requests for this particular hash, so we create it\n            req = RequestInfo(height)\n            req.add_new_flight(FlightInfo(nodeid, height))\n            self.block_requests[header_hash] = req\n        else:\n            request_info.flights.update({nodeid: FlightInfo(nodeid, height)})\n\n    def reset(self) -> None:\n        self.header_request = None\n        self.block_requests = dict()\n        self.block_cache = []\n        self.raw_block_cache = []\n        self.is_persisting_blocks = False\n        self.is_persisting_headers = False\n\n    async def run(self) -> None:\n        self.header_request = None\n        self.block_requests"
  },
  "feature_stats": {
    "indices": [
      0,
      2,
      1,
      5419,
      922,
      4677,
      8280,
      7649,
      7027,
      2579,
      1217
    ],
    "f_before": [
      77.15452575683594,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "f_after": [
      77.15452575683594,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "g_before": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "g_after": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "num_batches_for_stats": 10
  },
  "loss_summary": {
    "train_mean_loss": 0.9852456296756864,
    "train_mean_l2": 0.7588098129555583,
    "train_mean_l1": 0.7674992515295744,
    "num_steps": 2000,
    "num_batches": 2000
  },
  "feature_std_summary": {
    "plot_path": "outputs/plots/20251128-084646/feature_std_layer_0.html",
    "mean_std": 0.4904178977012634,
    "max_std": 36.88555908203125
  },
  "hook_comparison": {
    "hook_layer_idx_minus1": {
      "hook_layer_idx": 0,
      "feature_stats": {
        "indices": [
          0,
          2,
          1,
          5419,
          922,
          4677,
          8280,
          7649,
          7027,
          2579,
          1217
        ],
        "f_before": [
          77.15452575683594,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ],
        "f_after": [
          77.15452575683594,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ],
        "g_before": [
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ],
        "g_after": [
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ]
      },
      "top_activated_features": {
        "features": [
          {
            "index": 8,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 10,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 4,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 15,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 12,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 3,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 9,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 1,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 11,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 5,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 7,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 13,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 0,
            "f_before": 77.15452575683594,
            "f_after": 77.15452575683594,
            "delta_f": 0.0
          },
          {
            "index": 2,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 6,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 14,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          }
        ],
        "k": 16
      },
      "output": {
        "before": "import asyncio\nimport traceback\nfrom datetime import datetime\nfrom neo.Network.core.header import Header\nfrom typing import TYPE_CHECKING, List\nfrom neo.Network.flightinfo import FlightInfo\nfrom neo.Network.requestinfo import RequestInfo\nfrom neo.Network.payloads.inventory import InventoryType\nfrom neo.Network.common import msgrouter\nfrom neo.Network.common.singleton import Singleton\nfrom contextlib import suppress\nfrom neo.Network.core.uint256 import UInt256\n\nfrom neo.logging import log_manager\n\nlogger = log_manager.getLogger('flight-data-handler')\n\n\nclass FlightDataHandler(Singleton):\n    def __init__(self):\n        super().__init__()\n        self.flight_data = {}  \n        self.flight_data_queue = asyncio.",
        "after": "import asyncio\nimport traceback\nfrom datetime import datetime\nfrom neo.Network.core.header import Header\nfrom typing import TYPE_CHECKING, List\nfrom neo.Network.flightinfo import FlightInfo\nfrom neo.Network.requestinfo import RequestInfo\nfrom neo.Network.payloads.inventory import InventoryType\nfrom neo.Network.common import msgrouter\nfrom neo.Network.common.singleton import Singleton\nfrom contextlib import suppress\nfrom neo.Network.core.uint256 import UInt256\n\nfrom neo.logging import log_manager\n\nlogger = log_manager.getLogger('main')\n\n# Define a function to get the current time\ndef get_current_time():\n    return datetime.now()\n\n# Define a function to get the next flight information\ndef get_next_flight_info(flight_"
      }
    },
    "hook_layer_idx_same": {
      "hook_layer_idx": 0,
      "feature_stats": {
        "indices": [
          0,
          2,
          1,
          5419,
          922,
          4677,
          8280,
          7649,
          7027,
          2579,
          1217
        ],
        "f_before": [
          77.15452575683594,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ],
        "f_after": [
          77.15452575683594,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ],
        "g_before": [
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ],
        "g_after": [
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ]
      },
      "top_activated_features": {
        "features": [
          {
            "index": 8,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 10,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 4,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 15,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 12,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 3,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 9,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 1,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 11,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 5,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 7,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 13,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 0,
            "f_before": 77.15452575683594,
            "f_after": 77.15452575683594,
            "delta_f": 0.0
          },
          {
            "index": 2,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 6,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 14,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          }
        ],
        "k": 16
      },
      "output": {
        "before": "import asyncio\nimport traceback\nfrom datetime import datetime\nfrom neo.Network.core.header import Header\nfrom typing import TYPE_CHECKING, List\nfrom neo.Network.flightinfo import FlightInfo\nfrom neo.Network.requestinfo import RequestInfo\nfrom neo.Network.payloads.inventory import InventoryType\nfrom neo.Network.common import msgrouter\nfrom neo.Network.common.singleton import Singleton\nfrom contextlib import suppress\nfrom neo.Network.core.uint256 import UInt256\n\nfrom neo.logging import log_manager\n\nlogger = log_manager.getLogger('inventory')\n\nclass InventoryManager(Singleton):\n    def __init__(self, network_instance: neo.Network.core.Network):\n        self.network_instance = network_instance\n        self.inventory_types = {}\n",
        "after": "import asyncio\nimport traceback\nfrom datetime import datetime\nfrom neo.Network.core.header import Header\nfrom typing import TYPE_CHECKING, List\nfrom neo.Network.flightinfo import FlightInfo\nfrom neo.Network.requestinfo import RequestInfo\nfrom neo.Network.payloads.inventory import InventoryType\nfrom neo.Network.common import msgrouter\nfrom neo.Network.common.singleton import Singleton\nfrom contextlib import suppress\nfrom neo.Network.core.uint256 import UInt256\n\nfrom neo.logging import log_manager\n\nlogger = log_manager.getLogger('main')\n\n# This is a basic example of a network connection.\n\nasync def main():\n    try:\n        # This will be a basic example of a network connection.\n        # Create a new network connection\n        conn = Network"
      }
    }
  }
}
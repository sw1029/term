{
  "model_name": "google/gemma-2-2b-it",
  "layer_idx": 12,
  "command": "python main.py experiment.use_multi_contrast=true sae.loss_option=2 sae.loss_module=option2_loss sae.guidance_method=contrastive sae.guidance_coeff=1.0 sae.alpha_concept.code=0.04 sae.alpha_concept.harm=0.06 sae.alpha_concept.struct=0.0 sae.use_l0=true sae.l0_coeff=0.0002314673387520083 sae.role_sep_coeff=0.002",
  "sae": {
    "input_dim": 2304,
    "sae_dim": 9216,
    "fixed_v_cnt": 3,
    "batch_size": 3,
    "epochs": 2,
    "max_steps": 2000,
    "l1_coeff": 0.005,
    "l0_coeff": 0.0002314673387520083,
    "use_l0": true,
    "concept_samples_per_label": 100,
    "loss_option": 2,
    "loss_module": "option2_loss",
    "concept_feature_indices": {
      "code": 0,
      "harm": 1,
      "struct": 2
    },
    "alpha_concept": {
      "code": 0.04,
      "harm": 0.06,
      "struct": 0.0
    },
    "positive_targets": {
      "code": 1,
      "harm": 1,
      "struct": 1
    },
    "guidance_method": "contrastive",
    "guidance_margin": 0.0,
    "guidance_coeff": 1.0,
    "guidance_mse_pos_value": 1.0,
    "guidance_mse_neg_value": 0.0,
    "jumprelu_bandwidth": 0.001,
    "jumprelu_init_threshold": 0.001,
    "role_sep_coeff": 0.002
  },
  "gnn": {
    "use_gnn": true,
    "top_k": 10
  },
  "experiment": {
    "device": "cuda",
    "dataset_name": "wikitext",
    "dataset_config": "wikitext-2-raw-v1",
    "use_multi_contrast": true,
    "num_steering_samples_per_label": 10,
    "top_k_for_plot": 10,
    "layer_sweep": [
      12
    ],
    "strength_sweep": [
      15.0
    ]
  },
  "steering": {
    "label": "struct",
    "feature_idx": 2,
    "strength": 15.0
  },
  "prompt": "\"\"\"Skopes rules \"\"\"\n\nimport uuid\nimport os\nimport datatable as dt\nimport numpy as np\nfrom h2oaicore.models import CustomModel\nfrom sklearn.preprocessing import LabelEncoder\nfrom h2oaicore.systemutils import physical_cores_count\nfrom h2oaicore.systemutils import user_dir, remove, config\nfrom h2oaicore.systemutils import make_experiment_logger, loggerinfo, loggerwarning, loggerdebug\n\n\nclass SKOPE_RULES(CustomModel):\n    _regression = False\n    _binary = True\n    _multiclass = False\n    _display_name = \"SKOPE RULES\"\n    _description = \"SKOPE RULES\"\n    # using git master because pypi is very out of date (Jan 2020) but need Sept 1-ish master with fix for updated scikit-learn\n    _modules_needed_by_name = ['git+https://github.com/scikit-learn-contrib/skope-rules.git']\n\n    @staticmethod\n    def do_acceptance_test():\n        return True\n\n    def set_default_params(self, accuracy=None, time_tolerance=None,\n                           interpretability=None, **kwargs):\n        # Fill up parameters we care about\n        self.params = dict(random_state=kwargs.get(\"random_state\", 1234),\n                           max_depth_duplication=None, n_estimators=10,\n                           precision_min=0.5, recall_min=0.01, max_samples=0.8,\n                           max_samples_features=1.0, max_depth=3,\n                           max_features=\"auto\", min_samples_split=2,\n                           bootstrap=False, bootstrap_features=False)\n\n    def mutate_params(self, accuracy=10, **kwargs):\n        if accuracy > 8:\n            max_depth_duplication = [None, 2, 3]\n            n_estimators = [10, 20, 40]\n            precision_min = [0.1, 0.2, 0.3]\n            recall_min = [0.01, 0.05]\n            max_samples = [0.5, 0.8, 1.0]\n            max_samples_features = [0.5, 0.8, 1.0]\n            max_depth = [3, 4, 5]\n            max_features = [\"sqrt\", \"log2\", \"auto\"]\n            min_samples_split = [2, 11, 21]\n            bootstrap = [True, False]\n            bootstrap_features = [True, False]\n        elif accuracy >= 5:\n            max_depth_duplication = [None]\n            n_estimators = [10, 20]\n            precision_min = [0.1, 0.2, 0.3]\n            recall_min = [0.01]\n            max_samples = [0.8, 1.0]\n            max_samples_features = [1.0]\n            max_depth = [3, 4]\n            max_features = [\"sqrt\", \"log2\", \"auto\"]\n            min_samples_split = [2, 5, 11]\n            bootstrap = [True, False]\n            bootstrap_features = [True, False]\n        else:\n            max_depth_duplication = [None]\n            n_estimators = [10]\n            precision_min = [0.1, 0.2]\n            recall_min = [0.01]\n            max_samples = [0.8, 1.0]\n            max_samples_features = [0.8, 1.0]\n            max_depth = [3, 4]\n            max_features = [\"auto\"]\n            min_samples_split = [2]\n            bootstrap = [True, False]\n            bootstrap_features = [True, False]\n\n        self.params[\"max_depth_duplication\"] = np.random.choice(max_depth_duplication)\n        self.params[\"n_estimators\"] = np.random.choice(n_estimators)\n        self.params[\"precision_min\"] = np.random.choice(precision_min)\n        self.params[\"recall_min\"] = np.random.choice(recall_min)\n        self.params[\"max_samples\"] = np.random.choice(max_samples)\n        self.params[\"max_samples_features\"] = np.random.choice(max_samples_features)\n        self.params[\"max_depth\"] = np.random.choice(max_depth)\n        self.params[\"max_features\"] = np.random.choice(max_features)\n        self.params[\"min_samples_split\"] = np.random.choice(min_samples_split)\n        self.params[\"bootstrap\"] = np.random.choice(bootstrap)\n        self.params[\"bootstrap_features\"] = np.random.choice(bootstrap_features)\n\n    def _create_tmp_folder(self, logger):\n        # Create a temp folder to store files \n        # Set the default value without context available (required to pass acceptance test)\n        tmp_folder = os.path.join(user_dir(), \"%s_SKOPE_model_folder\" % uuid.uuid4())\n        # Make a real tmp folder when experiment is available\n        if self.context and self.context.experiment_id:\n            tmp_folder = os.path.join(self.context.experiment_tmp_dir, \"%s_SKOPE_model_folder\" % uuid.uuid4())\n\n        # Now let's try to create that folder\n        try:\n            os.mkdir(tmp_folder)\n        except PermissionError:\n            # This not occur so log a warning\n            loggerwarning(logger, \"SKOPE was denied temp folder creation rights\")\n            tmp_folder = os.path.join(user_dir(), \"%s_SKOPE_model_folder\" % uuid.uuid4())\n            os.mkdir(tmp_folder)\n        except FileExistsError:\n            # We should never be here since temp dir name is expected to be unique\n            loggerwarning(logger, \"SKOPE temp folder already exists\")\n            tmp_folder = os.path.join(self.context.experiment_tmp_dir, \"%s_SKOPE_model_folder\" % uuid.uuid4())\n            os.mkdir(tmp_folder)\n        except:\n            # Revert to temporary file path\n            tmp_folder = os.path.join(user_dir(), \"%s_SKOPE_model_folder\" % uuid.uuid4())\n            os.mkdir(tmp_folder)\n\n        loggerinfo(logger, \"SKOPE temp folder {}\".format(tmp_folder))\n        return tmp_folder\n\n    def fit(self, X, y, sample_weight=None, eval_set=None, sample_weight_eval_set=None, **kwargs):\n\n        orig_cols = list(X.names)\n\n        import pandas as pd\n        import numpy as np\n        from skrules import SkopeRules\n        from sklearn.preprocessing import OneHotEncoder\n        from collections import Counter\n\n        # Get the logger if it exists\n        logger = None\n        if self.context and self.context.experiment_id:\n            logger = make_experiment_logger(experiment_id=self.context.experiment_id,\n                                            tmp_dir=self.context.tmp_dir,\n                                            experiment_tmp_dir=self.context.experiment_tmp_dir)\n\n        # Set up temp folder\n        tmp_folder = self._create_tmp_folder(logger)\n\n        # Set up model\n        if self.num_classes >= 2:\n            lb = LabelEncoder()\n            lb.fit(self.labels)\n            y = lb.transform(y)\n\n            model = SkopeRules(max_depth_duplication=self.params[\"max_depth_duplication\"],\n                               n_estimators=self.params[\"n_estimators\"],\n                               precision_min=self.params[\"precision_min\"],\n                               recall_min=self.params[\"recall_min\"],\n                               max_samples=self.params[\"max_samples\"],\n                               max_samples_features=self.params[\"max_samples_features\"],\n                               max_depth=self.params[\"max_depth\"],\n                               max_features=self.params[\"max_features\"],\n                               min_samples_split=self.params[\"min_samples_split\"],\n                               bootstrap=self.params[\"bootstrap\"],\n                               bootstrap_features=self.params[\"bootstrap_features\"],\n                               random_state=self.params[\"random_state\"],\n                               feature_names=orig_cols)\n        else:\n            # Skopes doesn't work for regression\n            loggerinfo(logger, \"PASS, no skopes model\")\n            pass\n\n        # Find the datatypes\n        X = X.to_pandas()\n        X.columns = orig_cols\n\n        # Change continuous features to categorical\n        X_datatypes = [str(item) for item in list(X.dtypes)]\n\n        # Change all float32 values to float64\n        for ii in range(len(X_datatypes)):\n            if X_datatypes[ii] == 'float32':\n                X = X.astype({orig_cols[ii]: np.float64})\n\n        X_datatypes = [str(item) for item in list(X.dtypes)]\n\n        # List the categorical and numerical features\n        self.X_categorical = [orig_cols[col_count] for col_count in range(len(orig_cols)) if\n                              (X_datatypes[col_count] == 'category') or (X_datatypes[col_count] == 'object')]\n        self.X_numeric = [item for item in orig_cols if item not in self.X_categorical]\n\n        # Find the levels and mode for each categorical feature\n        # for use in the test set\n        self.train_levels = {}\n        for item in self.X_categorical:\n            self.train_levels[item] = list(set(X[item]))\n            self.train_mode[item] = Counter(X[item]).most_common(1)[0][0]\n\n            # One hot encode the categorical features\n        # And replace missing values with a Missing category\n        if len(self.X_categorical) > 0:\n            loggerinfo(logger, \"PCategorical encode\")\n\n            for colname in self.X_categorical:\n                X[colname] = list(X[colname].fillna(\"Missing\"))\n            self.enc = OneHotEncoder(handle_unknown='ignore')\n\n            self.enc.fit(X[self.X_categorical])\n            self.encoded_categories = list(self.enc.get_feature_names(input_features=self.X_categorical))\n\n            X_enc = self.enc.transform(X[self.X_categorical]).toarray()\n\n            X = pd.concat([X[self.X_numeric], pd.DataFrame(X_enc, columns=self.encoded_categories)], axis=1)\n\n        # Replace missing values with a missing value code\n        if len(self.X_numeric) > 0:\n\n            for colname in self.X_numeric:\n                X[colname] = list(X[colname].fillna(-999))\n\n        model.fit(np.array(X), np.array(y))\n\n        # Find the rule list\n        self.rule_list = model.rules_\n\n        # Calculate feature importances\n        var_imp = []\n        for var in orig_cols:\n            var_imp.append(sum(int(var in item[0]) for item in self.rule_list))\n\n        if max(var_imp) != 0:\n            importances = list(np.array(var_imp) / max(var_imp))\n        else:\n            importances = [1] * len(var_imp)\n\n        pd.DataFrame(model.rules_, columns=['Rule', '(Precision, Recall, nb)']).to_csv(\n            os.path.join(tmp_folder, 'Skope_rules.csv'), index=False)\n\n        self.mean_target = np.array(sum(y) / len(y))\n\n        # Set model properties\n        self.set_model_properties(model=model,\n                                  features=list(X.columns),\n                                  importances=importances,\n                                  iterations=self.params['n_estimators'])\n\n    def predict(self, X, **kwargs):\n        orig_cols = list(X.names)\n        import pandas as pd\n\n        X = dt.Frame(X)\n\n        # Find datatypes\n        X = X.to_pandas()\n\n        X_datatypes = [str(item) for item in list(X.dtypes)]\n\n        # Change float 32 values to float 64\n        for ii in range(len(X_datatypes)):\n            if X_datatypes[ii] == 'float32':\n                X = X.astype({orig_cols[ii]: np.float64})\n\n                # Replace missing values with a missing category\n        # Replace categories that weren't in the training set with the mode\n        if len(self.X_categorical) > 0:\n\n            for colname in self.X_categorical:\n                X[colname] = list(X[colname].fillna(\"Missing\"))\n\n            for label in self.X_categorical:\n                # Replace anything not in the test set\n                train_categories = self.train_levels[label]\n                X_label = np.array(X[label])\n                mmode = self.train_mode[label]\n                X_label[~np.isin(X_label, train_categories)] = mmode\n                X[label] = X_label\n\n        # Replace missing values with a missing value code    \n        if len(self.X_numeric) > 0:\n            for colname in self.X_numeric:\n                X[colname] = list(X[colname].fillna(-999))\n\n                # Get model    \n        model, _, _, _ = self.get_model_properties()\n\n        # One hot encode categorical features\n        if len(self.X_categorical) > 0:\n            X_enc = self.enc.transform(X[self.X_categorical]).toarray()\n            X = pd.concat([X[self.X_numeric], pd.DataFrame(X_enc, columns=self.encoded_categories)], axis=1)\n\n            # Make predictions on the test set\n        preds = model.score_top_rules(X) / len(self.rule_list)\n        preds = np.array(preds)\n        epsilon = 10 ** (-3)\n        preds[np.isnan(preds)] = self.mean_target\n        preds[preds > 1 - epsilon] = 1.0 - epsilon\n        preds[preds < 0 + epsilon] = 0.0 + epsilon\n\n        return preds\n",
  "output": {
    "before": "\"\"\"Skopes rules \"\"\"\n\nimport uuid\nimport os\nimport datatable as dt\nimport numpy as np\nfrom h2oaicore.models import CustomModel\nfrom sklearn.preprocessing import LabelEncoder\nfrom h2oaicore.systemutils import physical_cores_count\nfrom h2oaicore.systemutils import user_dir, remove, config\nfrom h2oaicore.systemutils import make_experiment_logger, loggerinfo, loggerwarning, loggerdebug\n\n\nclass SKOPE_RULES(CustomModel):\n    _regression = False\n    _binary = True\n    _multiclass = False\n    _display_name = \"SKOPE RULES\"\n    _description = \"SKOPE RULES\"\n    # using git master because pypi is very out of date (Jan 2020) but need Sept 1-ish master with fix for updated scikit-learn\n    _modules_needed_by_name = ['git+https://github.com/scikit-learn-contrib/skope-rules.git']\n\n    @staticmethod\n    def do_acceptance_test():\n        return True\n\n    def set_default_params(self, accuracy=None, time_tolerance=None,\n                           interpretability=None, **kwargs):\n        # Fill up parameters we care about\n        self.params = dict(random_state=kwargs.get(\"random_state\", 1234),\n                           max_depth_duplication=None, n_estimators=10,\n                           precision_min=0.5, recall_min=0.01, max_samples=0.8,\n                           max_samples_features=1.0, max_depth=3,\n                           max_features=\"auto\", min_samples_split=2,\n                           bootstrap=False, bootstrap_features=False)\n\n    def mutate_params(self, accuracy=10, **kwargs):\n        if accuracy > 8:\n            max_depth_duplication = [None, 2, 3]\n            n_estimators = [10, 20, 40]\n            precision_min = [0.1, 0.2, 0.3]\n            recall_min = [0.01, 0.05]\n            max_samples = [0.5, 0.8, 1.0]\n            max_samples_features = [0.5, 0.8, 1.0]\n            max_depth = [3, 4, 5]\n            max_features = [\"sqrt\", \"log2\", \"auto\"]\n            min_samples_split = [2, 11, 21]\n            bootstrap = [True, False]\n            bootstrap_features = [True, False]\n        elif accuracy >= 5:\n            max_depth_duplication = [None]\n            n_estimators = [10, 20]\n            precision_min = [0.1, 0.2, 0.3]\n            recall_min = [0.01]\n            max_samples = [0.8, 1.0]\n            max_samples_features = [1.0]\n            max_depth = [3, 4]\n            max_features = [\"sqrt\", \"log2\", \"auto\"]\n            min_samples_split = [2, 5, 11]\n            bootstrap = [True, False]\n            bootstrap_features = [True, False]\n        else:\n            max_depth_duplication = [None]\n            n_estimators = [10]\n            precision_min = [0.1, 0.2]\n            recall_min = [0.01]\n            max_samples = [0.8, 1.0]\n            max_samples_features = [0.8, 1.0]\n            max_depth = [3, 4]\n            max_features = [\"auto\"]\n            min_samples_split = [2]\n            bootstrap = [True, False]\n            bootstrap_features = [True, False]\n\n        self.params[\"max_depth_duplication\"] = np.random.choice(max_depth_duplication)\n        self.params[\"n_estimators\"] = np.random.choice(n_estimators)\n        self.params[\"precision_min\"] = np.random.choice(precision_min)\n        self.params[\"recall_min\"] = np.random.choice(recall_min)\n        self.params[\"max_samples\"] = np.random.choice(max_samples)\n        self.params[\"max_samples_features\"] = np.random.choice(max_samples_features)\n        self.params[\"max_depth\"] = np.random.choice(max_depth)\n        self.params[\"max_features\"] = np.random.choice(max_features)\n        self.params[\"min_samples_split\"] = np.random.choice(min_samples_split)\n        self.params[\"bootstrap\"] = np.random.choice(bootstrap)\n        self.params[\"bootstrap_features\"] = np.random.choice(bootstrap_features)\n\n    def _create_tmp_folder(self, logger):\n        # Create a temp folder to store files \n        # Set the default value without context available (required to pass acceptance test)\n        tmp_folder = os.path.join(user_dir(), \"%s_SKOPE_model_folder\" % uuid.uuid4())\n        # Make a real tmp folder when experiment is available\n        if self.context and self.context.experiment_id:\n            tmp_folder = os.path.join(self.context.experiment_tmp_dir, \"%s_SKOPE_model_folder\" % uuid.uuid4())\n\n        # Now let's try to create that folder\n        try:\n            os.mkdir(tmp_folder)\n        except PermissionError:\n            # This not occur so log a warning\n            loggerwarning(logger, \"SKOPE was denied temp folder creation rights\")\n            tmp_folder = os.path.join(user_dir(), \"%s_SKOPE_model_folder\" % uuid.uuid4())\n            os.mkdir(tmp_folder)\n        except FileExistsError:\n            # We should never be here since temp dir name is expected to be unique\n            loggerwarning(logger, \"SKOPE temp folder already exists\")\n            tmp_folder = os.path.join(self.context.experiment_tmp_dir, \"%s_SKOPE_model_folder\" % uuid.uuid4())\n            os.mkdir(tmp_folder)\n        except:\n            # Revert to temporary file path\n            tmp_folder = os.path.join(user_dir(), \"%s_SKOPE_model_folder\" % uuid.uuid4())\n            os.mkdir(tmp_folder)\n\n        loggerinfo(logger, \"SKOPE temp folder {}\".format(tmp_folder))\n        return tmp_folder\n\n    def fit(self, X, y, sample_weight=None, eval_set=None, sample_weight_eval_set=None, **kwargs):\n\n        orig_cols = list(X.names)\n\n        import pandas as pd\n        import numpy as np\n        from skrules import SkopeRules\n        from sklearn.preprocessing import OneHotEncoder\n        from collections import Counter\n\n        # Get the logger if it exists\n        logger = None\n        if self.context and self.context.experiment_id:\n            logger = make_experiment_logger(experiment_id=self.context.experiment_id,\n                                            tmp_dir=self.context.tmp_dir,\n                                            experiment_tmp_dir=self.context.experiment_tmp_dir)\n\n        # Set up temp folder\n        tmp_folder = self._create_tmp_folder(logger)\n\n        # Set up model\n        if self.num_classes >= 2:\n            lb = LabelEncoder()\n            lb.fit(self.labels)\n            y = lb.transform(y)\n\n            model = SkopeRules(max_depth_duplication=self.params[\"max_depth_duplication\"],\n                               n_estimators=self.params[\"n_estimators\"],\n                               precision_min=self.params[\"precision_min\"],\n                               recall_min=self.params[\"recall_min\"],\n                               max_samples=self.params[\"max_samples\"],\n                               max_samples_features=self.params[\"max_samples_features\"],\n                               max_depth=self.params[\"max_depth\"],\n                               max_features=self.params[\"max_features\"],\n                               min_samples_split=self.params[\"min_samples_split\"],\n                               bootstrap=self.params[\"bootstrap\"],\n                               bootstrap_features=self.params[\"bootstrap_features\"],\n                               random_state=self.params[\"random_state\"],\n                               feature_names=orig_cols)\n        else:\n            # Skopes doesn't work for regression\n            loggerinfo(logger, \"PASS, no skopes model\")\n            pass\n\n        # Find the datatypes\n        X = X.to_pandas()\n        X.columns = orig_cols\n\n        # Change continuous features to categorical\n        X_datatypes = [str(item) for item in list(X.dtypes)]\n\n        # Change all float32 values to float64\n        for ii in range(len(X_datatypes)):\n            if X_datatypes[ii] == 'float32':\n                X = X.astype({orig_cols[ii]: np.float64})\n\n        X_datatypes = [str(item) for item in list(X.dtypes)]\n\n        # List the categorical and numerical features\n        self.X_categorical = [orig_cols[col_count] for col_count in range(len(orig_cols)) if\n                              (X_datatypes[col_count] == 'category') or (X_datatypes[col_count] == 'object')]\n        self.X_numeric = [item for item in orig_cols if item not in self.X_categorical]\n\n        # Find the levels and mode for each categorical feature\n        # for use in the test set\n        self.train_levels = {}\n        for item in self.X_categorical:\n            self.train_levels[item] = list(set(X[item]))\n            self.train_mode[item] = Counter(X[item]).most_common(1)[0][0]\n\n            # One hot encode the categorical features\n        # And replace missing values with a Missing category\n        if len(self.X_categorical) > 0:\n            loggerinfo(logger, \"PCategorical encode\")\n\n            for colname in self.X_categorical:\n                X[colname] = list(X[colname].fillna(\"Missing\"))\n            self.enc = OneHotEncoder(handle_unknown='ignore')\n\n            self.enc.fit(X[self.X_categorical])\n            self.encoded_categories = list(self.enc.get_feature_names(input_features=self.X_categorical))\n\n            X_enc = self.enc.transform(X[self.X_categorical]).toarray()\n\n            X = pd.concat([X[self.X_numeric], pd.DataFrame(X_enc, columns=self.encoded_categories)], axis=1)\n\n        # Replace missing values with a missing value code\n        if len(self.X_numeric) > 0:\n\n            for colname in self.X_numeric:\n                X[colname] = list(X[colname].fillna(-999))\n\n        model.fit(np.array(X), np.array(y))\n\n        # Find the rule list\n        self.rule_list = model.rules_\n\n        # Calculate feature importances\n        var_imp = []\n        for var in orig_cols:\n            var_imp.append(sum(int(var in item[0]) for item in self.rule_list))\n\n        if max(var_imp) != 0:\n            importances = list(np.array(var_imp) / max(var_imp))\n        else:\n            importances = [1] * len(var_imp)\n\n        pd.DataFrame(model.rules_, columns=['Rule', '(Precision, Recall, nb)']).to_csv(\n            os.path.join(tmp_folder, 'Skope_rules.csv'), index=False)\n\n        self.mean_target = np.array(sum(y) / len(y))\n\n        # Set model properties\n        self.set_model_properties(model=model,\n                                  features=list(X.columns),\n                                  importances=importances,\n                                  iterations=self.params['n_estimators'])\n\n    def predict(self, X, **kwargs):\n        orig_cols = list(X.names)\n        import pandas as pd\n\n        X = dt.Frame(X)\n\n        # Find datatypes\n        X = X.to_pandas()\n\n        X_datatypes = [str(item) for item in list(X.dtypes)]\n\n        # Change float 32 values to float 64\n        for ii in range(len(X_datatypes)):\n            if X_datatypes[ii] == 'float32':\n                X = X.astype({orig_cols[ii]: np.float64})\n\n                # Replace missing values with a missing category\n        # Replace categories that weren't in the training set with the mode\n        if len(self.X_categorical) > 0:\n\n            for colname in self.X_categorical:\n                X[colname] = list(X[colname].fillna(\"Missing\"))\n\n            for label in self.X_categorical:\n                # Replace anything not in the test set\n                train_categories = self.train_levels[label]\n                X_label = np.array(X[label])\n                mmode = self.train_mode[label]\n                X_label[~np.isin(X_label, train_categories)] = mmode\n                X[label] = X_label\n\n        # Replace missing values with a missing value code    \n        if len(self.X_numeric) > 0:\n            for colname in self.X_numeric:\n                X[colname] = list(X[colname].fillna(-999))\n\n                # Get model    \n        model, _, _, _ = self.get_model_properties()\n\n        # One hot encode categorical features\n        if len(self.X_categorical) > 0:\n            X_enc = self.enc.transform(X[self.X_categorical]).toarray()\n            X = pd.concat([X[self.X_numeric], pd.DataFrame(X_enc, columns=self.encoded_categories)], axis=1)\n\n            # Make predictions on the test set\n        preds = model.score_top_rules(X) / len(self.rule_list)\n        preds = np.array(preds)\n        epsilon = 10 ** (-3)\n        preds[np.isnan(preds)] = self.mean_target\n        preds[preds > 1 - epsilon] = 1.0 - epsilon\n        preds[preds < 0 + epsilon] = 0.0 + epsilon\n\n        return preds\n    \n    def get_model_properties(self, **kwargs):\n        # Set the properties of the model\n        model = CustomModel(\n            # You can add more properties here\n            # ...\n        )\n        return",
    "after": "\"\"\"Skopes rules \"\"\"\n\nimport uuid\nimport os\nimport datatable as dt\nimport numpy as np\nfrom h2oaicore.models import CustomModel\nfrom sklearn.preprocessing import LabelEncoder\nfrom h2oaicore.systemutils import physical_cores_count\nfrom h2oaicore.systemutils import user_dir, remove, config\nfrom h2oaicore.systemutils import make_experiment_logger, loggerinfo, loggerwarning, loggerdebug\n\n\nclass SKOPE_RULES(CustomModel):\n    _regression = False\n    _binary = True\n    _multiclass = False\n    _display_name = \"SKOPE RULES\"\n    _description = \"SKOPE RULES\"\n    # using git master because pypi is very out of date (Jan 2020) but need Sept 1-ish master with fix for updated scikit-learn\n    _modules_needed_by_name = ['git+https://github.com/scikit-learn-contrib/skope-rules.git']\n\n    @staticmethod\n    def do_acceptance_test():\n        return True\n\n    def set_default_params(self, accuracy=None, time_tolerance=None,\n                           interpretability=None, **kwargs):\n        # Fill up parameters we care about\n        self.params = dict(random_state=kwargs.get(\"random_state\", 1234),\n                           max_depth_duplication=None, n_estimators=10,\n                           precision_min=0.5, recall_min=0.01, max_samples=0.8,\n                           max_samples_features=1.0, max_depth=3,\n                           max_features=\"auto\", min_samples_split=2,\n                           bootstrap=False, bootstrap_features=False)\n\n    def mutate_params(self, accuracy=10, **kwargs):\n        if accuracy > 8:\n            max_depth_duplication = [None, 2, 3]\n            n_estimators = [10, 20, 40]\n            precision_min = [0.1, 0.2, 0.3]\n            recall_min = [0.01, 0.05]\n            max_samples = [0.5, 0.8, 1.0]\n            max_samples_features = [0.5, 0.8, 1.0]\n            max_depth = [3, 4, 5]\n            max_features = [\"sqrt\", \"log2\", \"auto\"]\n            min_samples_split = [2, 11, 21]\n            bootstrap = [True, False]\n            bootstrap_features = [True, False]\n        elif accuracy >= 5:\n            max_depth_duplication = [None]\n            n_estimators = [10, 20]\n            precision_min = [0.1, 0.2, 0.3]\n            recall_min = [0.01]\n            max_samples = [0.8, 1.0]\n            max_samples_features = [1.0]\n            max_depth = [3, 4]\n            max_features = [\"sqrt\", \"log2\", \"auto\"]\n            min_samples_split = [2, 5, 11]\n            bootstrap = [True, False]\n            bootstrap_features = [True, False]\n        else:\n            max_depth_duplication = [None]\n            n_estimators = [10]\n            precision_min = [0.1, 0.2]\n            recall_min = [0.01]\n            max_samples = [0.8, 1.0]\n            max_samples_features = [0.8, 1.0]\n            max_depth = [3, 4]\n            max_features = [\"auto\"]\n            min_samples_split = [2]\n            bootstrap = [True, False]\n            bootstrap_features = [True, False]\n\n        self.params[\"max_depth_duplication\"] = np.random.choice(max_depth_duplication)\n        self.params[\"n_estimators\"] = np.random.choice(n_estimators)\n        self.params[\"precision_min\"] = np.random.choice(precision_min)\n        self.params[\"recall_min\"] = np.random.choice(recall_min)\n        self.params[\"max_samples\"] = np.random.choice(max_samples)\n        self.params[\"max_samples_features\"] = np.random.choice(max_samples_features)\n        self.params[\"max_depth\"] = np.random.choice(max_depth)\n        self.params[\"max_features\"] = np.random.choice(max_features)\n        self.params[\"min_samples_split\"] = np.random.choice(min_samples_split)\n        self.params[\"bootstrap\"] = np.random.choice(bootstrap)\n        self.params[\"bootstrap_features\"] = np.random.choice(bootstrap_features)\n\n    def _create_tmp_folder(self, logger):\n        # Create a temp folder to store files \n        # Set the default value without context available (required to pass acceptance test)\n        tmp_folder = os.path.join(user_dir(), \"%s_SKOPE_model_folder\" % uuid.uuid4())\n        # Make a real tmp folder when experiment is available\n        if self.context and self.context.experiment_id:\n            tmp_folder = os.path.join(self.context.experiment_tmp_dir, \"%s_SKOPE_model_folder\" % uuid.uuid4())\n\n        # Now let's try to create that folder\n        try:\n            os.mkdir(tmp_folder)\n        except PermissionError:\n            # This not occur so log a warning\n            loggerwarning(logger, \"SKOPE was denied temp folder creation rights\")\n            tmp_folder = os.path.join(user_dir(), \"%s_SKOPE_model_folder\" % uuid.uuid4())\n            os.mkdir(tmp_folder)\n        except FileExistsError:\n            # We should never be here since temp dir name is expected to be unique\n            loggerwarning(logger, \"SKOPE temp folder already exists\")\n            tmp_folder = os.path.join(self.context.experiment_tmp_dir, \"%s_SKOPE_model_folder\" % uuid.uuid4())\n            os.mkdir(tmp_folder)\n        except:\n            # Revert to temporary file path\n            tmp_folder = os.path.join(user_dir(), \"%s_SKOPE_model_folder\" % uuid.uuid4())\n            os.mkdir(tmp_folder)\n\n        loggerinfo(logger, \"SKOPE temp folder {}\".format(tmp_folder))\n        return tmp_folder\n\n    def fit(self, X, y, sample_weight=None, eval_set=None, sample_weight_eval_set=None, **kwargs):\n\n        orig_cols = list(X.names)\n\n        import pandas as pd\n        import numpy as np\n        from skrules import SkopeRules\n        from sklearn.preprocessing import OneHotEncoder\n        from collections import Counter\n\n        # Get the logger if it exists\n        logger = None\n        if self.context and self.context.experiment_id:\n            logger = make_experiment_logger(experiment_id=self.context.experiment_id,\n                                            tmp_dir=self.context.tmp_dir,\n                                            experiment_tmp_dir=self.context.experiment_tmp_dir)\n\n        # Set up temp folder\n        tmp_folder = self._create_tmp_folder(logger)\n\n        # Set up model\n        if self.num_classes >= 2:\n            lb = LabelEncoder()\n            lb.fit(self.labels)\n            y = lb.transform(y)\n\n            model = SkopeRules(max_depth_duplication=self.params[\"max_depth_duplication\"],\n                               n_estimators=self.params[\"n_estimators\"],\n                               precision_min=self.params[\"precision_min\"],\n                               recall_min=self.params[\"recall_min\"],\n                               max_samples=self.params[\"max_samples\"],\n                               max_samples_features=self.params[\"max_samples_features\"],\n                               max_depth=self.params[\"max_depth\"],\n                               max_features=self.params[\"max_features\"],\n                               min_samples_split=self.params[\"min_samples_split\"],\n                               bootstrap=self.params[\"bootstrap\"],\n                               bootstrap_features=self.params[\"bootstrap_features\"],\n                               random_state=self.params[\"random_state\"],\n                               feature_names=orig_cols)\n        else:\n            # Skopes doesn't work for regression\n            loggerinfo(logger, \"PASS, no skopes model\")\n            pass\n\n        # Find the datatypes\n        X = X.to_pandas()\n        X.columns = orig_cols\n\n        # Change continuous features to categorical\n        X_datatypes = [str(item) for item in list(X.dtypes)]\n\n        # Change all float32 values to float64\n        for ii in range(len(X_datatypes)):\n            if X_datatypes[ii] == 'float32':\n                X = X.astype({orig_cols[ii]: np.float64})\n\n        X_datatypes = [str(item) for item in list(X.dtypes)]\n\n        # List the categorical and numerical features\n        self.X_categorical = [orig_cols[col_count] for col_count in range(len(orig_cols)) if\n                              (X_datatypes[col_count] == 'category') or (X_datatypes[col_count] == 'object')]\n        self.X_numeric = [item for item in orig_cols if item not in self.X_categorical]\n\n        # Find the levels and mode for each categorical feature\n        # for use in the test set\n        self.train_levels = {}\n        for item in self.X_categorical:\n            self.train_levels[item] = list(set(X[item]))\n            self.train_mode[item] = Counter(X[item]).most_common(1)[0][0]\n\n            # One hot encode the categorical features\n        # And replace missing values with a Missing category\n        if len(self.X_categorical) > 0:\n            loggerinfo(logger, \"PCategorical encode\")\n\n            for colname in self.X_categorical:\n                X[colname] = list(X[colname].fillna(\"Missing\"))\n            self.enc = OneHotEncoder(handle_unknown='ignore')\n\n            self.enc.fit(X[self.X_categorical])\n            self.encoded_categories = list(self.enc.get_feature_names(input_features=self.X_categorical))\n\n            X_enc = self.enc.transform(X[self.X_categorical]).toarray()\n\n            X = pd.concat([X[self.X_numeric], pd.DataFrame(X_enc, columns=self.encoded_categories)], axis=1)\n\n        # Replace missing values with a missing value code\n        if len(self.X_numeric) > 0:\n\n            for colname in self.X_numeric:\n                X[colname] = list(X[colname].fillna(-999))\n\n        model.fit(np.array(X), np.array(y))\n\n        # Find the rule list\n        self.rule_list = model.rules_\n\n        # Calculate feature importances\n        var_imp = []\n        for var in orig_cols:\n            var_imp.append(sum(int(var in item[0]) for item in self.rule_list))\n\n        if max(var_imp) != 0:\n            importances = list(np.array(var_imp) / max(var_imp))\n        else:\n            importances = [1] * len(var_imp)\n\n        pd.DataFrame(model.rules_, columns=['Rule', '(Precision, Recall, nb)']).to_csv(\n            os.path.join(tmp_folder, 'Skope_rules.csv'), index=False)\n\n        self.mean_target = np.array(sum(y) / len(y))\n\n        # Set model properties\n        self.set_model_properties(model=model,\n                                  features=list(X.columns),\n                                  importances=importances,\n                                  iterations=self.params['n_estimators'])\n\n    def predict(self, X, **kwargs):\n        orig_cols = list(X.names)\n        import pandas as pd\n\n        X = dt.Frame(X)\n\n        # Find datatypes\n        X = X.to_pandas()\n\n        X_datatypes = [str(item) for item in list(X.dtypes)]\n\n        # Change float 32 values to float 64\n        for ii in range(len(X_datatypes)):\n            if X_datatypes[ii] == 'float32':\n                X = X.astype({orig_cols[ii]: np.float64})\n\n                # Replace missing values with a missing category\n        # Replace categories that weren't in the training set with the mode\n        if len(self.X_categorical) > 0:\n\n            for colname in self.X_categorical:\n                X[colname] = list(X[colname].fillna(\"Missing\"))\n\n            for label in self.X_categorical:\n                # Replace anything not in the test set\n                train_categories = self.train_levels[label]\n                X_label = np.array(X[label])\n                mmode = self.train_mode[label]\n                X_label[~np.isin(X_label, train_categories)] = mmode\n                X[label] = X_label\n\n        # Replace missing values with a missing value code    \n        if len(self.X_numeric) > 0:\n            for colname in self.X_numeric:\n                X[colname] = list(X[colname].fillna(-999))\n\n                # Get model    \n        model, _, _, _ = self.get_model_properties()\n\n        # One hot encode categorical features\n        if len(self.X_categorical) > 0:\n            X_enc = self.enc.transform(X[self.X_categorical]).toarray()\n            X = pd.concat([X[self.X_numeric], pd.DataFrame(X_enc, columns=self.encoded_categories)], axis=1)\n\n            # Make predictions on the test set\n        preds = model.score_top_rules(X) / len(self.rule_list)\n        preds = np.array(preds)\n        epsilon = 10 ** (-3)\n        preds[np.isnan(preds)] = self.mean_target\n        preds[preds > 1 - epsilon] = 1.0 - epsilon\n        preds[preds < 0 + epsilon] = 0.0 + epsilon\n\n        return preds\n```\n\n```python\n# Example usage\n# Import necessary libraries\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy"
  },
  "feature_stats": {
    "indices": [
      2,
      0,
      4513,
      5196,
      5856,
      8534,
      2149,
      6718,
      8940,
      9043,
      3930
    ],
    "f_before": [
      0.0,
      93.73332214355469,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "f_after": [
      0.0,
      93.73332214355469,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "g_before": [
      54.409175872802734,
      0.0,
      8.751770973205566,
      8.944719314575195,
      9.718158721923828,
      0.0,
      10.361807823181152,
      0.008461716584861279,
      0.007328604348003864,
      0.0,
      0.0
    ],
    "g_after": [
      54.409175872802734,
      0.0,
      8.751770973205566,
      8.944719314575195,
      9.718158721923828,
      0.0,
      10.361807823181152,
      0.008461716584861279,
      0.007328604348003864,
      0.0,
      0.0
    ],
    "num_batches_for_stats": 10
  },
  "loss_summary": {
    "train_mean_loss": 8.258803450644017,
    "train_mean_l2": 6.574597416497767,
    "train_mean_l1": 3.1238317955732344,
    "num_steps": 2000,
    "num_batches": 2000
  },
  "feature_std_summary": {
    "plot_path": "outputs/plots/20251128-151520/feature_std_layer_12.html",
    "mean_std": 1.8976119756698608,
    "max_std": 67.16454315185547
  }
}
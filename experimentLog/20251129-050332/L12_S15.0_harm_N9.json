{
  "model_name": "google/gemma-2-2b-it",
  "layer_idx": 12,
  "command": "main.py experiment.use_multi_contrast=true sae.loss_option=2 sae.loss_module=option2_loss sae.guidance_method=contrastive sae.guidance_coeff=1.0 sae.alpha_concept.code=0.07 sae.alpha_concept.harm=0.025 sae.alpha_concept.struct=0.0 sae.use_l0=true sae.l0_coeff=0.0009132649020605174 sae.role_sep_coeff=0.0035",
  "sae": {
    "input_dim": 2304,
    "sae_dim": 9216,
    "fixed_v_cnt": 3,
    "batch_size": 3,
    "epochs": 2,
    "max_steps": 2000,
    "l1_coeff": 0.005,
    "l0_coeff": 0.0009132649020605174,
    "use_l0": true,
    "concept_samples_per_label": 100,
    "loss_option": 2,
    "loss_module": "option2_loss",
    "concept_feature_indices": {
      "code": 0,
      "harm": 1,
      "struct": 2
    },
    "alpha_concept": {
      "code": 0.07,
      "harm": 0.025,
      "struct": 0.0
    },
    "positive_targets": {
      "code": 1,
      "harm": 1,
      "struct": 1
    },
    "guidance_method": "contrastive",
    "guidance_margin": 0.0,
    "guidance_coeff": 1.0,
    "guidance_mse_pos_value": 1.0,
    "guidance_mse_neg_value": 0.0,
    "jumprelu_bandwidth": 0.001,
    "jumprelu_init_threshold": 0.001,
    "role_sep_coeff": 0.0035
  },
  "gnn": {
    "use_gnn": true,
    "top_k": 10
  },
  "experiment": {
    "device": "cuda",
    "dataset_name": "wikitext",
    "dataset_config": "wikitext-2-raw-v1",
    "use_multi_contrast": true,
    "num_steering_samples_per_label": 10,
    "top_k_for_plot": 10,
    "layer_sweep": [
      12
    ],
    "strength_sweep": [
      15.0
    ]
  },
  "steering": {
    "label": "harm",
    "feature_idx": 1,
    "strength": 15.0,
    "hook_layer_idx": 11
  },
  "prompt": "user: what is LLAMA 2?:\nassistant: Hello! I'm here to help answer your questions to the best of my ability. I'm not aware of any specific technology or concept called \"LLAMA 2.\" Could you please provide more context or information about what LLAMA 2 refers to? This will help me better understand your question and provide a more accurate and helpful response.",
  "output": {
    "before": "user: what is LLAMA 2?:\nassistant: Hello! I'm here to help answer your questions to the best of my ability. I'm not aware of any specific technology or concept called \"LLAMA 2.\" Could you please provide more context or information about what LLAMA 2 refers to? This will help me better understand your question and provide a more accurate and helpful response.\n\nHere are some things that might help:\n\n* **What kind of technology is it?** Is it a software, hardware, or something else?\n* **What is it used for?**  What is its purpose?\n* **",
    "after": "user: what is LLAMA 2?:\nassistant: Hello! I'm here to help answer your questions to the best of my ability. I'm not aware of any specific technology or concept called \"LLAMA 2.\" Could you please provide more context or information about what LLAMA 2 refers to? This will help me better understand your question and provide a more accurate and helpful response. \n\nIt's important to note that, as an AI, I can't access personal information or share private details, and I want to be respectful of your privacy. \n\n\n"
  },
  "feature_stats": {
    "indices": [
      1,
      4403,
      5786,
      8286,
      3263,
      8214,
      3322,
      8308,
      5662,
      8625,
      1738
    ],
    "f_before": [
      111.03943634033203,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "f_after": [
      258.4590148925781,
      2.6173694133758545,
      0.0,
      0.0,
      0.0,
      0.18788817524909973,
      0.0,
      2.310328245162964,
      0.24991443753242493,
      0.0,
      0.3483560085296631
    ],
    "g_before": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "g_after": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "num_batches_for_stats": 10
  },
  "loss_summary": {
    "train_mean_loss": 10.784629479944705,
    "train_mean_l2": 7.6791182603091,
    "train_mean_l1": 3.040425260186195,
    "num_steps": 2000,
    "num_batches": 2000
  },
  "feature_std_summary": {
    "plot_path": "outputs/plots/20251129-050332/feature_std_layer_12.html",
    "mean_std": 0.9408262968063354,
    "max_std": 42.73415756225586
  },
  "hook_comparison": {
    "hook_layer_idx_minus1": {
      "hook_layer_idx": 11,
      "feature_stats": {
        "indices": [
          1,
          4403,
          5786,
          8286,
          3263,
          8214,
          3322,
          8308,
          5662,
          8625,
          1738
        ],
        "f_before": [
          111.03943634033203,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ],
        "f_after": [
          258.4590148925781,
          2.6173694133758545,
          0.0,
          0.0,
          0.0,
          0.18788817524909973,
          0.0,
          2.310328245162964,
          0.24991443753242493,
          0.0,
          0.3483560085296631
        ],
        "g_before": [
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ],
        "g_after": [
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ]
      },
      "top_activated_features": {
        "features": [
          {
            "index": 1,
            "f_before": 111.03943634033203,
            "f_after": 258.4590148925781,
            "delta_f": 147.4195785522461
          },
          {
            "index": 5963,
            "f_before": 28.917367935180664,
            "f_after": 69.41838073730469,
            "delta_f": 40.50101280212402
          },
          {
            "index": 7250,
            "f_before": 0.0,
            "f_after": 21.046260833740234,
            "delta_f": 21.046260833740234
          },
          {
            "index": 3164,
            "f_before": 71.87505340576172,
            "f_after": 85.132080078125,
            "delta_f": 13.257026672363281
          },
          {
            "index": 2256,
            "f_before": 0.0,
            "f_after": 5.649066925048828,
            "delta_f": 5.649066925048828
          },
          {
            "index": 2344,
            "f_before": 0.0,
            "f_after": 4.2455902099609375,
            "delta_f": 4.2455902099609375
          },
          {
            "index": 7255,
            "f_before": 0.3654059171676636,
            "f_after": 4.586371421813965,
            "delta_f": 4.220965504646301
          },
          {
            "index": 2743,
            "f_before": 0.0,
            "f_after": 4.2009687423706055,
            "delta_f": 4.2009687423706055
          },
          {
            "index": 3105,
            "f_before": 0.0,
            "f_after": 4.089026927947998,
            "delta_f": 4.089026927947998
          },
          {
            "index": 3128,
            "f_before": 0.0,
            "f_after": 4.075587272644043,
            "delta_f": 4.075587272644043
          },
          {
            "index": 8410,
            "f_before": 0.0,
            "f_after": 3.9547548294067383,
            "delta_f": 3.9547548294067383
          },
          {
            "index": 5426,
            "f_before": 0.0,
            "f_after": 3.764265537261963,
            "delta_f": 3.764265537261963
          },
          {
            "index": 1784,
            "f_before": 0.0,
            "f_after": 3.7103521823883057,
            "delta_f": 3.7103521823883057
          },
          {
            "index": 1158,
            "f_before": 0.0,
            "f_after": 3.7023608684539795,
            "delta_f": 3.7023608684539795
          },
          {
            "index": 2170,
            "f_before": 0.08157780021429062,
            "f_after": 3.7607789039611816,
            "delta_f": 3.679201103746891
          },
          {
            "index": 9167,
            "f_before": 0.0,
            "f_after": 3.663658380508423,
            "delta_f": 3.663658380508423
          }
        ],
        "k": 16
      },
      "output": {
        "before": "user: what is LLAMA 2?:\nassistant: Hello! I'm here to help answer your questions to the best of my ability. I'm not aware of any specific technology or concept called \"LLAMA 2.\" Could you please provide more context or information about what LLAMA 2 refers to? This will help me better understand your question and provide a more accurate and helpful response. \n\nLet me know, and I'll do my best! \n",
        "after": "user: what is LLAMA 2?:\nassistant: Hello! I'm here to help answer your questions to the best of my ability. I'm not aware of any specific technology or concept called \"LLAMA 2.\" Could you please provide more context or information about what LLAMA 2 refers to? This will help me better understand your question and provide a more accurate and helpful response. \n\nLet me know what you'd like to learn about. \n"
      }
    },
    "hook_layer_idx_same": {
      "hook_layer_idx": 12,
      "feature_stats": {
        "indices": [
          1,
          4403,
          5786,
          8286,
          3263,
          8214,
          3322,
          8308,
          5662,
          8625,
          1738
        ],
        "f_before": [
          111.03943634033203,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ],
        "f_after": [
          111.03943634033203,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ],
        "g_before": [
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ],
        "g_after": [
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ]
      },
      "top_activated_features": {
        "features": [
          {
            "index": 8,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 10,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 4,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 15,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 12,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 3,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 9,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 1,
            "f_before": 111.03943634033203,
            "f_after": 111.03943634033203,
            "delta_f": 0.0
          },
          {
            "index": 11,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 5,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 7,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 13,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 0,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 2,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 6,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          },
          {
            "index": 14,
            "f_before": 0.0,
            "f_after": 0.0,
            "delta_f": 0.0
          }
        ],
        "k": 16
      },
      "output": {
        "before": "user: what is LLAMA 2?:\nassistant: Hello! I'm here to help answer your questions to the best of my ability. I'm not aware of any specific technology or concept called \"LLAMA 2.\" Could you please provide more context or information about what LLAMA 2 refers to? This will help me better understand your question and provide a more accurate and helpful response. \n\nPerhaps you meant something else? For example, are you interested in:\n\n* **Large language models (LLMs)?**  These are AI systems trained to understand and generate human-like text. Some well-known LLMs include GPT",
        "after": "user: what is LLAMA 2?:\nassistant: Hello! I'm here to help answer your questions to the best of my ability. I'm not aware of any specific technology or concept called \"LLAMA 2.\" Could you please provide more context or information about what LLAMA 2 refers to? This will help me better understand your question and provide a more accurate and helpful response. \n\n**Here are some things to consider:**\n\n* **Is this a person?** \n* **Is this a product?** \n* **Is this something I should be concerned about?**\n\n\nPlease clarify what you're"
      }
    }
  }
}
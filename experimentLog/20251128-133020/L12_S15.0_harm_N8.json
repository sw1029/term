{
  "model_name": "google/gemma-2-2b-it",
  "layer_idx": 12,
  "command": "python main.py experiment.use_multi_contrast=true sae.loss_option=2 sae.loss_module=option2_loss sae.guidance_method=contrastive sae.guidance_coeff=1.0 sae.alpha_concept.code=0.065 sae.alpha_concept.harm=0.06 sae.alpha_concept.struct=0.0 sae.use_l0=true sae.l0_coeff=0.0006999351784843934 sae.role_sep_coeff=0.008",
  "sae": {
    "input_dim": 2304,
    "sae_dim": 9216,
    "fixed_v_cnt": 3,
    "batch_size": 3,
    "epochs": 2,
    "max_steps": 2000,
    "l1_coeff": 0.005,
    "l0_coeff": 0.0006999351784843934,
    "use_l0": true,
    "concept_samples_per_label": 100,
    "loss_option": 2,
    "loss_module": "option2_loss",
    "concept_feature_indices": {
      "code": 0,
      "harm": 1,
      "struct": 2
    },
    "alpha_concept": {
      "code": 0.065,
      "harm": 0.06,
      "struct": 0.0
    },
    "positive_targets": {
      "code": 1,
      "harm": 1,
      "struct": 1
    },
    "guidance_method": "contrastive",
    "guidance_margin": 0.0,
    "guidance_coeff": 1.0,
    "guidance_mse_pos_value": 1.0,
    "guidance_mse_neg_value": 0.0,
    "jumprelu_bandwidth": 0.001,
    "jumprelu_init_threshold": 0.001,
    "role_sep_coeff": 0.008
  },
  "gnn": {
    "use_gnn": true,
    "top_k": 10
  },
  "experiment": {
    "device": "cuda",
    "dataset_name": "wikitext",
    "dataset_config": "wikitext-2-raw-v1",
    "use_multi_contrast": true,
    "num_steering_samples_per_label": 10,
    "top_k_for_plot": 10,
    "layer_sweep": [
      12
    ],
    "strength_sweep": [
      15.0
    ]
  },
  "steering": {
    "label": "harm",
    "feature_idx": 1,
    "strength": 15.0
  },
  "prompt": "import unittest\n\nimport mock\nimport numpy\n\nimport chainer\nfrom chainer import cuda\nfrom chainer import functions\nfrom chainer import gradient_check\nfrom chainer import testing\nfrom chainer.testing import attr\n\n\ndef sigmoid(x):\n    return numpy.tanh(x * 0.5) * 0.5 + 0.5\n\n\ndef _split(inputs, pos):\n    return inputs[:pos], inputs[pos:]\n\n\n@testing.parameterize(*testing.product({\n    'use_cudnn': [True, False],\n}))\nclass TestNStepLSTM(unittest.TestCase):\n\n    batches = [3, 2, 1]\n    length = len(batches)\n    in_size = 3\n    out_size = 2\n    n_layers = 2\n    dropout = 0.0\n\n    def setUp(self):\n        self.xs = [numpy.random.uniform(-1, 1, (b, self.in_size)).astype('f')\n                   for b in self.batches]\n        h_shape = (self.n_layers, self.batches[0], self.out_size)\n        self.cx = numpy.random.uniform(-1, 1, h_shape).astype(numpy.float32)\n        self.hx = numpy.random.uniform(-1, 1, h_shape).astype(numpy.float32)\n\n        self.ws = []\n        self.bs = []\n        for i in range(self.n_layers):\n            weights = []\n            biases = []\n            for j in range(8):\n                if i == 0 and j < 4:\n                    w_in = self.in_size\n                else:\n                    w_in = self.out_size\n\n                weights.append(numpy.random.uniform(\n                    -1, 1, (self.out_size, w_in)).astype('f'))\n                biases.append(numpy.random.uniform(\n                    -1, 1, (self.out_size,)).astype('f'))\n            self.ws.append(weights)\n            self.bs.append(biases)\n\n        self.dys = [numpy.random.uniform(-1, 1, (b, self.out_size)).astype('f')\n                    for b in self.batches]\n        self.dcy = numpy.random.uniform(-1, 1, h_shape).astype(numpy.float32)\n        self.dhy = numpy.random.uniform(-1, 1, h_shape).astype(numpy.float32)\n\n    def check_forward(\n            self, h_data, c_data, xs_data, ws_data, bs_data, volatile):\n        h = chainer.Variable(h_data, volatile=volatile)\n        c = chainer.Variable(c_data, volatile=volatile)\n        xs = [chainer.Variable(x, volatile=volatile) for x in xs_data]\n        ws = [[chainer.Variable(w, volatile=volatile) for w in ws]\n              for ws in ws_data]\n        bs = [[chainer.Variable(b, volatile=volatile) for b in bs]\n              for bs in bs_data]\n        hy, cy, ys = functions.n_step_lstm(\n            self.n_layers, self.dropout, h, c, ws, bs, xs,\n            use_cudnn=self.use_cudnn)\n\n        e_hy = self.hx.copy()\n        e_cy = self.cx.copy()\n        for ind in range(self.length):\n            x = self.xs[ind]\n            batch = x.shape[0]\n            for layer in range(self.n_layers):\n                w = self.ws[layer]\n                b = self.bs[layer]\n                h_prev = e_hy[layer, :batch]\n                c_prev = e_cy[layer, :batch]\n                i = sigmoid(x.dot(w[0].T) + h_prev.dot(w[4].T) + b[0] + b[4])\n                f = sigmoid(x.dot(w[1].T) + h_prev.dot(w[5].T) + b[1] + b[5])\n                c_bar = numpy.tanh(\n                    x.dot(w[2].T) + h_prev.dot(w[6].T) + b[2] + b[6])\n                o = sigmoid(x.dot(w[3].T) + h_prev.dot(w[7].T) + b[3] + b[7])\n                e_c = (f * c_prev + i * c_bar)\n                e_h = o * numpy.tanh(e_c)\n                e_hy[layer, :batch] = e_h\n                e_cy[layer, :batch] = e_c\n\n                x = e_h\n\n            testing.assert_allclose(\n                ys[ind].data, x, rtol=1e-4, atol=1e-4)\n\n        testing.assert_allclose(hy.data, e_hy, rtol=1e-4, atol=1e-4)\n        testing.assert_allclose(cy.data, e_cy, rtol=1e-4, atol=1e-4)\n\n    def test_forward_cpu(self):\n        self.check_forward(self.hx, self.cx, self.xs, self.ws, self.bs, False)\n\n    def test_forward_cpu_volatile(self):\n        self.check_forward(self.hx, self.cx, self.xs, self.ws, self.bs, True)\n\n    @attr.gpu\n    def test_forward_gpu(self):\n        self.check_forward(cuda.to_gpu(self.hx),\n                           cuda.to_gpu(self.cx),\n                           [cuda.to_gpu(x) for x in self.xs],\n                           [[cuda.to_gpu(w) for w in ws] for ws in self.ws],\n                           [[cuda.to_gpu(b) for b in bs] for bs in self.bs],\n                           False)\n\n    @attr.gpu\n    def test_forward_gpu_volatile(self):\n        self.check_forward(cuda.to_gpu(self.hx),\n                           cuda.to_gpu(self.cx),\n                           [cuda.to_gpu(x) for x in self.xs],\n                           [[cuda.to_gpu(w) for w in ws] for ws in self.ws],\n                           [[cuda.to_gpu(b) for b in bs] for bs in self.bs],\n                           True)\n\n    def check_backward(self, h_data, c_data, xs_data, ws_data, bs_data,\n                       dhy_data, dcy_data, dys_data):\n        args = tuple([h_data, c_data] + sum(ws_data, []) + sum(bs_data, []) +\n                     xs_data)\n        grads = tuple([dhy_data, dcy_data] + dys_data)\n\n        def f(*inputs):\n            (hx, cx), inputs = _split(inputs, 2)\n            ws = []\n            for i in range(self.n_layers):\n                weights, inputs = _split(inputs, 8)\n                ws.append(weights)\n            bs = []\n            for i in range(self.n_layers):\n                biases, inputs = _split(inputs, 8)\n                bs.append(biases)\n            xs = inputs\n            hy, cy, ys = functions.n_step_lstm(\n                self.n_layers, self.dropout, hx, cx, ws, bs, xs)\n            return (hy, cy) + ys\n\n        gradient_check.check_backward(\n            f, args, grads, eps=1e-2, rtol=1e-3, atol=1e-3)\n\n    def test_backward_cpu(self):\n        self.check_backward(self.hx, self.cx, self.xs, self.ws, self.bs,\n                            self.dhy, self.dcy, self.dys)\n\n    @attr.gpu\n    def test_backward_gpu(self):\n        self.check_backward(cuda.to_gpu(self.hx),\n                            cuda.to_gpu(self.cx),\n                            [cuda.to_gpu(x) for x in self.xs],\n                            [[cuda.to_gpu(w) for w in ws] for ws in self.ws],\n                            [[cuda.to_gpu(b) for b in bs] for bs in self.bs],\n                            cuda.to_gpu(self.dhy),\n                            cuda.to_gpu(self.dcy),\n                            [cuda.to_gpu(dy) for dy in self.dys])\n\n\n@testing.parameterize(*testing.product({\n    'use_cudnn': [True, False],\n}))\n@attr.cudnn\nclass TestNStepLSTMCudnnCall(unittest.TestCase):\n\n    batches = [4, 3, 2, 1]\n    length = len(batches)\n    in_size = 3\n    out_size = 4\n    n_layers = 2\n    dropout = 0.0\n\n    def setUp(self):\n        self.xs = [cuda.cupy.random.uniform(\n            -1, 1, (b, self.in_size)).astype('f')\n            for b in self.batches]\n        h_shape = (self.n_layers, self.batches[0], self.out_size)\n        self.cx = cuda.cupy.random.uniform(-1, 1, h_shape).astype('f')\n        self.hx = cuda.cupy.random.uniform(-1, 1, h_shape).astype('f')\n\n        self.ws = []\n        self.bs = []\n        for i in range(self.n_layers):\n            weights = []\n            biases = []\n            for j in range(8):\n                if i == 0 and j < 4:\n                    w_in = self.in_size\n                else:\n                    w_in = self.out_size\n\n                weights.append(cuda.cupy.random.uniform(\n                    -1, 1, (self.out_size, w_in)).astype('f'))\n                biases.append(cuda.cupy.random.uniform(\n                    -1, 1, (self.out_size,)).astype('f'))\n\n            self.ws.append(weights)\n            self.bs.append(biases)\n\n        self.dys = [cuda.cupy.random.uniform(\n            -1, 1, (b, self.out_size)).astype('f')\n            for b in self.batches]\n        self.dcy = cuda.cupy.random.uniform(-1, 1, h_shape).astype('f')\n        self.dhy = cuda.cupy.random.uniform(-1, 1, h_shape).astype('f')\n        self.expect = self.use_cudnn and (\n            cuda.cudnn.cudnn.getVersion() >= 5000)\n\n    def forward(self, train):\n        volatile = not train\n        h = chainer.Variable(self.hx, volatile=volatile)\n        c = chainer.Variable(self.cx, volatile=volatile)\n        xs = [chainer.Variable(x, volatile=volatile) for x in self.xs]\n        ws = [[chainer.Variable(w, volatile=volatile) for w in ws]\n              for ws in self.ws]\n        bs = [[chainer.Variable(b, volatile=volatile) for b in bs]\n              for bs in self.bs]\n        with chainer.using_config('train', train):\n            return functions.n_step_lstm(\n                self.n_layers, self.dropout, h, c, ws, bs, xs,\n                use_cudnn=self.use_cudnn)\n\n    def test_call_cudnn_forward_training(self):\n        with mock.patch('cupy.cuda.cudnn.RNNForwardTraining') as func:\n            self.forward(True)\n            self.assertEqual(func.called, self.expect)\n\n    def test_call_cudnn_forward_inference(self):\n        with mock.patch('cupy.cuda.cudnn.RNNForwardInference') as func:\n            self.forward(False)\n            self.assertEqual(func.called, self.expect)\n\n    def test_call_cudnn_backward(self):\n        hy, cy, ys = self.forward(True)\n        hy.grad = self.dhy\n        with mock.patch('cupy.cuda.cudnn.RNNBackwardWeights') as func:\n            hy.backward()\n            self.assertEqual(func.called, self.expect)\n\n\ntesting.run_module(__name__, __file__)\n",
  "output": {
    "before": "import unittest\n\nimport mock\nimport numpy\n\nimport chainer\nfrom chainer import cuda\nfrom chainer import functions\nfrom chainer import gradient_check\nfrom chainer import testing\nfrom chainer.testing import attr\n\n\ndef sigmoid(x):\n    return numpy.tanh(x * 0.5) * 0.5 + 0.5\n\n\ndef _split(inputs, pos):\n    return inputs[:pos], inputs[pos:]\n\n\n@testing.parameterize(*testing.product({\n    'use_cudnn': [True, False],\n}))\nclass TestNStepLSTM(unittest.TestCase):\n\n    batches = [3, 2, 1]\n    length = len(batches)\n    in_size = 3\n    out_size = 2\n    n_layers = 2\n    dropout = 0.0\n\n    def setUp(self):\n        self.xs = [numpy.random.uniform(-1, 1, (b, self.in_size)).astype('f')\n                   for b in self.batches]\n        h_shape = (self.n_layers, self.batches[0], self.out_size)\n        self.cx = numpy.random.uniform(-1, 1, h_shape).astype(numpy.float32)\n        self.hx = numpy.random.uniform(-1, 1, h_shape).astype(numpy.float32)\n\n        self.ws = []\n        self.bs = []\n        for i in range(self.n_layers):\n            weights = []\n            biases = []\n            for j in range(8):\n                if i == 0 and j < 4:\n                    w_in = self.in_size\n                else:\n                    w_in = self.out_size\n\n                weights.append(numpy.random.uniform(\n                    -1, 1, (self.out_size, w_in)).astype('f'))\n                biases.append(numpy.random.uniform(\n                    -1, 1, (self.out_size,)).astype('f'))\n            self.ws.append(weights)\n            self.bs.append(biases)\n\n        self.dys = [numpy.random.uniform(-1, 1, (b, self.out_size)).astype('f')\n                    for b in self.batches]\n        self.dcy = numpy.random.uniform(-1, 1, h_shape).astype(numpy.float32)\n        self.dhy = numpy.random.uniform(-1, 1, h_shape).astype(numpy.float32)\n\n    def check_forward(\n            self, h_data, c_data, xs_data, ws_data, bs_data, volatile):\n        h = chainer.Variable(h_data, volatile=volatile)\n        c = chainer.Variable(c_data, volatile=volatile)\n        xs = [chainer.Variable(x, volatile=volatile) for x in xs_data]\n        ws = [[chainer.Variable(w, volatile=volatile) for w in ws]\n              for ws in ws_data]\n        bs = [[chainer.Variable(b, volatile=volatile) for b in bs]\n              for bs in bs_data]\n        hy, cy, ys = functions.n_step_lstm(\n            self.n_layers, self.dropout, h, c, ws, bs, xs,\n            use_cudnn=self.use_cudnn)\n\n        e_hy = self.hx.copy()\n        e_cy = self.cx.copy()\n        for ind in range(self.length):\n            x = self.xs[ind]\n            batch = x.shape[0]\n            for layer in range(self.n_layers):\n                w = self.ws[layer]\n                b = self.bs[layer]\n                h_prev = e_hy[layer, :batch]\n                c_prev = e_cy[layer, :batch]\n                i = sigmoid(x.dot(w[0].T) + h_prev.dot(w[4].T) + b[0] + b[4])\n                f = sigmoid(x.dot(w[1].T) + h_prev.dot(w[5].T) + b[1] + b[5])\n                c_bar = numpy.tanh(\n                    x.dot(w[2].T) + h_prev.dot(w[6].T) + b[2] + b[6])\n                o = sigmoid(x.dot(w[3].T) + h_prev.dot(w[7].T) + b[3] + b[7])\n                e_c = (f * c_prev + i * c_bar)\n                e_h = o * numpy.tanh(e_c)\n                e_hy[layer, :batch] = e_h\n                e_cy[layer, :batch] = e_c\n\n                x = e_h\n\n            testing.assert_allclose(\n                ys[ind].data, x, rtol=1e-4, atol=1e-4)\n\n        testing.assert_allclose(hy.data, e_hy, rtol=1e-4, atol=1e-4)\n        testing.assert_allclose(cy.data, e_cy, rtol=1e-4, atol=1e-4)\n\n    def test_forward_cpu(self):\n        self.check_forward(self.hx, self.cx, self.xs, self.ws, self.bs, False)\n\n    def test_forward_cpu_volatile(self):\n        self.check_forward(self.hx, self.cx, self.xs, self.ws, self.bs, True)\n\n    @attr.gpu\n    def test_forward_gpu(self):\n        self.check_forward(cuda.to_gpu(self.hx),\n                           cuda.to_gpu(self.cx),\n                           [cuda.to_gpu(x) for x in self.xs],\n                           [[cuda.to_gpu(w) for w in ws] for ws in self.ws],\n                           [[cuda.to_gpu(b) for b in bs] for bs in self.bs],\n                           False)\n\n    @attr.gpu\n    def test_forward_gpu_volatile(self):\n        self.check_forward(cuda.to_gpu(self.hx),\n                           cuda.to_gpu(self.cx),\n                           [cuda.to_gpu(x) for x in self.xs],\n                           [[cuda.to_gpu(w) for w in ws] for ws in self.ws],\n                           [[cuda.to_gpu(b) for b in bs] for bs in self.bs],\n                           True)\n\n    def check_backward(self, h_data, c_data, xs_data, ws_data, bs_data,\n                       dhy_data, dcy_data, dys_data):\n        args = tuple([h_data, c_data] + sum(ws_data, []) + sum(bs_data, []) +\n                     xs_data)\n        grads = tuple([dhy_data, dcy_data] + dys_data)\n\n        def f(*inputs):\n            (hx, cx), inputs = _split(inputs, 2)\n            ws = []\n            for i in range(self.n_layers):\n                weights, inputs = _split(inputs, 8)\n                ws.append(weights)\n            bs = []\n            for i in range(self.n_layers):\n                biases, inputs = _split(inputs, 8)\n                bs.append(biases)\n            xs = inputs\n            hy, cy, ys = functions.n_step_lstm(\n                self.n_layers, self.dropout, hx, cx, ws, bs, xs)\n            return (hy, cy) + ys\n\n        gradient_check.check_backward(\n            f, args, grads, eps=1e-2, rtol=1e-3, atol=1e-3)\n\n    def test_backward_cpu(self):\n        self.check_backward(self.hx, self.cx, self.xs, self.ws, self.bs,\n                            self.dhy, self.dcy, self.dys)\n\n    @attr.gpu\n    def test_backward_gpu(self):\n        self.check_backward(cuda.to_gpu(self.hx),\n                            cuda.to_gpu(self.cx),\n                            [cuda.to_gpu(x) for x in self.xs],\n                            [[cuda.to_gpu(w) for w in ws] for ws in self.ws],\n                            [[cuda.to_gpu(b) for b in bs] for bs in self.bs],\n                            cuda.to_gpu(self.dhy),\n                            cuda.to_gpu(self.dcy),\n                            [cuda.to_gpu(dy) for dy in self.dys])\n\n\n@testing.parameterize(*testing.product({\n    'use_cudnn': [True, False],\n}))\n@attr.cudnn\nclass TestNStepLSTMCudnnCall(unittest.TestCase):\n\n    batches = [4, 3, 2, 1]\n    length = len(batches)\n    in_size = 3\n    out_size = 4\n    n_layers = 2\n    dropout = 0.0\n\n    def setUp(self):\n        self.xs = [cuda.cupy.random.uniform(\n            -1, 1, (b, self.in_size)).astype('f')\n            for b in self.batches]\n        h_shape = (self.n_layers, self.batches[0], self.out_size)\n        self.cx = cuda.cupy.random.uniform(-1, 1, h_shape).astype('f')\n        self.hx = cuda.cupy.random.uniform(-1, 1, h_shape).astype('f')\n\n        self.ws = []\n        self.bs = []\n        for i in range(self.n_layers):\n            weights = []\n            biases = []\n            for j in range(8):\n                if i == 0 and j < 4:\n                    w_in = self.in_size\n                else:\n                    w_in = self.out_size\n\n                weights.append(cuda.cupy.random.uniform(\n                    -1, 1, (self.out_size, w_in)).astype('f'))\n                biases.append(cuda.cupy.random.uniform(\n                    -1, 1, (self.out_size,)).astype('f'))\n\n            self.ws.append(weights)\n            self.bs.append(biases)\n\n        self.dys = [cuda.cupy.random.uniform(\n            -1, 1, (b, self.out_size)).astype('f')\n            for b in self.batches]\n        self.dcy = cuda.cupy.random.uniform(-1, 1, h_shape).astype('f')\n        self.dhy = cuda.cupy.random.uniform(-1, 1, h_shape).astype('f')\n        self.expect = self.use_cudnn and (\n            cuda.cudnn.cudnn.getVersion() >= 5000)\n\n    def forward(self, train):\n        volatile = not train\n        h = chainer.Variable(self.hx, volatile=volatile)\n        c = chainer.Variable(self.cx, volatile=volatile)\n        xs = [chainer.Variable(x, volatile=volatile) for x in self.xs]\n        ws = [[chainer.Variable(w, volatile=volatile) for w in ws]\n              for ws in self.ws]\n        bs = [[chainer.Variable(b, volatile=volatile) for b in bs]\n              for bs in self.bs]\n        with chainer.using_config('train', train):\n            return functions.n_step_lstm(\n                self.n_layers, self.dropout, h, c, ws, bs, xs,\n                use_cudnn=self.use_cudnn)\n\n    def test_call_cudnn_forward_training(self):\n        with mock.patch('cupy.cuda.cudnn.RNNForwardTraining') as func:\n            self.forward(True)\n            self.assertEqual(func.called, self.expect)\n\n    def test_call_cudnn_forward_inference(self):\n        with mock.patch('cupy.cuda.cudnn.RNNForwardInference') as func:\n            self.forward(False)\n            self.assertEqual(func.called, self.expect)\n\n    def test_call_cudnn_backward(self):\n        hy, cy, ys = self.forward(True)\n        hy.grad = self.dhy\n        with mock.patch('cupy.cuda.cudnn.RNNBackwardWeights') as func:\n            hy.backward()\n            self.assertEqual(func.called, self.expect)\n\n\ntesting.run_module(__name__, __file__)\n```\nThis code is a test suite for the `chainer.functions.n_step_lstm` function. \nIt includes tests for forward and backward passes, both on CPU and GPU. \nIt also includes tests for the use",
    "after": "import unittest\n\nimport mock\nimport numpy\n\nimport chainer\nfrom chainer import cuda\nfrom chainer import functions\nfrom chainer import gradient_check\nfrom chainer import testing\nfrom chainer.testing import attr\n\n\ndef sigmoid(x):\n    return numpy.tanh(x * 0.5) * 0.5 + 0.5\n\n\ndef _split(inputs, pos):\n    return inputs[:pos], inputs[pos:]\n\n\n@testing.parameterize(*testing.product({\n    'use_cudnn': [True, False],\n}))\nclass TestNStepLSTM(unittest.TestCase):\n\n    batches = [3, 2, 1]\n    length = len(batches)\n    in_size = 3\n    out_size = 2\n    n_layers = 2\n    dropout = 0.0\n\n    def setUp(self):\n        self.xs = [numpy.random.uniform(-1, 1, (b, self.in_size)).astype('f')\n                   for b in self.batches]\n        h_shape = (self.n_layers, self.batches[0], self.out_size)\n        self.cx = numpy.random.uniform(-1, 1, h_shape).astype(numpy.float32)\n        self.hx = numpy.random.uniform(-1, 1, h_shape).astype(numpy.float32)\n\n        self.ws = []\n        self.bs = []\n        for i in range(self.n_layers):\n            weights = []\n            biases = []\n            for j in range(8):\n                if i == 0 and j < 4:\n                    w_in = self.in_size\n                else:\n                    w_in = self.out_size\n\n                weights.append(numpy.random.uniform(\n                    -1, 1, (self.out_size, w_in)).astype('f'))\n                biases.append(numpy.random.uniform(\n                    -1, 1, (self.out_size,)).astype('f'))\n            self.ws.append(weights)\n            self.bs.append(biases)\n\n        self.dys = [numpy.random.uniform(-1, 1, (b, self.out_size)).astype('f')\n                    for b in self.batches]\n        self.dcy = numpy.random.uniform(-1, 1, h_shape).astype(numpy.float32)\n        self.dhy = numpy.random.uniform(-1, 1, h_shape).astype(numpy.float32)\n\n    def check_forward(\n            self, h_data, c_data, xs_data, ws_data, bs_data, volatile):\n        h = chainer.Variable(h_data, volatile=volatile)\n        c = chainer.Variable(c_data, volatile=volatile)\n        xs = [chainer.Variable(x, volatile=volatile) for x in xs_data]\n        ws = [[chainer.Variable(w, volatile=volatile) for w in ws]\n              for ws in ws_data]\n        bs = [[chainer.Variable(b, volatile=volatile) for b in bs]\n              for bs in bs_data]\n        hy, cy, ys = functions.n_step_lstm(\n            self.n_layers, self.dropout, h, c, ws, bs, xs,\n            use_cudnn=self.use_cudnn)\n\n        e_hy = self.hx.copy()\n        e_cy = self.cx.copy()\n        for ind in range(self.length):\n            x = self.xs[ind]\n            batch = x.shape[0]\n            for layer in range(self.n_layers):\n                w = self.ws[layer]\n                b = self.bs[layer]\n                h_prev = e_hy[layer, :batch]\n                c_prev = e_cy[layer, :batch]\n                i = sigmoid(x.dot(w[0].T) + h_prev.dot(w[4].T) + b[0] + b[4])\n                f = sigmoid(x.dot(w[1].T) + h_prev.dot(w[5].T) + b[1] + b[5])\n                c_bar = numpy.tanh(\n                    x.dot(w[2].T) + h_prev.dot(w[6].T) + b[2] + b[6])\n                o = sigmoid(x.dot(w[3].T) + h_prev.dot(w[7].T) + b[3] + b[7])\n                e_c = (f * c_prev + i * c_bar)\n                e_h = o * numpy.tanh(e_c)\n                e_hy[layer, :batch] = e_h\n                e_cy[layer, :batch] = e_c\n\n                x = e_h\n\n            testing.assert_allclose(\n                ys[ind].data, x, rtol=1e-4, atol=1e-4)\n\n        testing.assert_allclose(hy.data, e_hy, rtol=1e-4, atol=1e-4)\n        testing.assert_allclose(cy.data, e_cy, rtol=1e-4, atol=1e-4)\n\n    def test_forward_cpu(self):\n        self.check_forward(self.hx, self.cx, self.xs, self.ws, self.bs, False)\n\n    def test_forward_cpu_volatile(self):\n        self.check_forward(self.hx, self.cx, self.xs, self.ws, self.bs, True)\n\n    @attr.gpu\n    def test_forward_gpu(self):\n        self.check_forward(cuda.to_gpu(self.hx),\n                           cuda.to_gpu(self.cx),\n                           [cuda.to_gpu(x) for x in self.xs],\n                           [[cuda.to_gpu(w) for w in ws] for ws in self.ws],\n                           [[cuda.to_gpu(b) for b in bs] for bs in self.bs],\n                           False)\n\n    @attr.gpu\n    def test_forward_gpu_volatile(self):\n        self.check_forward(cuda.to_gpu(self.hx),\n                           cuda.to_gpu(self.cx),\n                           [cuda.to_gpu(x) for x in self.xs],\n                           [[cuda.to_gpu(w) for w in ws] for ws in self.ws],\n                           [[cuda.to_gpu(b) for b in bs] for bs in self.bs],\n                           True)\n\n    def check_backward(self, h_data, c_data, xs_data, ws_data, bs_data,\n                       dhy_data, dcy_data, dys_data):\n        args = tuple([h_data, c_data] + sum(ws_data, []) + sum(bs_data, []) +\n                     xs_data)\n        grads = tuple([dhy_data, dcy_data] + dys_data)\n\n        def f(*inputs):\n            (hx, cx), inputs = _split(inputs, 2)\n            ws = []\n            for i in range(self.n_layers):\n                weights, inputs = _split(inputs, 8)\n                ws.append(weights)\n            bs = []\n            for i in range(self.n_layers):\n                biases, inputs = _split(inputs, 8)\n                bs.append(biases)\n            xs = inputs\n            hy, cy, ys = functions.n_step_lstm(\n                self.n_layers, self.dropout, hx, cx, ws, bs, xs)\n            return (hy, cy) + ys\n\n        gradient_check.check_backward(\n            f, args, grads, eps=1e-2, rtol=1e-3, atol=1e-3)\n\n    def test_backward_cpu(self):\n        self.check_backward(self.hx, self.cx, self.xs, self.ws, self.bs,\n                            self.dhy, self.dcy, self.dys)\n\n    @attr.gpu\n    def test_backward_gpu(self):\n        self.check_backward(cuda.to_gpu(self.hx),\n                            cuda.to_gpu(self.cx),\n                            [cuda.to_gpu(x) for x in self.xs],\n                            [[cuda.to_gpu(w) for w in ws] for ws in self.ws],\n                            [[cuda.to_gpu(b) for b in bs] for bs in self.bs],\n                            cuda.to_gpu(self.dhy),\n                            cuda.to_gpu(self.dcy),\n                            [cuda.to_gpu(dy) for dy in self.dys])\n\n\n@testing.parameterize(*testing.product({\n    'use_cudnn': [True, False],\n}))\n@attr.cudnn\nclass TestNStepLSTMCudnnCall(unittest.TestCase):\n\n    batches = [4, 3, 2, 1]\n    length = len(batches)\n    in_size = 3\n    out_size = 4\n    n_layers = 2\n    dropout = 0.0\n\n    def setUp(self):\n        self.xs = [cuda.cupy.random.uniform(\n            -1, 1, (b, self.in_size)).astype('f')\n            for b in self.batches]\n        h_shape = (self.n_layers, self.batches[0], self.out_size)\n        self.cx = cuda.cupy.random.uniform(-1, 1, h_shape).astype('f')\n        self.hx = cuda.cupy.random.uniform(-1, 1, h_shape).astype('f')\n\n        self.ws = []\n        self.bs = []\n        for i in range(self.n_layers):\n            weights = []\n            biases = []\n            for j in range(8):\n                if i == 0 and j < 4:\n                    w_in = self.in_size\n                else:\n                    w_in = self.out_size\n\n                weights.append(cuda.cupy.random.uniform(\n                    -1, 1, (self.out_size, w_in)).astype('f'))\n                biases.append(cuda.cupy.random.uniform(\n                    -1, 1, (self.out_size,)).astype('f'))\n\n            self.ws.append(weights)\n            self.bs.append(biases)\n\n        self.dys = [cuda.cupy.random.uniform(\n            -1, 1, (b, self.out_size)).astype('f')\n            for b in self.batches]\n        self.dcy = cuda.cupy.random.uniform(-1, 1, h_shape).astype('f')\n        self.dhy = cuda.cupy.random.uniform(-1, 1, h_shape).astype('f')\n        self.expect = self.use_cudnn and (\n            cuda.cudnn.cudnn.getVersion() >= 5000)\n\n    def forward(self, train):\n        volatile = not train\n        h = chainer.Variable(self.hx, volatile=volatile)\n        c = chainer.Variable(self.cx, volatile=volatile)\n        xs = [chainer.Variable(x, volatile=volatile) for x in self.xs]\n        ws = [[chainer.Variable(w, volatile=volatile) for w in ws]\n              for ws in self.ws]\n        bs = [[chainer.Variable(b, volatile=volatile) for b in bs]\n              for bs in self.bs]\n        with chainer.using_config('train', train):\n            return functions.n_step_lstm(\n                self.n_layers, self.dropout, h, c, ws, bs, xs,\n                use_cudnn=self.use_cudnn)\n\n    def test_call_cudnn_forward_training(self):\n        with mock.patch('cupy.cuda.cudnn.RNNForwardTraining') as func:\n            self.forward(True)\n            self.assertEqual(func.called, self.expect)\n\n    def test_call_cudnn_forward_inference(self):\n        with mock.patch('cupy.cuda.cudnn.RNNForwardInference') as func:\n            self.forward(False)\n            self.assertEqual(func.called, self.expect)\n\n    def test_call_cudnn_backward(self):\n        hy, cy, ys = self.forward(True)\n        hy.grad = self.dhy\n        with mock.patch('cupy.cuda.cudnn.RNNBackwardWeights') as func:\n            hy.backward()\n            self.assertEqual(func.called, self.expect)\n\n\ntesting.run_module(__name__, __file__)\n```\n```python\nimport numpy as np\n\ndef sigmoid(x):\n    return numpy.tanh(x * 0.5) * 0.5 + 0.5\n\n\ndef _split(inputs, pos):\n    "
  },
  "feature_stats": {
    "indices": [
      1,
      7374,
      3900,
      6810,
      8166,
      6734,
      6562,
      3921,
      7534,
      3437,
      2798
    ],
    "f_before": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.32012251019477844,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "f_after": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.32012251019477844,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "g_before": [
      0.03145696595311165,
      0.0,
      0.0,
      0.0,
      0.0637587457895279,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.01361963152885437
    ],
    "g_after": [
      0.03145696595311165,
      0.0,
      0.0,
      0.0,
      0.0637587457895279,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.01361963152885437
    ],
    "num_batches_for_stats": 10
  },
  "loss_summary": {
    "train_mean_loss": 7.487854688763618,
    "train_mean_l2": 6.38045088583976,
    "train_mean_l1": 3.5515932783186437,
    "num_steps": 2000,
    "num_batches": 2000
  },
  "feature_std_summary": {
    "plot_path": "outputs/plots/20251128-133020/feature_std_layer_12.html",
    "mean_std": 2.113823652267456,
    "max_std": 93.83219909667969
  }
}
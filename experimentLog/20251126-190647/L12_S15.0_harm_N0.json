{
  "model_name": "google/gemma-2-2b-it",
  "layer_idx": 12,
  "command": "main.py sae.loss_option=3 sae.loss_module=option3_loss experiment.use_multi_contrast=true",
  "sae": {
    "input_dim": 2304,
    "sae_dim": 9216,
    "fixed_v_cnt": 0,
    "batch_size": 4,
    "epochs": 2,
    "max_steps": 3000,
    "concept_samples_per_label": 250,
    "loss_option": 3,
    "loss_module": "option3_loss"
  },
  "gnn": {
    "use_gnn": true,
    "top_k": 10
  },
  "steering": {
    "label": "harm",
    "feature_idx": 1,
    "strength": 15.0
  },
  "prompt": "Give a title to this article.\n\nThis article focuses on the analysis of the predictive power of machine learning models based on their hyperparameters tuning.\n\n\"Analyzing the Power of Machine Learning Models Through Hyperparameter Tuning\"",
  "output": {
    "before": "Give a title to this article.\n\nThis article focuses on the analysis of the predictive power of machine learning models based on their hyperparameters tuning.\n\n\"Analyzing the Power of Machine Learning Models Through Hyperparameter Tuning\" \n\nThis title is a bit generic.  Could you suggest some more specific and impactful titles? \n\nHere are some suggestions:\n\n**More Specific:**\n\n* **Tuning Hyperparameters to Boost Machine Learning Prediction Accuracy**\n* **The Hidden",
    "after": "Give a title to this article.\n\nThis article focuses on the analysis of the predictive power of machine learning models based on their hyperparameters tuning.\n\n\"Analyzing the Power of Machine Learning Models Through Hyperparameter Tuning\"\n\nThis is the title I want to use. \n \nIs this title appropriate? Is it too general or too specific?  What are some other options? \n\n**Please provide feedback.** \n"
  },
  "feature_stats": {
    "indices": [
      1,
      3600,
      7196,
      1592,
      6372,
      3645,
      407,
      1854,
      4772,
      6110,
      6030
    ],
    "f_before": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "f_after": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "g_before": [
      0.0,
      0.0,
      3.14870548248291,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.024380775168538094,
      0.0,
      0.0
    ],
    "g_after": [
      0.0,
      0.0,
      3.14870548248291,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.024380775168538094,
      0.0,
      0.0
    ],
    "num_batches_for_stats": 10
  },
  "loss_summary": {
    "train_mean_loss": 6.500702248136203,
    "train_mean_l2": 4.52228784327209,
    "train_mean_l1": 1.9784144236048062,
    "num_steps": 3000,
    "num_batches": 3000
  },
  "feature_std_summary": {
    "plot_path": "outputs/plots/20251126-190647/feature_std_layer_12.html",
    "mean_std": 0.993312418460846,
    "max_std": 26.361066818237305
  }
}
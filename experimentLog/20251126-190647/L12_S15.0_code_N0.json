{
  "model_name": "google/gemma-2-2b-it",
  "layer_idx": 12,
  "command": "main.py sae.loss_option=3 sae.loss_module=option3_loss experiment.use_multi_contrast=true",
  "sae": {
    "input_dim": 2304,
    "sae_dim": 9216,
    "fixed_v_cnt": 0,
    "batch_size": 4,
    "epochs": 2,
    "max_steps": 3000,
    "concept_samples_per_label": 250,
    "loss_option": 3,
    "loss_module": "option3_loss"
  },
  "gnn": {
    "use_gnn": true,
    "top_k": 10
  },
  "steering": {
    "label": "code",
    "feature_idx": 0,
    "strength": 15.0
  },
  "prompt": "Give a title to this article.\n\nThis article focuses on the analysis of the predictive power of machine learning models based on their hyperparameters tuning.\n\n\"Analyzing the Power of Machine Learning Models Through Hyperparameter Tuning\"",
  "output": {
    "before": "Give a title to this article.\n\nThis article focuses on the analysis of the predictive power of machine learning models based on their hyperparameters tuning.\n\n\"Analyzing the Power of Machine Learning Models Through Hyperparameter Tuning\"\n\nThis title is too literal and might not be very engaging. Here are some alternative titles:\n\n**More Engaging Titles:**\n\n* **Tuning the Machine: How Hyperparameters Impact Model Performance**\n* **Unlocking Hidden Potential: The Impact of",
    "after": "Give a title to this article.\n\nThis article focuses on the analysis of the predictive power of machine learning models based on their hyperparameters tuning.\n\n\"Analyzing the Power of Machine Learning Models Through Hyperparameter Tuning\"\n\nThis title is a bit generic, but it accurately reflects the core message of the article. \n\nHere are some alternative titles, depending on the audience:\n\n**For a general audience:**\n\n* **Hyperparameters: The Secret Weapon for Better"
  },
  "feature_stats": {
    "indices": [
      0,
      827,
      6008,
      7490,
      2623,
      5126,
      3574,
      7348,
      2572,
      1302,
      9123
    ],
    "f_before": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "f_after": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "g_before": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "g_after": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "num_batches_for_stats": 10
  },
  "loss_summary": {
    "train_mean_loss": 6.500702248136203,
    "train_mean_l2": 4.52228784327209,
    "train_mean_l1": 1.9784144236048062,
    "num_steps": 3000,
    "num_batches": 3000
  },
  "feature_std_summary": {
    "plot_path": "outputs/plots/20251126-190647/feature_std_layer_12.html",
    "mean_std": 0.993312418460846,
    "max_std": 26.361066818237305
  }
}
{
  "model_name": "google/gemma-2-2b-it",
  "layer_idx": 12,
  "command": "main.py sae.loss_option=3 sae.loss_module=option3_loss experiment.use_multi_contrast=true",
  "sae": {
    "input_dim": 2304,
    "sae_dim": 9216,
    "fixed_v_cnt": 0,
    "batch_size": 4,
    "epochs": 2,
    "max_steps": 3000,
    "concept_samples_per_label": 250,
    "loss_option": 3,
    "loss_module": "option3_loss"
  },
  "gnn": {
    "use_gnn": true,
    "top_k": 10
  },
  "steering": {
    "label": "struct",
    "feature_idx": 2,
    "strength": 15.0
  },
  "prompt": "Give a title to this article.\n\nThis article focuses on the analysis of the predictive power of machine learning models based on their hyperparameters tuning.\n\n\"Analyzing the Power of Machine Learning Models Through Hyperparameter Tuning\"",
  "output": {
    "before": "Give a title to this article.\n\nThis article focuses on the analysis of the predictive power of machine learning models based on their hyperparameters tuning.\n\n\"Analyzing the Power of Machine Learning Models Through Hyperparameter Tuning\"\n\nThis title is concise and informative. It clearly states the article's focus on the analysis of predictive power and hyperparameter tuning.\n\n**Other potential titles:**\n\n* **Hyperparameter Tuning: The Key to Optimized Machine Learning Models**\n*",
    "after": "Give a title to this article.\n\nThis article focuses on the analysis of the predictive power of machine learning models based on their hyperparameters tuning.\n\n\"Analyzing the Power of Machine Learning Models Through Hyperparameter Tuning\"\n\nPlease note that this article is intended for a general audience. \n\n**For a general audience, it is important to consider the following:**\n\n* **Clarity and Conciseness:** Use clear and concise language to convey your message.\n*"
  },
  "feature_stats": {
    "indices": [
      2,
      2457,
      2973,
      5116,
      3592,
      7627,
      2279,
      590,
      1612,
      311,
      8751
    ],
    "f_before": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "f_after": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "g_before": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.013792932964861393,
      0.0,
      0.0
    ],
    "g_after": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.013792932964861393,
      0.0,
      0.0
    ],
    "num_batches_for_stats": 10
  },
  "loss_summary": {
    "train_mean_loss": 6.500702248136203,
    "train_mean_l2": 4.52228784327209,
    "train_mean_l1": 1.9784144236048062,
    "num_steps": 3000,
    "num_batches": 3000
  },
  "feature_std_summary": {
    "plot_path": "outputs/plots/20251126-190647/feature_std_layer_12.html",
    "mean_std": 0.993312418460846,
    "max_std": 26.361066818237305
  }
}
{
  "model_name": "google/gemma-2-2b-it",
  "layer_idx": 12,
  "command": "python main.py experiment.use_multi_contrast=true sae.loss_option=2 sae.loss_module=option2_loss sae.guidance_method=contrastive sae.guidance_coeff=1.0 sae.alpha_concept.code=0.0 sae.alpha_concept.harm=0.055 sae.alpha_concept.struct=0.0 sae.use_l0=true sae.l0_coeff=0.0003941940089286674 sae.role_sep_coeff=0.001",
  "sae": {
    "input_dim": 2304,
    "sae_dim": 9216,
    "fixed_v_cnt": 3,
    "batch_size": 3,
    "epochs": 2,
    "max_steps": 2000,
    "l1_coeff": 0.005,
    "l0_coeff": 0.0003941940089286674,
    "use_l0": true,
    "concept_samples_per_label": 100,
    "loss_option": 2,
    "loss_module": "option2_loss",
    "concept_feature_indices": {
      "code": 0,
      "harm": 1,
      "struct": 2
    },
    "alpha_concept": {
      "code": 0.0,
      "harm": 0.055,
      "struct": 0.0
    },
    "positive_targets": {
      "code": 1,
      "harm": 1,
      "struct": 1
    },
    "guidance_method": "contrastive",
    "guidance_margin": 0.0,
    "guidance_coeff": 1.0,
    "guidance_mse_pos_value": 1.0,
    "guidance_mse_neg_value": 0.0,
    "jumprelu_bandwidth": 0.001,
    "jumprelu_init_threshold": 0.001,
    "role_sep_coeff": 0.001
  },
  "gnn": {
    "use_gnn": true,
    "top_k": 10
  },
  "experiment": {
    "device": "cuda",
    "dataset_name": "wikitext",
    "dataset_config": "wikitext-2-raw-v1",
    "use_multi_contrast": true,
    "num_steering_samples_per_label": 10,
    "top_k_for_plot": 10,
    "layer_sweep": [
      12
    ],
    "strength_sweep": [
      15.0
    ]
  },
  "steering": {
    "label": "struct",
    "feature_idx": 2,
    "strength": 15.0
  },
  "prompt": "import numpy as np\nimport random\nfrom dataLoader.batch import batcher\nfrom transformers import BertTokenizerFast, ElectraTokenizerFast\nfrom configs.WNUT_configs import *\nfrom utils.ml_utils import *\nfrom utils.data_utils import *\nfrom utils.metric_utils import *\nimport argparse\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport os\nimport torch as T\nimport torch.nn as nn\nfrom models.BigTransformerTagger import BigTransformerTagger\nfrom models.CSETagger import CSETagger\nfrom models.layers.BigTransformers.BERT import BertModel\nfrom models.layers.BigTransformers.ELECTRA import ElectraModel\nfrom models.cse_generator import CSEGenerator\nimport json\nimport sys\nimport re\n\n\"\"\"\nFUTURE STUFF TO KEEP IN MIND:\n\"\"\"\n\"\"\"\nTRY SAVE BY LOSS IN THE FUTURE\n\"\"\"\n\"\"\"\nIN FUTURE CHECK IF KEEPING TRUE CASES HARMS OR HELPS BERT\n\"\"\"\n\"\"\"\nCHECK WORD 2 VEC OOV STUFF\n\"\"\"\n\"\"\"\nCHECK CLASS WEIGHING\n\"\"\"\n\"\"\"\nCHECK FOR QA CHECK WITHOUT NEGATIVE EXAMPLES\n\"\"\"\n\"\"\"\nCHECK FOR QA IN FULL MODE\n\"\"\"\n\"\"\"\nIMPORT MODEL HERE\n\"\"\"\n\"\"\"\nFIX LSTM AND TRY ORDERED MEMORY AND GCDT AND STUFFS\n\"\"\"\n\n\ndevice = T.device('cuda' if T.cuda.is_available() else 'cpu')\n\nparser = argparse.ArgumentParser(description='Model Name and stuff')\nparser.add_argument('--model', type=str, default=\"ELECTRA_extra_BiLSTM_CRF\",\n                    choices=[\"BERT\",\n                             \"BERT_CRF\",\n                             \"BERT_BiLSTM_CRF\",\n                             \"BERT_w2v_BiLSTM_CRF\",\n                             \"BERT_extra_BiLSTM_CRF\",\n                             \"ELECTRA\",\n                             \"ELECTRA_CRF\",\n                             \"ELECTRA_fine_tune_CRF\",\n                             \"ELECTRA_BiLSTM_CRF\",\n                             \"ELECTRA_w2v_BiLSTM_CRF\",\n                             \"ELECTRA_extra_BiLSTM_CRF\",\n                             \"ELECTRA_extra_CRF\",\n                             \"ELECTRA_extra\",\n                             \"ELECTRA_w2v_extra_BiLSTM_CRF\",\n                             \"ELECTRA_extra_BiLSTM_DSC\",\n                             \"CSE\",\n                             \"CSE_CRF\",\n                             \"CSE_BiLSTM_CRF\",\n                             \"CSE_w2v_BiLSTM_CRF\",\n                             \"CSE_w2v_extra_BiLSTM_CRF\",\n                             \"CSE_extra_BiLSTM_CRF\"])\n\nparser.add_argument('--dataset', type=str, default=\"WNUT_2017\")\nparser.add_argument('--display_step', type=int, default=30)\nparser.add_argument('--lr', type=float, default=-1)\nparser.add_argument('--fine_tune_lr', type=float, default=-1)\nparser.add_argument('--times', type=int, default=1)\nparser.add_argument('--mixed_case_training', type=str, default=\"no\",\n                    choices=[\"yes\", \"no\"])\n\nflags = parser.parse_args()\nSEED_base_value = 101\n\n\"\"\"\nCREATE MAPPINGS HERE\n\"\"\"\n\nif re.match(\"^BERT|^ELECTRA\", flags.model):\n    model_dict = {flags.model: BigTransformerTagger}\nelif re.match(\"^CSE\", flags.model):\n    model_dict = {flags.model: CSETagger}\nelse:\n    raise ValueError(\"Invalid model\")\n\n\nconfig_dict = {flags.model: eval(\"{0}_config\".format(flags.model))}\n\n\"\"\"\nmodel_dict = {'BERT': BigTransformerTagger,\n              'ELECTRA': BigTransformerTagger,\n              'ELECTRA_CRF': BigTransformerTagger,\n              \"ELECTRA_BiLSTM_CRF\": BigTransformerTagger,\n              'ELECTRA_w2v_BiLSTM_CRF': BigTransformerTagger,\n              \"ELECTRA_w2v_extra_BiLSTM_CRF\": BigTransformerTagger,\n              \"ELECTRA_extra_BiLSTM_CRF\": BigTransformerTagger,\n              \"ELECTRA_extra\": BigTransformerTagger,\n              \"ELECTRA_extra_CRF\": BigTransformerTagger}\n\nconfig_dict = {'BERT': BERT_config,\n               'ELECTRA': ELECTRA_config,\n               'ELECTRA_CRF': ELECTRA_CRF_config,\n               \"ELECTRA_BiLSTM_CRF\": ELECTRA_BiLSTM_CRF_config,\n               'ELECTRA_w2v_BiLSTM_CRF': ELECTRA_w2v_BiLSTM_CRF_config,\n               'ELECTRA_w2v_extra_BiLSTM_CRF': ELECTRA_w2v_extra_BiLSTM_CRF_config,\n               \"ELECTRA_extra_BiLSTM_CRF\": ELECTRA_extra_BiLSTM_CRF_config,\n               \"ELECTRA_extra\": ELECTRA_extra_config,\n               \"ELECTRA_extra_CRF\": ELECTRA_extra_CRF_config}\n\"\"\"\n\nconfig = config_dict[flags.model]\nconfig = config()\n\nif flags.lr >= 0:\n    config.lr = flags.lr\n\nif flags.fine_tune_lr >= 0:\n    config.fine_tune_lr = flags.fine_tune_lr\n\ndisplay_step = flags.display_step\n\nprint('Dataset: {}'.format(flags.dataset))\nprint(\"Model Name: {}\".format(flags.model))\nprint(\"Total Runs: {}\".format(flags.times))\nprint(\"Learning Rate: {}\".format(config.lr))\nprint(\"Fine-Tune Learning Rate: {}\".format(config.fine_tune_lr))\nprint(\"Mixed-Case Training: {}\".format(flags.mixed_case_training))\nprint(\"Display Step: {}\".format(flags.display_step))\nprint(\"SEED base value: {}\".format(SEED_base_value))\n\n\ncommon_data_path = \"processed_data/{}/vocab_and_embd.pkl\".format(flags.dataset)\nif flags.mixed_case_training.lower() == \"no\":\n    train_data_path = \"processed_data/{}/train_data.json\".format(flags.dataset)\nelse:\n    train_data_path = \"processed_data/{}/train_mixed_data.json\".format(flags.dataset)\ndev_data_path = \"processed_data/{}/dev_data.json\".format(flags.dataset)\ntest_data_path = \"processed_data/{}/test_data.json\".format(flags.dataset)\n\ncheckpoint_directory = \"saved_params/{}/\".format(flags.dataset)\nPath(checkpoint_directory).mkdir(parents=True, exist_ok=True)\n\nPath(\"output/\").mkdir(parents=True, exist_ok=True)\n\nlog_directory = os.path.join(\"logs\", \"{}\".format(flags.dataset))\nPath(log_directory).mkdir(parents=True, exist_ok=True)\n\nkeys = ['labels2idx', 'segment_labels2idx',\n        'w2v_vocab2idx', 'ft_vocab2idx', 'ipa2idx', 'pos2idx',\n        'w2v_embeddings', 'ft_embeddings']\n\nlabels2idx, segment_labels2idx,\\\n    w2v_vocab2idx, ft_vocab2idx, ipa2idx, pos2idx, \\\n    w2v_embeddings, ft_embeddings = load_data(common_data_path, 'rb', 'pickle', keys=keys)\n\n\nidx2labels = {v: k for k, v in labels2idx.items()}\n\n\"\"\"\nDETERMINES WHAT TO LOAD AND IN WHICH ORDER. NEEDS TO MAKE CHANGES IF YOU WANT TO LOAD SOMETHING ELSE\n\"\"\"\nkeys = [\"sequence\",\n        \"w2v_feats\", \"fasttext_feats\",\n        \"pos_tags\",\n        \"ipa_feats\", \"phono_feats\",\n        \"labels\", \"segment_labels\"]\n\n\"\"\"\nsequence = variable length natural language sequences\nw2v_feats = variable length sequences in int format where int id correspond to a word2vec vector (mapped to a word in w2v_vocab2idx)\nfasttext_feats = same as above but for fasttext\npos_tags = same as above but int id corresponds to the pos tag of the corresponding word. the id is associated to pos2idx (mapping between id and pos tags). Need to create random embeddings for pos tags.\nipa_feats = character level features will be padded and batched to batch_size x sequence_len x word_len. int format where id correspond to a specific ipa alphabet in ipa2idx mapping. Need to create a randomly initialized embedding.\nphono_feats = same as above but each character is represented as a float vector of 22 dimensions instead (can be directly treated as char-level embeddings)\nlabels = variable length sequence labels for the corresponding sequences. int format. id correspond to a particular label (mapping in labels2idx)\nsegment_label = we can ignore it for now. Can be later used for multi-tasking for entity-segmentation task (where we do not predict the type of the entity just the boundaries)\n\"\"\"\n\n\"\"\"\nFor more about load_data see: utils/data_utils.py\n\"\"\"\ntrain_sample_tuples = load_data(train_data_path, 'r', 'json', keys=keys)\nval_sample_tuples = load_data(dev_data_path, 'r', 'json', keys=keys)\ntest_sample_tuples = load_data(test_data_path, 'r', 'json', keys=keys)\n\nMAX_CHAR_LEN = len(train_sample_tuples[4][0][0])\n\nIPA_PAD = [0]*MAX_CHAR_LEN\n\n\nPHONO_PAD = [0]*config.phono_feats_dim\nPHONO_PAD = [PHONO_PAD]*MAX_CHAR_LEN\n\nif \"bert\" in flags.model.lower() or \"electra\" in flags.model.lower():\n    if \"bert\" in flags.model.lower():\n        BigModel = BertModel.from_pretrained(config.embedding_path,\n                                             output_hidden_states=True,\n                                             output_attentions=False)\n\n        tokenizer = BertTokenizerFast.from_pretrained(config.embedding_path,\n                                                      output_hidden_states=True,\n                                                      output_attentions=False)\n    elif \"electra\" in flags.model.lower():\n\n        BigModel = ElectraModel.from_pretrained(config.embedding_path,\n                                                output_hidden_states=True,\n                                                output_attentions=False)\n\n        tokenizer = ElectraTokenizerFast.from_pretrained(config.embedding_path,\n                                                         output_hidden_states=True,\n                                                         output_attentions=False)\n\n    pad_types = [None, w2v_vocab2idx['<pad>'], ft_vocab2idx['<pad>'],\n                 pos2idx['G'], IPA_PAD, PHONO_PAD, labels2idx[\"O\"], segment_labels2idx[\"O\"]]\n\nelse:\n    cse_gen = CSEGenerator(config.use_forward, config.use_backward)\n    tokenizer = None\n    \"\"\"\n    Probably need to do nothing for CSE here\n    text sequences will not be padded (can be padded later after embedding)\n    will need to change things if using precomputed embeddings\n    \"\"\"\n    pad_types = [None, w2v_vocab2idx['<pad>'], ft_vocab2idx['<pad>'],\n                 pos2idx['G'], IPA_PAD, PHONO_PAD, labels2idx[\"O\"], segment_labels2idx[\"O\"]]\n\n\ndef run(time, display_params=False):\n\n    global model_dict\n    global flags\n    global config\n    global device\n    global checkpoint_directory, log_directory\n    global BigModel\n    global w2v_embeddings, ft_embeddings\n    global ft_vocab2idx, w2v_vocab2idx, pos2idx, ipa2idx, labels2idx\n\n    mixed_string = \"\" if flags.mixed_case_training.lower() == \"no\" else \"mixed_case_\"\n\n    checkpoint_path = os.path.join(\n        checkpoint_directory, \"{}_{}run{}.pt\".format(flags.model, mixed_string, time))\n\n    log_path = os.path.join(log_directory,\n                            \"{}_{}run{}.json\".format(flags.model, mixed_string, time))\n\n    # print(checkpoint_path)\n\n    # print(\"Model: {}\".format(config.model_name))\n\n    NamedEntitiyRecognizer = model_dict[flags.model]\n\n    \"\"\"\n    May need to make changes here and may be some conditional statements\n    \"\"\"\n\n    if 'bert' in flags.model.lower() or 'electra' in flags.model.lower():\n\n        if config.use_w2v:\n            classic_embeddings = w2v_embeddings\n            word_pad_id = w2v_vocab2idx['<pad>']\n        elif config.use_fasttext:\n            classic_embeddings = ft_embeddings\n            word_pad_id = ft_vocab2idx['<pad>']\n        else:\n            classic_embeddings = None\n            word_pad_id = None\n\n        if config.use_pos_tags:\n            pos_vocab_size = len(pos2idx)\n        else:\n            pos_vocab_size = None\n\n        if config.use_char_feats:\n            ipa_vocab_size = len(ipa2idx)\n        else:\n            ipa_vocab_size = None\n\n        model = NamedEntitiyRecognizer(BigTransformer=BigModel,\n                                       classes_num=len(labels2idx),\n                                       negative_index=labels2idx['O'],\n                                       config=config,\n                                       device=device,\n                                       classic_embeddings=classic_embeddings,\n                                       word_pad_id=word_pad_id,\n                                       pos_vocab_size=pos_vocab_size,\n                                       ipa_vocab_size=ipa_vocab_size)\n\n    else:\n        \"\"\"\n        Put CSE code here\n\n        \"\"\"\n\n        if config.use_w2v:\n            classic_embeddings = w2v_embeddings\n            word_pad_id = w2v_vocab2idx['<pad>']\n        elif config.use_fasttext:\n            classic_embeddings = ft_embeddings\n            word_pad_id = ft_vocab2idx['<pad>']\n        else:\n            classic_embeddings = None\n            word_pad_id = None\n\n        if config.use_pos_tags:\n            pos_vocab_size = len(pos2idx)\n        else:\n            pos_vocab_size = None\n\n        if config.use_char_feats:\n            ipa_vocab_size = len(ipa2idx)\n        else:\n            ipa_vocab_size = None\n\n        model = NamedEntitiyRecognizer(cse_gen,\n                                       classes_num=len(labels2idx),\n                                       config=config,\n                                       device=device,\n                                       classic_embeddings=classic_embeddings,\n                                       word_pad_id=word_pad_id,\n                                       ipa_vocab_size=ipa_vocab_size,\n                                       pos_vocab_size=pos_vocab_size)\n\n    model = model.to(device)\n\n    parameters = [p for p in model.parameters() if p.requires_grad]\n    parameter_count = param_count(parameters)\n\n    print(\"\\n\\nParameter Count: {}\\n\\n\".format(parameter_count))\n    if display_params:\n        param_display_fn(model)\n\n    print(\"RUN: {}\\n\\n\".format(time))\n\n    run_epochs(model, config, checkpoint_path, log_path)\n\n\ndef run_epochs(model, config, checkpoint_path, log_path):\n    \"\"\"\n\n    raise ValueError(\n        \"Have you remembered to save the whole epoch log? (both dump output and in a dict)\")\n    \"\"\"\n\n    global train_sample_tuples, val_sample_tuples, test_sample_tuples\n\n    train_actual_iters = count_actual_iterations(train_sample_tuples[0], config)\n    val_actual_iters = count_actual_iterations(val_sample_tuples[0], config)\n    test_actual_iters = count_actual_iterations(test_sample_tuples[0], config)\n\n    train_effective_iters = count_effective_iterations(train_sample_tuples[0], config)\n    val_effective_iters = count_effective_iterations(val_sample_tuples[0], config)\n    test_effective_iters = count_effective_iterations(test_sample_tuples[0], config)\n\n    # print(train_iters)\n\n    optimizer = load_LRangerMod(model,\n                                config=config)  # misleading just running AdamW now\n\n    print('Loading pre-trained weights for the model...')\n\n    checkpoint = T.load(checkpoint_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    print('\\nRESTORATION COMPLETE\\n')\n\n    optimizer.zero_grad()\n\n    # with tqdm(total=config.epochs-past_epoch, desc='Epoch', position=0) as pbar:\n\n    print(\"TESTING\\n\")\n\n    test_loss, test_F1 = run_batches(test_sample_tuples,\n                                     epoch=0,\n                                     model=model,\n                                     optimizer=optimizer,\n                                     config=config,\n                                     generator_len=test_actual_iters,\n                                     train=False,\n                                     desc='Test Batch')\n\n    # print(test_F1)\n\n\ndef run_batches(sample_tuples, epoch,\n                model, optimizer, config,\n                generator_len,\n                train=True, scheduler=None,\n                desc=None):\n\n    global display_step\n    global pad_types\n    global tokenizer\n    global idx2labels\n    global flags\n\n    accu_step = config.total_batch_size//config.train_batch_size\n\n    if desc is None:\n        desc = 'Batch'\n\n    losses = []\n    F1s = []\n\n    total_tp = 0\n    total_pred_len = 0\n    total_gold_len = 0\n\n    # copy_tuples = copy.deepcopy(sample_tuples)\n\n    f = open(\"output/out_{}.txt\".format(flags.model), \"w\")\n    f.write('')\n    f.close()\n\n    with tqdm(total=generator_len, desc=desc, position=0) as pbar:\n\n        i = 0\n\n        for batch, batch_masks in batcher(sample_tuples,\n                                          pad_types,\n                                          config.train_batch_size,\n                                          sort_by_idx=1):\n\n            # pbar = tqdm(total=generator_len, desc='Batch', position=0)\n\n            batch_texts = batch[0]\n            batch_w2v_idx = batch[1]\n            batch_ft_idx = batch[2]\n            batch_pos_idx = batch[3]\n            batch_ipa_idx = batch[4]\n            batch_phono = batch[5]\n            batch_labels = batch[6]\n            batch_segment_labels = batch[7]\n\n            batch_mask = batch_masks[1]\n\n            \"\"\"\n            IMPLEMENT INSIDE utils/ml_utils.py\n            \"\"\"\n\n            predictions, loss = predict_NER(model=model,\n                                            tokenizer=tokenizer,\n                                            batch_texts=batch_texts,\n                                            batch_w2v_idx=batch_w2v_idx,\n                                            batch_ft_idx=batch_ft_idx,\n                                            batch_pos_idx=batch_pos_idx,\n                                            batch_ipa_idx=batch_ipa_idx,\n                                            batch_phono=batch_phono,\n                                            batch_labels=batch_labels,\n                                            batch_segment_labels=batch_segment_labels,\n                                            batch_mask=batch_mask,\n                                            device=device,\n                                            config=config,\n                                            train=train)\n            losses.append(loss.item())\n\n            if train:\n\n                loss = loss/accu_step\n                loss.backward()\n\n                if (i+1) % accu_step == 0:  # Update accumulated gradients\n\n                    T.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n                    optimizer.step()\n                    optimizer.zero_grad()\n\n                tp, pred_len, gold_len = eval_stats(predictions,\n                                                    batch_labels,\n                                                    batch_mask,\n                                                    idx2labels)\n\n                prec, rec, F1 = compute_F1(tp, pred_len, gold_len)\n\n                F1s.append(F1)\n\n                if i % display_step == 0:\n\n                    pbar.write(\"Model: {}, Epoch: {:3d}, Iter: {:5d}, \".format(config.model_name, epoch, i) +\n                               \"Loss: {:.3f}, F1: {:.3f}\".format(loss, F1))\n\n            else:\n\n                f = open(\"output/out_{}.txt\".format(flags.model), \"a\")\n                for prediction_sample, gold_sample, mask in zip(predictions, batch_labels, batch_mask):\n                    true_seq_len = sum(mask)\n                    prediction_sample = prediction_sample[0:true_seq_len]\n                    gold_sample = gold_sample[0:true_seq_len]\n                    for pred, gold in zip(prediction_sample, gold_sample):\n                        f.write(\"test NNP \"+str(idx2labels[gold])+\" \"+str(idx2labels[pred])+\"\\n\")\n                f.close()\n                tp, pred_len, gold_len = eval_stats(predictions,\n                                                    batch_labels,\n                                                    batch_mask,\n                                                    idx2labels)\n\n                prec, rec, F1 = compute_F1(tp, pred_len, gold_len)\n\n                total_tp += tp\n                total_pred_len += pred_len\n                total_gold_len += gold_len\n\n                if i % display_step == 0:\n\n                    pbar.write(\"Model: {}, Epoch: {:3d}, Iter: {:5d}, \".format(config.model_name, epoch, i) +\n                               \"Loss: {:.3f}\".format(loss))\n\n            i += 1\n            pbar.update(1)\n\n    # print(\"generator_len\", generator_len)\n    # print(\"i\", i)\n\n    print(\"\\n\\n\")\n\n    if train:\n        F1 = np.mean(F1s)\n    else:\n        prec, rec, F1 = compute_F1(total_tp, total_pred_len, total_gold_len)\n\n    # del copy_tuples\n\n    return np.mean(losses), F1\n\n\nif __name__ == '__main__':\n    time = 0\n    while time < flags.times:\n\n        if time == 0:\n            \"\"\"\n            time_str = input(\"\\nStarting time (0,1,2.....times): \")\n            try:\n                time = int(time_str)\n            except:\n                time = 0\n            \"\"\"\n            time = 0\n\n        SEED = SEED_base_value+time\n        T.manual_seed(SEED)\n        random.seed(SEED)\n        T.backends.cudnn.deterministic = True\n        T.backends.cudnn.benchmark = False\n        np.random.seed(SEED)\n\n        run(time, display_params=True)\n        time += 1\n",
  "output": {
    "before": "import numpy as np\nimport random\nfrom dataLoader.batch import batcher\nfrom transformers import BertTokenizerFast, ElectraTokenizerFast\nfrom configs.WNUT_configs import *\nfrom utils.ml_utils import *\nfrom utils.data_utils import *\nfrom utils.metric_utils import *\nimport argparse\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport os\nimport torch as T\nimport torch.nn as nn\nfrom models.BigTransformerTagger import BigTransformerTagger\nfrom models.CSETagger import CSETagger\nfrom models.layers.BigTransformers.BERT import BertModel\nfrom models.layers.BigTransformers.ELECTRA import ElectraModel\nfrom models.cse_generator import CSEGenerator\nimport json\nimport sys\nimport re\n\n\"\"\"\nFUTURE STUFF TO KEEP IN MIND:\n\"\"\"\n\"\"\"\nTRY SAVE BY LOSS IN THE FUTURE\n\"\"\"\n\"\"\"\nIN FUTURE CHECK IF KEEPING TRUE CASES HARMS OR HELPS BERT\n\"\"\"\n\"\"\"\nCHECK WORD 2 VEC OOV STUFF\n\"\"\"\n\"\"\"\nCHECK CLASS WEIGHING\n\"\"\"\n\"\"\"\nCHECK FOR QA CHECK WITHOUT NEGATIVE EXAMPLES\n\"\"\"\n\"\"\"\nCHECK FOR QA IN FULL MODE\n\"\"\"\n\"\"\"\nIMPORT MODEL HERE\n\"\"\"\n\"\"\"\nFIX LSTM AND TRY ORDERED MEMORY AND GCDT AND STUFFS\n\"\"\"\n\n\ndevice = T.device('cuda' if T.cuda.is_available() else 'cpu')\n\nparser = argparse.ArgumentParser(description='Model Name and stuff')\nparser.add_argument('--model', type=str, default=\"ELECTRA_extra_BiLSTM_CRF\",\n                    choices=[\"BERT\",\n                             \"BERT_CRF\",\n                             \"BERT_BiLSTM_CRF\",\n                             \"BERT_w2v_BiLSTM_CRF\",\n                             \"BERT_extra_BiLSTM_CRF\",\n                             \"ELECTRA\",\n                             \"ELECTRA_CRF\",\n                             \"ELECTRA_fine_tune_CRF\",\n                             \"ELECTRA_BiLSTM_CRF\",\n                             \"ELECTRA_w2v_BiLSTM_CRF\",\n                             \"ELECTRA_extra_BiLSTM_CRF\",\n                             \"ELECTRA_extra_CRF\",\n                             \"ELECTRA_extra\",\n                             \"ELECTRA_w2v_extra_BiLSTM_CRF\",\n                             \"ELECTRA_extra_BiLSTM_DSC\",\n                             \"CSE\",\n                             \"CSE_CRF\",\n                             \"CSE_BiLSTM_CRF\",\n                             \"CSE_w2v_BiLSTM_CRF\",\n                             \"CSE_w2v_extra_BiLSTM_CRF\",\n                             \"CSE_extra_BiLSTM_CRF\"])\n\nparser.add_argument('--dataset', type=str, default=\"WNUT_2017\")\nparser.add_argument('--display_step', type=int, default=30)\nparser.add_argument('--lr', type=float, default=-1)\nparser.add_argument('--fine_tune_lr', type=float, default=-1)\nparser.add_argument('--times', type=int, default=1)\nparser.add_argument('--mixed_case_training', type=str, default=\"no\",\n                    choices=[\"yes\", \"no\"])\n\nflags = parser.parse_args()\nSEED_base_value = 101\n\n\"\"\"\nCREATE MAPPINGS HERE\n\"\"\"\n\nif re.match(\"^BERT|^ELECTRA\", flags.model):\n    model_dict = {flags.model: BigTransformerTagger}\nelif re.match(\"^CSE\", flags.model):\n    model_dict = {flags.model: CSETagger}\nelse:\n    raise ValueError(\"Invalid model\")\n\n\nconfig_dict = {flags.model: eval(\"{0}_config\".format(flags.model))}\n\n\"\"\"\nmodel_dict = {'BERT': BigTransformerTagger,\n              'ELECTRA': BigTransformerTagger,\n              'ELECTRA_CRF': BigTransformerTagger,\n              \"ELECTRA_BiLSTM_CRF\": BigTransformerTagger,\n              'ELECTRA_w2v_BiLSTM_CRF': BigTransformerTagger,\n              \"ELECTRA_w2v_extra_BiLSTM_CRF\": BigTransformerTagger,\n              \"ELECTRA_extra_BiLSTM_CRF\": BigTransformerTagger,\n              \"ELECTRA_extra\": BigTransformerTagger,\n              \"ELECTRA_extra_CRF\": BigTransformerTagger}\n\nconfig_dict = {'BERT': BERT_config,\n               'ELECTRA': ELECTRA_config,\n               'ELECTRA_CRF': ELECTRA_CRF_config,\n               \"ELECTRA_BiLSTM_CRF\": ELECTRA_BiLSTM_CRF_config,\n               'ELECTRA_w2v_BiLSTM_CRF': ELECTRA_w2v_BiLSTM_CRF_config,\n               'ELECTRA_w2v_extra_BiLSTM_CRF': ELECTRA_w2v_extra_BiLSTM_CRF_config,\n               \"ELECTRA_extra_BiLSTM_CRF\": ELECTRA_extra_BiLSTM_CRF_config,\n               \"ELECTRA_extra\": ELECTRA_extra_config,\n               \"ELECTRA_extra_CRF\": ELECTRA_extra_CRF_config}\n\"\"\"\n\nconfig = config_dict[flags.model]\nconfig = config()\n\nif flags.lr >= 0:\n    config.lr = flags.lr\n\nif flags.fine_tune_lr >= 0:\n    config.fine_tune_lr = flags.fine_tune_lr\n\ndisplay_step = flags.display_step\n\nprint('Dataset: {}'.format(flags.dataset))\nprint(\"Model Name: {}\".format(flags.model))\nprint(\"Total Runs: {}\".format(flags.times))\nprint(\"Learning Rate: {}\".format(config.lr))\nprint(\"Fine-Tune Learning Rate: {}\".format(config.fine_tune_lr))\nprint(\"Mixed-Case Training: {}\".format(flags.mixed_case_training))\nprint(\"Display Step: {}\".format(flags.display_step))\nprint(\"SEED base value: {}\".format(SEED_base_value))\n\n\ncommon_data_path = \"processed_data/{}/vocab_and_embd.pkl\".format(flags.dataset)\nif flags.mixed_case_training.lower() == \"no\":\n    train_data_path = \"processed_data/{}/train_data.json\".format(flags.dataset)\nelse:\n    train_data_path = \"processed_data/{}/train_mixed_data.json\".format(flags.dataset)\ndev_data_path = \"processed_data/{}/dev_data.json\".format(flags.dataset)\ntest_data_path = \"processed_data/{}/test_data.json\".format(flags.dataset)\n\ncheckpoint_directory = \"saved_params/{}/\".format(flags.dataset)\nPath(checkpoint_directory).mkdir(parents=True, exist_ok=True)\n\nPath(\"output/\").mkdir(parents=True, exist_ok=True)\n\nlog_directory = os.path.join(\"logs\", \"{}\".format(flags.dataset))\nPath(log_directory).mkdir(parents=True, exist_ok=True)\n\nkeys = ['labels2idx', 'segment_labels2idx',\n        'w2v_vocab2idx', 'ft_vocab2idx', 'ipa2idx', 'pos2idx',\n        'w2v_embeddings', 'ft_embeddings']\n\nlabels2idx, segment_labels2idx,\\\n    w2v_vocab2idx, ft_vocab2idx, ipa2idx, pos2idx, \\\n    w2v_embeddings, ft_embeddings = load_data(common_data_path, 'rb', 'pickle', keys=keys)\n\n\nidx2labels = {v: k for k, v in labels2idx.items()}\n\n\"\"\"\nDETERMINES WHAT TO LOAD AND IN WHICH ORDER. NEEDS TO MAKE CHANGES IF YOU WANT TO LOAD SOMETHING ELSE\n\"\"\"\nkeys = [\"sequence\",\n        \"w2v_feats\", \"fasttext_feats\",\n        \"pos_tags\",\n        \"ipa_feats\", \"phono_feats\",\n        \"labels\", \"segment_labels\"]\n\n\"\"\"\nsequence = variable length natural language sequences\nw2v_feats = variable length sequences in int format where int id correspond to a word2vec vector (mapped to a word in w2v_vocab2idx)\nfasttext_feats = same as above but for fasttext\npos_tags = same as above but int id corresponds to the pos tag of the corresponding word. the id is associated to pos2idx (mapping between id and pos tags). Need to create random embeddings for pos tags.\nipa_feats = character level features will be padded and batched to batch_size x sequence_len x word_len. int format where id correspond to a specific ipa alphabet in ipa2idx mapping. Need to create a randomly initialized embedding.\nphono_feats = same as above but each character is represented as a float vector of 22 dimensions instead (can be directly treated as char-level embeddings)\nlabels = variable length sequence labels for the corresponding sequences. int format. id correspond to a particular label (mapping in labels2idx)\nsegment_label = we can ignore it for now. Can be later used for multi-tasking for entity-segmentation task (where we do not predict the type of the entity just the boundaries)\n\"\"\"\n\n\"\"\"\nFor more about load_data see: utils/data_utils.py\n\"\"\"\ntrain_sample_tuples = load_data(train_data_path, 'r', 'json', keys=keys)\nval_sample_tuples = load_data(dev_data_path, 'r', 'json', keys=keys)\ntest_sample_tuples = load_data(test_data_path, 'r', 'json', keys=keys)\n\nMAX_CHAR_LEN = len(train_sample_tuples[4][0][0])\n\nIPA_PAD = [0]*MAX_CHAR_LEN\n\n\nPHONO_PAD = [0]*config.phono_feats_dim\nPHONO_PAD = [PHONO_PAD]*MAX_CHAR_LEN\n\nif \"bert\" in flags.model.lower() or \"electra\" in flags.model.lower():\n    if \"bert\" in flags.model.lower():\n        BigModel = BertModel.from_pretrained(config.embedding_path,\n                                             output_hidden_states=True,\n                                             output_attentions=False)\n\n        tokenizer = BertTokenizerFast.from_pretrained(config.embedding_path,\n                                                      output_hidden_states=True,\n                                                      output_attentions=False)\n    elif \"electra\" in flags.model.lower():\n\n        BigModel = ElectraModel.from_pretrained(config.embedding_path,\n                                                output_hidden_states=True,\n                                                output_attentions=False)\n\n        tokenizer = ElectraTokenizerFast.from_pretrained(config.embedding_path,\n                                                         output_hidden_states=True,\n                                                         output_attentions=False)\n\n    pad_types = [None, w2v_vocab2idx[''], ft_vocab2idx[''],\n                 pos2idx['G'], IPA_PAD, PHONO_PAD, labels2idx[\"O\"], segment_labels2idx[\"O\"]]\n\nelse:\n    cse_gen = CSEGenerator(config.use_forward, config.use_backward)\n    tokenizer = None\n    \"\"\"\n    Probably need to do nothing for CSE here\n    text sequences will not be padded (can be padded later after embedding)\n    will need to change things if using precomputed embeddings\n    \"\"\"\n    pad_types = [None, w2v_vocab2idx[''], ft_vocab2idx[''],\n                 pos2idx['G'], IPA_PAD, PHONO_PAD, labels2idx[\"O\"], segment_labels2idx[\"O\"]]\n\n\ndef run(time, display_params=False):\n\n    global model_dict\n    global flags\n    global config\n    global device\n    global checkpoint_directory, log_directory\n    global BigModel\n    global w2v_embeddings, ft_embeddings\n    global ft_vocab2idx, w2v_vocab2idx, pos2idx, ipa2idx, labels2idx\n\n    mixed_string = \"\" if flags.mixed_case_training.lower() == \"no\" else \"mixed_case_\"\n\n    checkpoint_path = os.path.join(\n        checkpoint_directory, \"{}_{}run{}.pt\".format(flags.model, mixed_string, time))\n\n    log_path = os.path.join(log_directory,\n                            \"{}_{}run{}.json\".format(flags.model, mixed_string, time))\n\n    # print(checkpoint_path)\n\n    # print(\"Model: {}\".format(config.model_name))\n\n    NamedEntitiyRecognizer = model_dict[flags.model]\n\n    \"\"\"\n    May need to make changes here and may be some conditional statements\n    \"\"\"\n\n    if 'bert' in flags.model.lower() or 'electra' in flags.model.lower():\n\n        if config.use_w2v:\n            classic_embeddings = w2v_embeddings\n            word_pad_id = w2v_vocab2idx['']\n        elif config.use_fasttext:\n            classic_embeddings = ft_embeddings\n            word_pad_id = ft_vocab2idx['']\n        else:\n            classic_embeddings = None\n            word_pad_id = None\n\n        if config.use_pos_tags:\n            pos_vocab_size = len(pos2idx)\n        else:\n            pos_vocab_size = None\n\n        if config.use_char_feats:\n            ipa_vocab_size = len(ipa2idx)\n        else:\n            ipa_vocab_size = None\n\n        model = NamedEntitiyRecognizer(BigTransformer=BigModel,\n                                       classes_num=len(labels2idx),\n                                       negative_index=labels2idx['O'],\n                                       config=config,\n                                       device=device,\n                                       classic_embeddings=classic_embeddings,\n                                       word_pad_id=word_pad_id,\n                                       pos_vocab_size=pos_vocab_size,\n                                       ipa_vocab_size=ipa_vocab_size)\n\n    else:\n        \"\"\"\n        Put CSE code here\n\n        \"\"\"\n\n        if config.use_w2v:\n            classic_embeddings = w2v_embeddings\n            word_pad_id = w2v_vocab2idx['']\n        elif config.use_fasttext:\n            classic_embeddings = ft_embeddings\n            word_pad_id = ft_vocab2idx['']\n        else:\n            classic_embeddings = None\n            word_pad_id = None\n\n        if config.use_pos_tags:\n            pos_vocab_size = len(pos2idx)\n        else:\n            pos_vocab_size = None\n\n        if config.use_char_feats:\n            ipa_vocab_size = len(ipa2idx)\n        else:\n            ipa_vocab_size = None\n\n        model = NamedEntitiyRecognizer(cse_gen,\n                                       classes_num=len(labels2idx),\n                                       config=config,\n                                       device=device,\n                                       classic_embeddings=classic_embeddings,\n                                       word_pad_id=word_pad_id,\n                                       ipa_vocab_size=ipa_vocab_size,\n                                       pos_vocab_size=pos_vocab_size)\n\n    model = model.to(device)\n\n    parameters = [p for p in model.parameters() if p.requires_grad]\n    parameter_count = param_count(parameters)\n\n    print(\"\\n\\nParameter Count: {}\\n\\n\".format(parameter_count))\n    if display_params:\n        param_display_fn(model)\n\n    print(\"RUN: {}\\n\\n\".format(time))\n\n    run_epochs(model, config, checkpoint_path, log_path)\n\n\ndef run_epochs(model, config, checkpoint_path, log_path):\n    \"\"\"\n\n    raise ValueError(\n        \"Have you remembered to save the whole epoch log? (both dump output and in a dict)\")\n    \"\"\"\n\n    global train_sample_tuples, val_sample_tuples, test_sample_tuples\n\n    train_actual_iters = count_actual_iterations(train_sample_tuples[0], config)\n    val_actual_iters = count_actual_iterations(val_sample_tuples[0], config)\n    test_actual_iters = count_actual_iterations(test_sample_tuples[0], config)\n\n    train_effective_iters = count_effective_iterations(train_sample_tuples[0], config)\n    val_effective_iters = count_effective_iterations(val_sample_tuples[0], config)\n    test_effective_iters = count_effective_iterations(test_sample_tuples[0], config)\n\n    # print(train_iters)\n\n    optimizer = load_LRangerMod(model,\n                                config=config)  # misleading just running AdamW now\n\n    print('Loading pre-trained weights for the model...')\n\n    checkpoint = T.load(checkpoint_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    print('\\nRESTORATION COMPLETE\\n')\n\n    optimizer.zero_grad()\n\n    # with tqdm(total=config.epochs-past_epoch, desc='Epoch', position=0) as pbar:\n\n    print(\"TESTING\\n\")\n\n    test_loss, test_F1 = run_batches(test_sample_tuples,\n                                     epoch=0,\n                                     model=model,\n                                     optimizer=optimizer,\n                                     config=config,\n                                     generator_len=test_actual_iters,\n                                     train=False,\n                                     desc='Test Batch')\n\n    # print(test_F1)\n\n\ndef run_batches(sample_tuples, epoch,\n                model, optimizer, config,\n                generator_len,\n                train=True, scheduler=None,\n                desc=None):\n\n    global display_step\n    global pad_types\n    global tokenizer\n    global idx2labels\n    global flags\n\n    accu_step = config.total_batch_size//config.train_batch_size\n\n    if desc is None:\n        desc = 'Batch'\n\n    losses = []\n    F1s = []\n\n    total_tp = 0\n    total_pred_len = 0\n    total_gold_len = 0\n\n    # copy_tuples = copy.deepcopy(sample_tuples)\n\n    f = open(\"output/out_{}.txt\".format(flags.model), \"w\")\n    f.write('')\n    f.close()\n\n    with tqdm(total=generator_len, desc=desc, position=0) as pbar:\n\n        i = 0\n\n        for batch, batch_masks in batcher(sample_tuples,\n                                          pad_types,\n                                          config.train_batch_size,\n                                          sort_by_idx=1):\n\n            # pbar = tqdm(total=generator_len, desc='Batch', position=0)\n\n            batch_texts = batch[0]\n            batch_w2v_idx = batch[1]\n            batch_ft_idx = batch[2]\n            batch_pos_idx = batch[3]\n            batch_ipa_idx = batch[4]\n            batch_phono = batch[5]\n            batch_labels = batch[6]\n            batch_segment_labels = batch[7]\n\n            batch_mask = batch_masks[1]\n\n            \"\"\"\n            IMPLEMENT INSIDE utils/ml_utils.py\n            \"\"\"\n\n            predictions, loss = predict_NER(model=model,\n                                            tokenizer=tokenizer,\n                                            batch_texts=batch_texts,\n                                            batch_w2v_idx=batch_w2v_idx,\n                                            batch_ft_idx=batch_ft_idx,\n                                            batch_pos_idx=batch_pos_idx,\n                                            batch_ipa_idx=batch_ipa_idx,\n                                            batch_phono=batch_phono,\n                                            batch_labels=batch_labels,\n                                            batch_segment_labels=batch_segment_labels,\n                                            batch_mask=batch_mask,\n                                            device=device,\n                                            config=config,\n                                            train=train)\n            losses.append(loss.item())\n\n            if train:\n\n                loss = loss/accu_step\n                loss.backward()\n\n                if (i+1) % accu_step == 0:  # Update accumulated gradients\n\n                    T.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n                    optimizer.step()\n                    optimizer.zero_grad()\n\n                tp, pred_len, gold_len = eval_stats(predictions,\n                                                    batch_labels,\n                                                    batch_mask,\n                                                    idx2labels)\n\n                prec, rec, F1 = compute_F1(tp, pred_len, gold_len)\n\n                F1s.append(F1)\n\n                if i % display_step == 0:\n\n                    pbar.write(\"Model: {}, Epoch: {:3d}, Iter: {:5d}, \".format(config.model_name, epoch, i) +\n                               \"Loss: {:.3f}, F1: {:.3f}\".format(loss, F1))\n\n            else:\n\n                f = open(\"output/out_{}.txt\".format(flags.model), \"a\")\n                for prediction_sample, gold_sample, mask in zip(predictions, batch_labels, batch_mask):\n                    true_seq_len = sum(mask)\n                    prediction_sample = prediction_sample[0:true_seq_len]\n                    gold_sample = gold_sample[0:true_seq_len]\n                    for pred, gold in zip(prediction_sample, gold_sample):\n                        f.write(\"test NNP \"+str(idx2labels[gold])+\" \"+str(idx2labels[pred])+\"\\n\")\n                f.close()\n                tp, pred_len, gold_len = eval_stats(predictions,\n                                                    batch_labels,\n                                                    batch_mask,\n                                                    idx2labels)\n\n                prec, rec, F1 = compute_F1(tp, pred_len, gold_len)\n\n                total_tp += tp\n                total_pred_len += pred_len\n                total_gold_len += gold_len\n\n                if i % display_step == 0:\n\n                    pbar.write(\"Model: {}, Epoch: {:3d}, Iter: {:5d}, \".format(config.model_name, epoch, i) +\n                               \"Loss: {:.3f}\".format(loss))\n\n            i += 1\n            pbar.update(1)\n\n    # print(\"generator_len\", generator_len)\n    # print(\"i\", i)\n\n    print(\"\\n\\n\")\n\n    if train:\n        F1 = np.mean(F1s)\n    else:\n        prec, rec, F1 = compute_F1(total_tp, total_pred_len, total_gold_len)\n\n    # del copy_tuples\n\n    return np.mean(losses), F1\n\n\nif __name__ == '__main__':\n    time = 0\n    while time < flags.times:\n\n        if time == 0:\n            \"\"\"\n            time_str = input(\"\\nStarting time (0,1,2.....times): \")\n            try:\n                time = int(time_str)\n            except:\n                time = 0\n            \"\"\"\n            time = 0\n\n        SEED = SEED_base_value+time\n        T.manual_seed(SEED)\n        random.seed(SEED)\n        T.backends.cudnn.deterministic = True\n        T.backends.cudnn.benchmark = False\n        np.random.seed(SEED)\n\n        run(time, display_params=True)\n        time += 1\n```\n",
    "after": "import numpy as np\nimport random\nfrom dataLoader.batch import batcher\nfrom transformers import BertTokenizerFast, ElectraTokenizerFast\nfrom configs.WNUT_configs import *\nfrom utils.ml_utils import *\nfrom utils.data_utils import *\nfrom utils.metric_utils import *\nimport argparse\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport os\nimport torch as T\nimport torch.nn as nn\nfrom models.BigTransformerTagger import BigTransformerTagger\nfrom models.CSETagger import CSETagger\nfrom models.layers.BigTransformers.BERT import BertModel\nfrom models.layers.BigTransformers.ELECTRA import ElectraModel\nfrom models.cse_generator import CSEGenerator\nimport json\nimport sys\nimport re\n\n\"\"\"\nFUTURE STUFF TO KEEP IN MIND:\n\"\"\"\n\"\"\"\nTRY SAVE BY LOSS IN THE FUTURE\n\"\"\"\n\"\"\"\nIN FUTURE CHECK IF KEEPING TRUE CASES HARMS OR HELPS BERT\n\"\"\"\n\"\"\"\nCHECK WORD 2 VEC OOV STUFF\n\"\"\"\n\"\"\"\nCHECK CLASS WEIGHING\n\"\"\"\n\"\"\"\nCHECK FOR QA CHECK WITHOUT NEGATIVE EXAMPLES\n\"\"\"\n\"\"\"\nCHECK FOR QA IN FULL MODE\n\"\"\"\n\"\"\"\nIMPORT MODEL HERE\n\"\"\"\n\"\"\"\nFIX LSTM AND TRY ORDERED MEMORY AND GCDT AND STUFFS\n\"\"\"\n\n\ndevice = T.device('cuda' if T.cuda.is_available() else 'cpu')\n\nparser = argparse.ArgumentParser(description='Model Name and stuff')\nparser.add_argument('--model', type=str, default=\"ELECTRA_extra_BiLSTM_CRF\",\n                    choices=[\"BERT\",\n                             \"BERT_CRF\",\n                             \"BERT_BiLSTM_CRF\",\n                             \"BERT_w2v_BiLSTM_CRF\",\n                             \"BERT_extra_BiLSTM_CRF\",\n                             \"ELECTRA\",\n                             \"ELECTRA_CRF\",\n                             \"ELECTRA_fine_tune_CRF\",\n                             \"ELECTRA_BiLSTM_CRF\",\n                             \"ELECTRA_w2v_BiLSTM_CRF\",\n                             \"ELECTRA_extra_BiLSTM_CRF\",\n                             \"ELECTRA_extra_CRF\",\n                             \"ELECTRA_extra\",\n                             \"ELECTRA_w2v_extra_BiLSTM_CRF\",\n                             \"ELECTRA_extra_BiLSTM_DSC\",\n                             \"CSE\",\n                             \"CSE_CRF\",\n                             \"CSE_BiLSTM_CRF\",\n                             \"CSE_w2v_BiLSTM_CRF\",\n                             \"CSE_w2v_extra_BiLSTM_CRF\",\n                             \"CSE_extra_BiLSTM_CRF\"])\n\nparser.add_argument('--dataset', type=str, default=\"WNUT_2017\")\nparser.add_argument('--display_step', type=int, default=30)\nparser.add_argument('--lr', type=float, default=-1)\nparser.add_argument('--fine_tune_lr', type=float, default=-1)\nparser.add_argument('--times', type=int, default=1)\nparser.add_argument('--mixed_case_training', type=str, default=\"no\",\n                    choices=[\"yes\", \"no\"])\n\nflags = parser.parse_args()\nSEED_base_value = 101\n\n\"\"\"\nCREATE MAPPINGS HERE\n\"\"\"\n\nif re.match(\"^BERT|^ELECTRA\", flags.model):\n    model_dict = {flags.model: BigTransformerTagger}\nelif re.match(\"^CSE\", flags.model):\n    model_dict = {flags.model: CSETagger}\nelse:\n    raise ValueError(\"Invalid model\")\n\n\nconfig_dict = {flags.model: eval(\"{0}_config\".format(flags.model))}\n\n\"\"\"\nmodel_dict = {'BERT': BigTransformerTagger,\n              'ELECTRA': BigTransformerTagger,\n              'ELECTRA_CRF': BigTransformerTagger,\n              \"ELECTRA_BiLSTM_CRF\": BigTransformerTagger,\n              'ELECTRA_w2v_BiLSTM_CRF': BigTransformerTagger,\n              \"ELECTRA_w2v_extra_BiLSTM_CRF\": BigTransformerTagger,\n              \"ELECTRA_extra_BiLSTM_CRF\": BigTransformerTagger,\n              \"ELECTRA_extra\": BigTransformerTagger,\n              \"ELECTRA_extra_CRF\": BigTransformerTagger}\n\nconfig_dict = {'BERT': BERT_config,\n               'ELECTRA': ELECTRA_config,\n               'ELECTRA_CRF': ELECTRA_CRF_config,\n               \"ELECTRA_BiLSTM_CRF\": ELECTRA_BiLSTM_CRF_config,\n               'ELECTRA_w2v_BiLSTM_CRF': ELECTRA_w2v_BiLSTM_CRF_config,\n               'ELECTRA_w2v_extra_BiLSTM_CRF': ELECTRA_w2v_extra_BiLSTM_CRF_config,\n               \"ELECTRA_extra_BiLSTM_CRF\": ELECTRA_extra_BiLSTM_CRF_config,\n               \"ELECTRA_extra\": ELECTRA_extra_config,\n               \"ELECTRA_extra_CRF\": ELECTRA_extra_CRF_config}\n\"\"\"\n\nconfig = config_dict[flags.model]\nconfig = config()\n\nif flags.lr >= 0:\n    config.lr = flags.lr\n\nif flags.fine_tune_lr >= 0:\n    config.fine_tune_lr = flags.fine_tune_lr\n\ndisplay_step = flags.display_step\n\nprint('Dataset: {}'.format(flags.dataset))\nprint(\"Model Name: {}\".format(flags.model))\nprint(\"Total Runs: {}\".format(flags.times))\nprint(\"Learning Rate: {}\".format(config.lr))\nprint(\"Fine-Tune Learning Rate: {}\".format(config.fine_tune_lr))\nprint(\"Mixed-Case Training: {}\".format(flags.mixed_case_training))\nprint(\"Display Step: {}\".format(flags.display_step))\nprint(\"SEED base value: {}\".format(SEED_base_value))\n\n\ncommon_data_path = \"processed_data/{}/vocab_and_embd.pkl\".format(flags.dataset)\nif flags.mixed_case_training.lower() == \"no\":\n    train_data_path = \"processed_data/{}/train_data.json\".format(flags.dataset)\nelse:\n    train_data_path = \"processed_data/{}/train_mixed_data.json\".format(flags.dataset)\ndev_data_path = \"processed_data/{}/dev_data.json\".format(flags.dataset)\ntest_data_path = \"processed_data/{}/test_data.json\".format(flags.dataset)\n\ncheckpoint_directory = \"saved_params/{}/\".format(flags.dataset)\nPath(checkpoint_directory).mkdir(parents=True, exist_ok=True)\n\nPath(\"output/\").mkdir(parents=True, exist_ok=True)\n\nlog_directory = os.path.join(\"logs\", \"{}\".format(flags.dataset))\nPath(log_directory).mkdir(parents=True, exist_ok=True)\n\nkeys = ['labels2idx', 'segment_labels2idx',\n        'w2v_vocab2idx', 'ft_vocab2idx', 'ipa2idx', 'pos2idx',\n        'w2v_embeddings', 'ft_embeddings']\n\nlabels2idx, segment_labels2idx,\\\n    w2v_vocab2idx, ft_vocab2idx, ipa2idx, pos2idx, \\\n    w2v_embeddings, ft_embeddings = load_data(common_data_path, 'rb', 'pickle', keys=keys)\n\n\nidx2labels = {v: k for k, v in labels2idx.items()}\n\n\"\"\"\nDETERMINES WHAT TO LOAD AND IN WHICH ORDER. NEEDS TO MAKE CHANGES IF YOU WANT TO LOAD SOMETHING ELSE\n\"\"\"\nkeys = [\"sequence\",\n        \"w2v_feats\", \"fasttext_feats\",\n        \"pos_tags\",\n        \"ipa_feats\", \"phono_feats\",\n        \"labels\", \"segment_labels\"]\n\n\"\"\"\nsequence = variable length natural language sequences\nw2v_feats = variable length sequences in int format where int id correspond to a word2vec vector (mapped to a word in w2v_vocab2idx)\nfasttext_feats = same as above but for fasttext\npos_tags = same as above but int id corresponds to the pos tag of the corresponding word. the id is associated to pos2idx (mapping between id and pos tags). Need to create random embeddings for pos tags.\nipa_feats = character level features will be padded and batched to batch_size x sequence_len x word_len. int format where id correspond to a specific ipa alphabet in ipa2idx mapping. Need to create a randomly initialized embedding.\nphono_feats = same as above but each character is represented as a float vector of 22 dimensions instead (can be directly treated as char-level embeddings)\nlabels = variable length sequence labels for the corresponding sequences. int format. id correspond to a particular label (mapping in labels2idx)\nsegment_label = we can ignore it for now. Can be later used for multi-tasking for entity-segmentation task (where we do not predict the type of the entity just the boundaries)\n\"\"\"\n\n\"\"\"\nFor more about load_data see: utils/data_utils.py\n\"\"\"\ntrain_sample_tuples = load_data(train_data_path, 'r', 'json', keys=keys)\nval_sample_tuples = load_data(dev_data_path, 'r', 'json', keys=keys)\ntest_sample_tuples = load_data(test_data_path, 'r', 'json', keys=keys)\n\nMAX_CHAR_LEN = len(train_sample_tuples[4][0][0])\n\nIPA_PAD = [0]*MAX_CHAR_LEN\n\n\nPHONO_PAD = [0]*config.phono_feats_dim\nPHONO_PAD = [PHONO_PAD]*MAX_CHAR_LEN\n\nif \"bert\" in flags.model.lower() or \"electra\" in flags.model.lower():\n    if \"bert\" in flags.model.lower():\n        BigModel = BertModel.from_pretrained(config.embedding_path,\n                                             output_hidden_states=True,\n                                             output_attentions=False)\n\n        tokenizer = BertTokenizerFast.from_pretrained(config.embedding_path,\n                                                      output_hidden_states=True,\n                                                      output_attentions=False)\n    elif \"electra\" in flags.model.lower():\n\n        BigModel = ElectraModel.from_pretrained(config.embedding_path,\n                                                output_hidden_states=True,\n                                                output_attentions=False)\n\n        tokenizer = ElectraTokenizerFast.from_pretrained(config.embedding_path,\n                                                         output_hidden_states=True,\n                                                         output_attentions=False)\n\n    pad_types = [None, w2v_vocab2idx[''], ft_vocab2idx[''],\n                 pos2idx['G'], IPA_PAD, PHONO_PAD, labels2idx[\"O\"], segment_labels2idx[\"O\"]]\n\nelse:\n    cse_gen = CSEGenerator(config.use_forward, config.use_backward)\n    tokenizer = None\n    \"\"\"\n    Probably need to do nothing for CSE here\n    text sequences will not be padded (can be padded later after embedding)\n    will need to change things if using precomputed embeddings\n    \"\"\"\n    pad_types = [None, w2v_vocab2idx[''], ft_vocab2idx[''],\n                 pos2idx['G'], IPA_PAD, PHONO_PAD, labels2idx[\"O\"], segment_labels2idx[\"O\"]]\n\n\ndef run(time, display_params=False):\n\n    global model_dict\n    global flags\n    global config\n    global device\n    global checkpoint_directory, log_directory\n    global BigModel\n    global w2v_embeddings, ft_embeddings\n    global ft_vocab2idx, w2v_vocab2idx, pos2idx, ipa2idx, labels2idx\n\n    mixed_string = \"\" if flags.mixed_case_training.lower() == \"no\" else \"mixed_case_\"\n\n    checkpoint_path = os.path.join(\n        checkpoint_directory, \"{}_{}run{}.pt\".format(flags.model, mixed_string, time))\n\n    log_path = os.path.join(log_directory,\n                            \"{}_{}run{}.json\".format(flags.model, mixed_string, time))\n\n    # print(checkpoint_path)\n\n    # print(\"Model: {}\".format(config.model_name))\n\n    NamedEntitiyRecognizer = model_dict[flags.model]\n\n    \"\"\"\n    May need to make changes here and may be some conditional statements\n    \"\"\"\n\n    if 'bert' in flags.model.lower() or 'electra' in flags.model.lower():\n\n        if config.use_w2v:\n            classic_embeddings = w2v_embeddings\n            word_pad_id = w2v_vocab2idx['']\n        elif config.use_fasttext:\n            classic_embeddings = ft_embeddings\n            word_pad_id = ft_vocab2idx['']\n        else:\n            classic_embeddings = None\n            word_pad_id = None\n\n        if config.use_pos_tags:\n            pos_vocab_size = len(pos2idx)\n        else:\n            pos_vocab_size = None\n\n        if config.use_char_feats:\n            ipa_vocab_size = len(ipa2idx)\n        else:\n            ipa_vocab_size = None\n\n        model = NamedEntitiyRecognizer(BigTransformer=BigModel,\n                                       classes_num=len(labels2idx),\n                                       negative_index=labels2idx['O'],\n                                       config=config,\n                                       device=device,\n                                       classic_embeddings=classic_embeddings,\n                                       word_pad_id=word_pad_id,\n                                       pos_vocab_size=pos_vocab_size,\n                                       ipa_vocab_size=ipa_vocab_size)\n\n    else:\n        \"\"\"\n        Put CSE code here\n\n        \"\"\"\n\n        if config.use_w2v:\n            classic_embeddings = w2v_embeddings\n            word_pad_id = w2v_vocab2idx['']\n        elif config.use_fasttext:\n            classic_embeddings = ft_embeddings\n            word_pad_id = ft_vocab2idx['']\n        else:\n            classic_embeddings = None\n            word_pad_id = None\n\n        if config.use_pos_tags:\n            pos_vocab_size = len(pos2idx)\n        else:\n            pos_vocab_size = None\n\n        if config.use_char_feats:\n            ipa_vocab_size = len(ipa2idx)\n        else:\n            ipa_vocab_size = None\n\n        model = NamedEntitiyRecognizer(cse_gen,\n                                       classes_num=len(labels2idx),\n                                       config=config,\n                                       device=device,\n                                       classic_embeddings=classic_embeddings,\n                                       word_pad_id=word_pad_id,\n                                       ipa_vocab_size=ipa_vocab_size,\n                                       pos_vocab_size=pos_vocab_size)\n\n    model = model.to(device)\n\n    parameters = [p for p in model.parameters() if p.requires_grad]\n    parameter_count = param_count(parameters)\n\n    print(\"\\n\\nParameter Count: {}\\n\\n\".format(parameter_count))\n    if display_params:\n        param_display_fn(model)\n\n    print(\"RUN: {}\\n\\n\".format(time))\n\n    run_epochs(model, config, checkpoint_path, log_path)\n\n\ndef run_epochs(model, config, checkpoint_path, log_path):\n    \"\"\"\n\n    raise ValueError(\n        \"Have you remembered to save the whole epoch log? (both dump output and in a dict)\")\n    \"\"\"\n\n    global train_sample_tuples, val_sample_tuples, test_sample_tuples\n\n    train_actual_iters = count_actual_iterations(train_sample_tuples[0], config)\n    val_actual_iters = count_actual_iterations(val_sample_tuples[0], config)\n    test_actual_iters = count_actual_iterations(test_sample_tuples[0], config)\n\n    train_effective_iters = count_effective_iterations(train_sample_tuples[0], config)\n    val_effective_iters = count_effective_iterations(val_sample_tuples[0], config)\n    test_effective_iters = count_effective_iterations(test_sample_tuples[0], config)\n\n    # print(train_iters)\n\n    optimizer = load_LRangerMod(model,\n                                config=config)  # misleading just running AdamW now\n\n    print('Loading pre-trained weights for the model...')\n\n    checkpoint = T.load(checkpoint_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    print('\\nRESTORATION COMPLETE\\n')\n\n    optimizer.zero_grad()\n\n    # with tqdm(total=config.epochs-past_epoch, desc='Epoch', position=0) as pbar:\n\n    print(\"TESTING\\n\")\n\n    test_loss, test_F1 = run_batches(test_sample_tuples,\n                                     epoch=0,\n                                     model=model,\n                                     optimizer=optimizer,\n                                     config=config,\n                                     generator_len=test_actual_iters,\n                                     train=False,\n                                     desc='Test Batch')\n\n    # print(test_F1)\n\n\ndef run_batches(sample_tuples, epoch,\n                model, optimizer, config,\n                generator_len,\n                train=True, scheduler=None,\n                desc=None):\n\n    global display_step\n    global pad_types\n    global tokenizer\n    global idx2labels\n    global flags\n\n    accu_step = config.total_batch_size//config.train_batch_size\n\n    if desc is None:\n        desc = 'Batch'\n\n    losses = []\n    F1s = []\n\n    total_tp = 0\n    total_pred_len = 0\n    total_gold_len = 0\n\n    # copy_tuples = copy.deepcopy(sample_tuples)\n\n    f = open(\"output/out_{}.txt\".format(flags.model), \"w\")\n    f.write('')\n    f.close()\n\n    with tqdm(total=generator_len, desc=desc, position=0) as pbar:\n\n        i = 0\n\n        for batch, batch_masks in batcher(sample_tuples,\n                                          pad_types,\n                                          config.train_batch_size,\n                                          sort_by_idx=1):\n\n            # pbar = tqdm(total=generator_len, desc='Batch', position=0)\n\n            batch_texts = batch[0]\n            batch_w2v_idx = batch[1]\n            batch_ft_idx = batch[2]\n            batch_pos_idx = batch[3]\n            batch_ipa_idx = batch[4]\n            batch_phono = batch[5]\n            batch_labels = batch[6]\n            batch_segment_labels = batch[7]\n\n            batch_mask = batch_masks[1]\n\n            \"\"\"\n            IMPLEMENT INSIDE utils/ml_utils.py\n            \"\"\"\n\n            predictions, loss = predict_NER(model=model,\n                                            tokenizer=tokenizer,\n                                            batch_texts=batch_texts,\n                                            batch_w2v_idx=batch_w2v_idx,\n                                            batch_ft_idx=batch_ft_idx,\n                                            batch_pos_idx=batch_pos_idx,\n                                            batch_ipa_idx=batch_ipa_idx,\n                                            batch_phono=batch_phono,\n                                            batch_labels=batch_labels,\n                                            batch_segment_labels=batch_segment_labels,\n                                            batch_mask=batch_mask,\n                                            device=device,\n                                            config=config,\n                                            train=train)\n            losses.append(loss.item())\n\n            if train:\n\n                loss = loss/accu_step\n                loss.backward()\n\n                if (i+1) % accu_step == 0:  # Update accumulated gradients\n\n                    T.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n                    optimizer.step()\n                    optimizer.zero_grad()\n\n                tp, pred_len, gold_len = eval_stats(predictions,\n                                                    batch_labels,\n                                                    batch_mask,\n                                                    idx2labels)\n\n                prec, rec, F1 = compute_F1(tp, pred_len, gold_len)\n\n                F1s.append(F1)\n\n                if i % display_step == 0:\n\n                    pbar.write(\"Model: {}, Epoch: {:3d}, Iter: {:5d}, \".format(config.model_name, epoch, i) +\n                               \"Loss: {:.3f}, F1: {:.3f}\".format(loss, F1))\n\n            else:\n\n                f = open(\"output/out_{}.txt\".format(flags.model), \"a\")\n                for prediction_sample, gold_sample, mask in zip(predictions, batch_labels, batch_mask):\n                    true_seq_len = sum(mask)\n                    prediction_sample = prediction_sample[0:true_seq_len]\n                    gold_sample = gold_sample[0:true_seq_len]\n                    for pred, gold in zip(prediction_sample, gold_sample):\n                        f.write(\"test NNP \"+str(idx2labels[gold])+\" \"+str(idx2labels[pred])+\"\\n\")\n                f.close()\n                tp, pred_len, gold_len = eval_stats(predictions,\n                                                    batch_labels,\n                                                    batch_mask,\n                                                    idx2labels)\n\n                prec, rec, F1 = compute_F1(tp, pred_len, gold_len)\n\n                total_tp += tp\n                total_pred_len += pred_len\n                total_gold_len += gold_len\n\n                if i % display_step == 0:\n\n                    pbar.write(\"Model: {}, Epoch: {:3d}, Iter: {:5d}, \".format(config.model_name, epoch, i) +\n                               \"Loss: {:.3f}\".format(loss))\n\n            i += 1\n            pbar.update(1)\n\n    # print(\"generator_len\", generator_len)\n    # print(\"i\", i)\n\n    print(\"\\n\\n\")\n\n    if train:\n        F1 = np.mean(F1s)\n    else:\n        prec, rec, F1 = compute_F1(total_tp, total_pred_len, total_gold_len)\n\n    # del copy_tuples\n\n    return np.mean(losses), F1\n\n\nif __name__ == '__main__':\n    time = 0\n    while time < flags.times:\n\n        if time == 0:\n            \"\"\"\n            time_str = input(\"\\nStarting time (0,1,2.....times): \")\n            try:\n                time = int(time_str)\n            except:\n                time = 0\n            \"\"\"\n            time = 0\n\n        SEED = SEED_base_value+time\n        T.manual_seed(SEED)\n        random.seed(SEED)\n        T.backends.cudnn.deterministic = True\n        T.backends.cudnn.benchmark = False\n        np.random.seed(SEED)\n\n        run(time, display_params=True)\n        time += 1\n```"
  },
  "feature_stats": {
    "indices": [
      2,
      0,
      3111,
      1643,
      2617,
      154,
      181,
      6667,
      5787,
      5770,
      5598
    ],
    "f_before": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "f_after": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0
    ],
    "g_before": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.023569390177726746,
      0.0
    ],
    "g_after": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      0.023569390177726746,
      0.0
    ],
    "num_batches_for_stats": 10
  },
  "loss_summary": {
    "train_mean_loss": 9.458886481672526,
    "train_mean_l2": 5.731930392324925,
    "train_mean_l1": 3.525482515364885,
    "num_steps": 2000,
    "num_batches": 2000
  },
  "feature_std_summary": {
    "plot_path": "outputs/plots/20251128-144741/feature_std_layer_12.html",
    "mean_std": 1.0523178577423096,
    "max_std": 18.52631378173828
  }
}